<dec f='linux-4.14.y/include/linux/sched/stat.h' l='21' type='unsigned long nr_iowait()'/>
<use f='linux-4.14.y/fs/proc/stat.c' l='173' u='c' c='show_stat'/>
<def f='linux-4.14.y/kernel/sched/core.c' l='2889' ll='2897' type='unsigned long nr_iowait()'/>
<doc f='linux-4.14.y/kernel/sched/core.c' l='2859'>/*
 * IO-wait accounting, and how its mostly bollocks (on SMP).
 *
 * The idea behind IO-wait account is to account the idle time that we could
 * have spend running if it were not for IO. That is, if we were to improve the
 * storage performance, we&apos;d have a proportional reduction in IO-wait time.
 *
 * This all works nicely on UP, where, when a task blocks on IO, we account
 * idle time as IO-wait, because if the storage were faster, it could&apos;ve been
 * running and we&apos;d not be idle.
 *
 * This has been extended to SMP, by doing the same for each CPU. This however
 * is broken.
 *
 * Imagine for instance the case where two tasks block on one CPU, only the one
 * CPU will have IO-wait accounted, while the other has regular idle. Even
 * though, if the storage were faster, both could&apos;ve ran at the same time,
 * utilising both CPUs.
 *
 * This means, that when looking globally, the current IO-wait accounting on
 * SMP is a lower bound, by reason of under accounting.
 *
 * Worse, since the numbers are provided per CPU, they are sometimes
 * interpreted per CPU, and that is nonsensical. A blocked task isn&apos;t strictly
 * associated with any one particular CPU, it can wake to another CPU than it
 * blocked on. This means the per CPU IO-wait number is meaningless.
 *
 * Task CPU affinities can make all that even more &apos;interesting&apos;.
 */</doc>
