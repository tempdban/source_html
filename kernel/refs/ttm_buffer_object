<def f='linux-4.18.y/include/drm/ttm/ttm_bo_api.h' l='169' ll='234'/>
<size>552</size>
<doc f='linux-4.18.y/include/drm/ttm/ttm_bo_api.h' l='125'>/**
 * struct ttm_buffer_object
 *
 * @bdev: Pointer to the buffer object device structure.
 * @type: The bo type.
 * @destroy: Destruction function. If NULL, kfree is used.
 * @num_pages: Actual number of pages.
 * @acc_size: Accounted size for this object.
 * @kref: Reference count of this buffer object. When this refcount reaches
 * zero, the object is put on the delayed delete list.
 * @list_kref: List reference count of this buffer object. This member is
 * used to avoid destruction while the buffer object is still on a list.
 * Lru lists may keep one refcount, the delayed delete list, and kref != 0
 * keeps one refcount. When this refcount reaches zero,
 * the object is destroyed.
 * @mem: structure describing current placement.
 * @persistent_swap_storage: Usually the swap storage is deleted for buffers
 * pinned in physical memory. If this behaviour is not desired, this member
 * holds a pointer to a persistent shmem object.
 * @ttm: TTM structure holding system pages.
 * @evicted: Whether the object was evicted without user-space knowing.
 * @cpu_writes: For synchronization. Number of cpu writers.
 * @lru: List head for the lru list.
 * @ddestroy: List head for the delayed destroy list.
 * @swap: List head for swap LRU list.
 * @moving: Fence set when BO is moving
 * @vma_node: Address space manager node.
 * @offset: The current GPU offset, which can have different meanings
 * depending on the memory type. For SYSTEM type memory, it should be 0.
 * @cur_placement: Hint of current placement.
 * @wu_mutex: Wait unreserved mutex.
 *
 * Base class for TTM buffer object, that deals with data placement and CPU
 * mappings. GPU mappings are really up to the driver, but for simpler GPUs
 * the driver can usually use the placement offset @offset directly as the
 * GPU virtual address. For drivers implementing multiple
 * GPU memory manager contexts, the driver should manage the address space
 * in these contexts separately and use these objects to get the correct
 * placement and caching for these GPU maps. This makes it possible to use
 * these objects for even quite elaborate memory management schemes.
 * The destroy member, the API visibility of this object makes it possible
 * to derive driver specific types.
 */</doc>
<mbr r='ttm_buffer_object::bdev' o='0' t='struct ttm_bo_device *'/>
<mbr r='ttm_buffer_object::type' o='64' t='enum ttm_bo_type'/>
<mbr r='ttm_buffer_object::destroy' o='128' t='void (*)(struct ttm_buffer_object *)'/>
<mbr r='ttm_buffer_object::num_pages' o='192' t='unsigned long'/>
<mbr r='ttm_buffer_object::acc_size' o='256' t='size_t'/>
<mbr r='ttm_buffer_object::kref' o='320' t='struct kref'/>
<mbr r='ttm_buffer_object::list_kref' o='352' t='struct kref'/>
<mbr r='ttm_buffer_object::mem' o='384' t='struct ttm_mem_reg'/>
<mbr r='ttm_buffer_object::persistent_swap_storage' o='1152' t='struct file *'/>
<mbr r='ttm_buffer_object::ttm' o='1216' t='struct ttm_tt *'/>
<mbr r='ttm_buffer_object::evicted' o='1280' t='bool'/>
<mbr r='ttm_buffer_object::cpu_writers' o='1312' t='atomic_t'/>
<mbr r='ttm_buffer_object::lru' o='1344' t='struct list_head'/>
<mbr r='ttm_buffer_object::ddestroy' o='1472' t='struct list_head'/>
<mbr r='ttm_buffer_object::swap' o='1600' t='struct list_head'/>
<mbr r='ttm_buffer_object::io_reserve_lru' o='1728' t='struct list_head'/>
<mbr r='ttm_buffer_object::moving' o='1856' t='struct dma_fence *'/>
<mbr r='ttm_buffer_object::vma_node' o='1920' t='struct drm_vma_offset_node'/>
<mbr r='ttm_buffer_object::priority' o='3328' t='unsigned int'/>
<mbr r='ttm_buffer_object::offset' o='3392' t='uint64_t'/>
<mbr r='ttm_buffer_object::sg' o='3456' t='struct sg_table *'/>
<mbr r='ttm_buffer_object::resv' o='3520' t='struct reservation_object *'/>
<mbr r='ttm_buffer_object::ttm_resv' o='3584' t='struct reservation_object'/>
<mbr r='ttm_buffer_object::wu_mutex' o='4160' t='struct mutex'/>
