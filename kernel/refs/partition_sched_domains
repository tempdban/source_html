<dec f='linux-4.14.y/include/linux/sched/topology.h' l='165' type='void partition_sched_domains(int ndoms_new, cpumask_var_t * doms_new, struct sched_domain_attr * dattr_new)'/>
<use f='linux-4.14.y/kernel/cgroup/cpuset.c' l='852' u='c' c='rebuild_sched_domains_locked'/>
<use f='linux-4.14.y/kernel/sched/core.c' l='5587' u='c' c='cpuset_cpu_active'/>
<use f='linux-4.14.y/kernel/sched/core.c' l='5608' u='c' c='cpuset_cpu_inactive'/>
<def f='linux-4.14.y/kernel/sched/topology.c' l='1859' ll='1930' type='void partition_sched_domains(int ndoms_new, cpumask_var_t * doms_new, struct sched_domain_attr * dattr_new)'/>
<doc f='linux-4.14.y/kernel/sched/topology.c' l='1833'>/*
 * Partition sched domains as specified by the &apos;ndoms_new&apos;
 * cpumasks in the array doms_new[] of cpumasks. This compares
 * doms_new[] to the current sched domain partitioning, doms_cur[].
 * It destroys each deleted domain and builds each new domain.
 *
 * &apos;doms_new&apos; is an array of cpumask_var_t&apos;s of length &apos;ndoms_new&apos;.
 * The masks don&apos;t intersect (don&apos;t overlap.) We should setup one
 * sched domain for each mask. CPUs not in any of the cpumasks will
 * not be load balanced. If the same cpumask appears both in the
 * current &apos;doms_cur&apos; domains and in the new &apos;doms_new&apos;, we can leave
 * it as it is.
 *
 * The passed in &apos;doms_new&apos; should be allocated using
 * alloc_sched_domains.  This routine takes ownership of it and will
 * free_sched_domains it when done with it. If the caller failed the
 * alloc call, then it can pass in doms_new == NULL &amp;&amp; ndoms_new == 1,
 * and partition_sched_domains() will fallback to the single partition
 * &apos;fallback_doms&apos;, it also forces the domains to be rebuilt.
 *
 * If doms_new == NULL it will be replaced with cpu_online_mask.
 * ndoms_new == 0 is a special case for destroying existing domains,
 * and it will not create the default domain.
 *
 * Call with hotplug lock held
 */</doc>
