<dec f='linux-4.14.y/include/linux/mm.h' l='2126' type='int mm_take_all_locks(struct mm_struct * mm)'/>
<def f='linux-4.14.y/mm/mmap.c' l='3456' ll='3494' type='int mm_take_all_locks(struct mm_struct * mm)'/>
<doc f='linux-4.14.y/mm/mmap.c' l='3419'>/*
 * This operation locks against the VM for all pte/vma/mm related
 * operations that could ever happen on a certain mm. This includes
 * vmtruncate, try_to_unmap, and all page faults.
 *
 * The caller must take the mmap_sem in write mode before calling
 * mm_take_all_locks(). The caller isn&apos;t allowed to release the
 * mmap_sem until mm_drop_all_locks() returns.
 *
 * mmap_sem in write mode is required in order to block all operations
 * that could modify pagetables and free pages without need of
 * altering the vma layout. It&apos;s also needed in write mode to avoid new
 * anon_vmas to be associated with existing vmas.
 *
 * A single task can&apos;t take more than one mm_take_all_locks() in a row
 * or it would deadlock.
 *
 * The LSB in anon_vma-&gt;rb_root.rb_node and the AS_MM_ALL_LOCKS bitflag in
 * mapping-&gt;flags avoid to take the same lock twice, if more than one
 * vma in this mm is backed by the same anon_vma or address_space.
 *
 * We take locks in following order, accordingly to comment at beginning
 * of mm/rmap.c:
 *   - all hugetlbfs_i_mmap_rwsem_key locks (aka mapping-&gt;i_mmap_rwsem for
 *     hugetlb mapping);
 *   - all i_mmap_rwsem locks;
 *   - all anon_vma-&gt;rwseml
 *
 * We can take all locks within these types randomly because the VM code
 * doesn&apos;t nest them and we protected from parallel mm_take_all_locks() by
 * mm_all_locks_mutex.
 *
 * mm_take_all_locks() and mm_drop_all_locks are expensive operations
 * that may have to take thousand of locks.
 *
 * mm_take_all_locks() can fail if it&apos;s interrupted by signals.
 */</doc>
<use f='linux-4.14.y/mm/mmu_notifier.c' l='248' u='c' c='do_mmu_notifier_register'/>
