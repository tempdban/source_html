<dec f='linux-4.14.y/include/linux/percpu_counter.h' l='43' type='void percpu_counter_add_batch(struct percpu_counter * fbc, s64 amount, s32 batch)'/>
<use f='linux-4.14.y/include/linux/percpu_counter.h' l='55' u='c' c='percpu_counter_add'/>
<use f='linux-4.14.y/include/linux/mman.h' l='26' u='c' c='vm_acct_memory'/>
<use f='linux-4.14.y/include/linux/blk-cgroup.h' l='522' u='c' c='blkg_stat_add'/>
<use f='linux-4.14.y/include/linux/blk-cgroup.h' l='601' u='c' c='blkg_rwstat_add'/>
<use f='linux-4.14.y/include/linux/blk-cgroup.h' l='608' u='c' c='blkg_rwstat_add'/>
<use f='linux-4.14.y/include/linux/backing-dev.h' l='70' u='c' c='__add_wb_stat'/>
<use f='linux-4.14.y/lib/flex_proportions.c' l='211' u='c' c='fprop_reflect_period_percpu'/>
<use f='linux-4.14.y/lib/flex_proportions.c' l='223' u='c' c='__fprop_inc_percpu'/>
<use f='linux-4.14.y/lib/flex_proportions.c' l='271' u='c' c='__fprop_inc_percpu_max'/>
<def f='linux-4.14.y/lib/percpu_counter.c' l='83' ll='99' type='void percpu_counter_add_batch(struct percpu_counter * fbc, s64 amount, s32 batch)'/>
<dec f='linux-4.14.y/lib/percpu_counter.c' l='100' type='void percpu_counter_add_batch(struct percpu_counter * , s64 , s32 )'/>
<use f='linux-4.14.y/lib/percpu_counter.c' l='100' c='percpu_counter_add_batch'/>
<use f='linux-4.14.y/lib/percpu_counter.c' l='100' u='a'/>
<use f='linux-4.14.y/lib/percpu_counter.c' l='100' u='a'/>
<doc f='linux-4.14.y/lib/percpu_counter.c' l='76'>/**
 * This function is both preempt and irq safe. The former is due to explicit
 * preemption disable. The latter is guaranteed by the fact that the slow path
 * is explicitly protected by an irq-safe spinlock whereas the fast patch uses
 * this_cpu_add which is irq-safe by definition. Hence there is no need muck
 * with irq state before calling this one
 */</doc>
