<def f='linux-4.14.y/arch/x86/include/asm/processor.h' l='805' ll='810' type='void prefetchw(const void * x)'/>
<use f='linux-4.14.y/arch/x86/include/asm/processor.h' l='814' u='c' c='spin_lock_prefetch'/>
<doc f='linux-4.14.y/arch/x86/include/asm/processor.h' l='800'>/*
 * 3dnow prefetch to get an exclusive cache line.
 * Useful for spinlocks to avoid one state transition in the
 * cache coherency protocol:
 */</doc>
<use f='linux-4.14.y/arch/x86/mm/fault.c' l='1255' u='c' c='__do_page_fault'/>
<use f='linux-4.14.y/include/linux/netdevice.h' l='2938' u='c' c='netdev_txq_bql_enqueue_prefetchw'/>
<use f='linux-4.14.y/include/linux/netdevice.h' l='2952' u='c' c='netdev_txq_bql_complete_prefetchw'/>
<use f='linux-4.14.y/drivers/net/ethernet/intel/igb/igb_main.c' l='7473' u='c' c='igb_get_rx_buffer'/>
<use f='linux-4.14.y/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c' l='2071' u='c' c='ixgbe_get_rx_buffer'/>
<use f='linux-4.14.y/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c' l='856' u='c' c='ixgbevf_fetch_rx_buffer'/>
<use f='linux-4.14.y/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c' l='880' u='c' c='ixgbevf_fetch_rx_buffer'/>
<use f='linux-4.14.y/fs/ext4/readpage.c' l='129' u='c' c='ext4_mpage_readpages'/>
<use f='linux-4.14.y/fs/mpage.c' l='378' u='c' c='mpage_readpages'/>
<use f='linux-4.14.y/kernel/locking/qspinlock.c' l='441' u='c' c='queued_spin_lock_slowpath'/>
<use f='linux-4.14.y/mm/page_alloc.c' l='1272' u='c' c='__free_pages_boot_core'/>
<use f='linux-4.14.y/mm/page_alloc.c' l='1274' u='c' c='__free_pages_boot_core'/>
<use f='linux-4.14.y/mm/slab.c' l='3382' u='c' c='slab_alloc'/>
<use f='linux-4.14.y/mm/vmscan.c' l='142' u='c' c='isolate_lru_pages'/>
<use f='linux-4.14.y/net/core/skbuff.c' l='196' u='c' c='__alloc_skb'/>
<use f='linux-4.14.y/net/core/skbuff.c' l='213' u='c' c='__alloc_skb'/>
<use f='linux-4.14.y/net/ipv4/tcp_input.c' l='3109' u='c' c='tcp_clean_rtx_queue'/>
<use f='linux-4.14.y/net/ipv4/tcp_input.c' l='3579' u='c' c='tcp_ack'/>
<def f='linux-4.14.y/arch/x86/include/asm/processor.h' l='805' ll='810' type='void prefetchw(const void * x)'/>
<use f='linux-4.14.y/arch/x86/include/asm/processor.h' l='814' u='c' c='spin_lock_prefetch'/>
<doc f='linux-4.14.y/arch/x86/include/asm/processor.h' l='800'>/*
 * 3dnow prefetch to get an exclusive cache line.
 * Useful for spinlocks to avoid one state transition in the
 * cache coherency protocol:
 */</doc>
<use f='linux-4.14.y/arch/x86/mm/fault.c' l='1255' u='c' c='__do_page_fault'/>
<use f='linux-4.14.y/include/linux/netdevice.h' l='2938' u='c' c='netdev_txq_bql_enqueue_prefetchw'/>
<use f='linux-4.14.y/include/linux/netdevice.h' l='2952' u='c' c='netdev_txq_bql_complete_prefetchw'/>
<use f='linux-4.14.y/drivers/net/ethernet/intel/igb/igb_main.c' l='7473' u='c' c='igb_get_rx_buffer'/>
<use f='linux-4.14.y/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c' l='2071' u='c' c='ixgbe_get_rx_buffer'/>
<use f='linux-4.14.y/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c' l='856' u='c' c='ixgbevf_fetch_rx_buffer'/>
<use f='linux-4.14.y/drivers/net/ethernet/intel/ixgbevf/ixgbevf_main.c' l='880' u='c' c='ixgbevf_fetch_rx_buffer'/>
<use f='linux-4.14.y/fs/ext4/readpage.c' l='129' u='c' c='ext4_mpage_readpages'/>
<use f='linux-4.14.y/fs/f2fs/data.c' l='1220' u='c' c='f2fs_mpage_readpages'/>
<use f='linux-4.14.y/fs/mpage.c' l='378' u='c' c='mpage_readpages'/>
<use f='linux-4.14.y/kernel/locking/qspinlock.c' l='441' u='c' c='queued_spin_lock_slowpath'/>
<use f='linux-4.14.y/mm/page_alloc.c' l='1272' u='c' c='__free_pages_boot_core'/>
<use f='linux-4.14.y/mm/page_alloc.c' l='1274' u='c' c='__free_pages_boot_core'/>
<use f='linux-4.14.y/mm/slab.c' l='3382' u='c' c='slab_alloc'/>
<use f='linux-4.14.y/mm/vmscan.c' l='142' u='c' c='isolate_lru_pages'/>
<use f='linux-4.14.y/net/core/skbuff.c' l='196' u='c' c='__alloc_skb'/>
<use f='linux-4.14.y/net/core/skbuff.c' l='213' u='c' c='__alloc_skb'/>
<use f='linux-4.14.y/net/ipv4/tcp_input.c' l='3109' u='c' c='tcp_clean_rtx_queue'/>
<use f='linux-4.14.y/net/ipv4/tcp_input.c' l='3579' u='c' c='tcp_ack'/>
