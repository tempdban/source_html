<dec f='linux-4.18.y/include/linux/mm.h' l='1331' type='struct page * _vm_normal_page(struct vm_area_struct * vma, unsigned long addr, pte_t pte, bool with_public_device)'/>
<use f='linux-4.18.y/fs/proc/task_mmu.c' l='528' macro='1' u='c'/>
<use f='linux-4.18.y/fs/proc/task_mmu.c' l='713' macro='1' u='c'/>
<use f='linux-4.18.y/fs/proc/task_mmu.c' l='1039' macro='1' u='c'/>
<use f='linux-4.18.y/fs/proc/task_mmu.c' l='1255' u='c' c='pte_to_pagemap_entry'/>
<use f='linux-4.18.y/mm/gup.c' l='113' macro='1' u='c'/>
<use f='linux-4.18.y/mm/gup.c' l='470' macro='1' u='c'/>
<use f='linux-4.18.y/mm/madvise.c' l='357' u='c' c='madvise_free_pte_range'/>
<def f='linux-4.18.y/mm/memory.c' l='829' ll='901' type='struct page * _vm_normal_page(struct vm_area_struct * vma, unsigned long addr, pte_t pte, bool with_public_device)'/>
<use f='linux-4.18.y/mm/memory.c' l='1042' macro='1' u='c'/>
<use f='linux-4.18.y/mm/memory.c' l='1315' u='c' c='zap_pte_range'/>
<use f='linux-4.18.y/mm/memory.c' l='2745' macro='1' u='c'/>
<use f='linux-4.18.y/mm/memory.c' l='3830' macro='1' u='c'/>
<doc f='linux-4.18.y/mm/memory.c' l='787'>/*
 * vm_normal_page -- This function gets the &quot;struct page&quot; associated with a pte.
 *
 * &quot;Special&quot; mappings do not wish to be associated with a &quot;struct page&quot; (either
 * it doesn&apos;t exist, or it exists but they don&apos;t want to touch it). In this
 * case, NULL is returned here. &quot;Normal&quot; mappings do have a struct page.
 *
 * There are 2 broad cases. Firstly, an architecture may define a pte_special()
 * pte bit, in which case this function is trivial. Secondly, an architecture
 * may not have a spare pte bit, which requires a more complicated scheme,
 * described below.
 *
 * A raw VM_PFNMAP mapping (ie. one that is not COWed) is always considered a
 * special mapping (even if there are underlying and valid &quot;struct pages&quot;).
 * COWed pages of a VM_PFNMAP are always normal.
 *
 * The way we recognize COWed pages within VM_PFNMAP mappings is through the
 * rules set up by &quot;remap_pfn_range()&quot;: the vma will have the VM_PFNMAP bit
 * set, and the vm_pgoff will point to the first PFN mapped: thus every special
 * mapping will always honor the rule
 *
 *	pfn_of_page == vma-&gt;vm_pgoff + ((addr - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT)
 *
 * And for normal mappings this is false.
 *
 * This restricts such mappings to be a linear translation from virtual address
 * to pfn. To get around this restriction, we allow arbitrary mappings so long
 * as the vma is not a COW mapping; in that case, we know that all ptes are
 * special (because none can have been COWed).
 *
 *
 * In order to support COW of arbitrary special mappings, we have VM_MIXEDMAP.
 *
 * VM_MIXEDMAP mappings can likewise contain memory with or without &quot;struct
 * page&quot; backing, however the difference is that _all_ pages with a struct
 * page (that is, those where pfn_valid is true) are refcounted and considered
 * normal pages by the VM. The disadvantage is that pages are refcounted
 * (which can be slower and simply not an option for some PFNMAP users). The
 * advantage is that we don&apos;t have to follow the strict linearity rule of
 * PFNMAP mappings in order to support COWable mappings.
 *
 */</doc>
<use f='linux-4.18.y/mm/mlock.c' l='399' macro='1' u='c'/>
<use f='linux-4.18.y/mm/mprotect.c' l='84' macro='1' u='c'/>
