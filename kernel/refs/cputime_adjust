<dec f='linux-4.18.y/include/linux/sched/cputime.h' l='57' type='void cputime_adjust(struct task_cputime * curr, struct prev_cputime * prev, u64 * ut, u64 * st)'/>
<use f='linux-4.18.y/kernel/cgroup/rstat.c' l='405' u='c' c='cgroup_base_stat_cputime_show'/>
<def f='linux-4.18.y/kernel/sched/cputime.c' l='590' ll='660' type='void cputime_adjust(struct task_cputime * curr, struct prev_cputime * prev, u64 * ut, u64 * st)'/>
<use f='linux-4.18.y/kernel/sched/cputime.c' l='669' u='c' c='task_cputime_adjusted'/>
<use f='linux-4.18.y/kernel/sched/cputime.c' l='678' u='c' c='thread_group_cputime_adjusted'/>
<doc f='linux-4.18.y/kernel/sched/cputime.c' l='570'>/*
 * Adjust tick based cputime random precision against scheduler runtime
 * accounting.
 *
 * Tick based cputime accounting depend on random scheduling timeslices of a
 * task to be interrupted or not by the timer.  Depending on these
 * circumstances, the number of these interrupts may be over or
 * under-optimistic, matching the real user and system cputime with a variable
 * precision.
 *
 * Fix this by scaling these tick based values against the total runtime
 * accounted by the CFS scheduler.
 *
 * This code provides the following guarantees:
 *
 *   stime + utime == rtime
 *   stime_i+1 &gt;= stime_i, utime_i+1 &gt;= utime_i
 *
 * Assuming that rtime_i+1 &gt;= rtime_i.
 */</doc>
