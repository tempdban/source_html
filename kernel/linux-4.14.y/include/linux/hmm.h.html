<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>hmm.h source code [linux-4.14.y/include/linux/hmm.h] - Woboq Code Browser</title>
<link rel="stylesheet" href="../../../../data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="../../../../data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="../../../../data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="../../../../data/jquery/jquery-ui.min.js"></script>
<script>var file = 'linux-4.14.y/include/linux/hmm.h'; var root_path = '../../..'; var data_path = '../../../../data';</script>
<script src='../../../../data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../..'>linux-4.14.y</a>/<a href='..'>include</a>/<a href='./'>linux</a>/<a href='hmm.h.html'>hmm.h</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>/*</i></td></tr>
<tr><th id="2">2</th><td><i> * Copyright 2013 Red Hat Inc.</i></td></tr>
<tr><th id="3">3</th><td><i> *</i></td></tr>
<tr><th id="4">4</th><td><i> * This program is free software; you can redistribute it and/or modify</i></td></tr>
<tr><th id="5">5</th><td><i> * it under the terms of the GNU General Public License as published by</i></td></tr>
<tr><th id="6">6</th><td><i> * the Free Software Foundation; either version 2 of the License, or</i></td></tr>
<tr><th id="7">7</th><td><i> * (at your option) any later version.</i></td></tr>
<tr><th id="8">8</th><td><i> *</i></td></tr>
<tr><th id="9">9</th><td><i> * This program is distributed in the hope that it will be useful,</i></td></tr>
<tr><th id="10">10</th><td><i> * but WITHOUT ANY WARRANTY; without even the implied warranty of</i></td></tr>
<tr><th id="11">11</th><td><i> * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</i></td></tr>
<tr><th id="12">12</th><td><i> * GNU General Public License for more details.</i></td></tr>
<tr><th id="13">13</th><td><i> *</i></td></tr>
<tr><th id="14">14</th><td><i> * Authors: JÃ©rÃ´me Glisse &lt;jglisse@redhat.com&gt;</i></td></tr>
<tr><th id="15">15</th><td><i> */</i></td></tr>
<tr><th id="16">16</th><td><i>/*</i></td></tr>
<tr><th id="17">17</th><td><i> * Heterogeneous Memory Management (HMM)</i></td></tr>
<tr><th id="18">18</th><td><i> *</i></td></tr>
<tr><th id="19">19</th><td><i> * See Documentation/vm/hmm.txt for reasons and overview of what HMM is and it</i></td></tr>
<tr><th id="20">20</th><td><i> * is for. Here we focus on the HMM API description, with some explanation of</i></td></tr>
<tr><th id="21">21</th><td><i> * the underlying implementation.</i></td></tr>
<tr><th id="22">22</th><td><i> *</i></td></tr>
<tr><th id="23">23</th><td><i> * Short description: HMM provides a set of helpers to share a virtual address</i></td></tr>
<tr><th id="24">24</th><td><i> * space between CPU and a device, so that the device can access any valid</i></td></tr>
<tr><th id="25">25</th><td><i> * address of the process (while still obeying memory protection). HMM also</i></td></tr>
<tr><th id="26">26</th><td><i> * provides helpers to migrate process memory to device memory, and back. Each</i></td></tr>
<tr><th id="27">27</th><td><i> * set of functionality (address space mirroring, and migration to and from</i></td></tr>
<tr><th id="28">28</th><td><i> * device memory) can be used independently of the other.</i></td></tr>
<tr><th id="29">29</th><td><i> *</i></td></tr>
<tr><th id="30">30</th><td><i> *</i></td></tr>
<tr><th id="31">31</th><td><i> * HMM address space mirroring API:</i></td></tr>
<tr><th id="32">32</th><td><i> *</i></td></tr>
<tr><th id="33">33</th><td><i> * Use HMM address space mirroring if you want to mirror range of the CPU page</i></td></tr>
<tr><th id="34">34</th><td><i> * table of a process into a device page table. Here, "mirror" means "keep</i></td></tr>
<tr><th id="35">35</th><td><i> * synchronized". Prerequisites: the device must provide the ability to write-</i></td></tr>
<tr><th id="36">36</th><td><i> * protect its page tables (at PAGE_SIZE granularity), and must be able to</i></td></tr>
<tr><th id="37">37</th><td><i> * recover from the resulting potential page faults.</i></td></tr>
<tr><th id="38">38</th><td><i> *</i></td></tr>
<tr><th id="39">39</th><td><i> * HMM guarantees that at any point in time, a given virtual address points to</i></td></tr>
<tr><th id="40">40</th><td><i> * either the same memory in both CPU and device page tables (that is: CPU and</i></td></tr>
<tr><th id="41">41</th><td><i> * device page tables each point to the same pages), or that one page table (CPU</i></td></tr>
<tr><th id="42">42</th><td><i> * or device) points to no entry, while the other still points to the old page</i></td></tr>
<tr><th id="43">43</th><td><i> * for the address. The latter case happens when the CPU page table update</i></td></tr>
<tr><th id="44">44</th><td><i> * happens first, and then the update is mirrored over to the device page table.</i></td></tr>
<tr><th id="45">45</th><td><i> * This does not cause any issue, because the CPU page table cannot start</i></td></tr>
<tr><th id="46">46</th><td><i> * pointing to a new page until the device page table is invalidated.</i></td></tr>
<tr><th id="47">47</th><td><i> *</i></td></tr>
<tr><th id="48">48</th><td><i> * HMM uses mmu_notifiers to monitor the CPU page tables, and forwards any</i></td></tr>
<tr><th id="49">49</th><td><i> * updates to each device driver that has registered a mirror. It also provides</i></td></tr>
<tr><th id="50">50</th><td><i> * some API calls to help with taking a snapshot of the CPU page table, and to</i></td></tr>
<tr><th id="51">51</th><td><i> * synchronize with any updates that might happen concurrently.</i></td></tr>
<tr><th id="52">52</th><td><i> *</i></td></tr>
<tr><th id="53">53</th><td><i> *</i></td></tr>
<tr><th id="54">54</th><td><i> * HMM migration to and from device memory:</i></td></tr>
<tr><th id="55">55</th><td><i> *</i></td></tr>
<tr><th id="56">56</th><td><i> * HMM provides a set of helpers to hotplug device memory as ZONE_DEVICE, with</i></td></tr>
<tr><th id="57">57</th><td><i> * a new MEMORY_DEVICE_PRIVATE type. This provides a struct page for each page</i></td></tr>
<tr><th id="58">58</th><td><i> * of the device memory, and allows the device driver to manage its memory</i></td></tr>
<tr><th id="59">59</th><td><i> * using those struct pages. Having struct pages for device memory makes</i></td></tr>
<tr><th id="60">60</th><td><i> * migration easier. Because that memory is not addressable by the CPU it must</i></td></tr>
<tr><th id="61">61</th><td><i> * never be pinned to the device; in other words, any CPU page fault can always</i></td></tr>
<tr><th id="62">62</th><td><i> * cause the device memory to be migrated (copied/moved) back to regular memory.</i></td></tr>
<tr><th id="63">63</th><td><i> *</i></td></tr>
<tr><th id="64">64</th><td><i> * A new migrate helper (migrate_vma()) has been added (see mm/migrate.c) that</i></td></tr>
<tr><th id="65">65</th><td><i> * allows use of a device DMA engine to perform the copy operation between</i></td></tr>
<tr><th id="66">66</th><td><i> * regular system memory and device memory.</i></td></tr>
<tr><th id="67">67</th><td><i> */</i></td></tr>
<tr><th id="68">68</th><td><u>#<span data-ppcond="68">ifndef</span> <span class="macro" data-ref="_M/LINUX_HMM_H">LINUX_HMM_H</span></u></td></tr>
<tr><th id="69">69</th><td><u>#define <dfn class="macro" id="_M/LINUX_HMM_H" data-ref="_M/LINUX_HMM_H">LINUX_HMM_H</dfn></u></td></tr>
<tr><th id="70">70</th><td></td></tr>
<tr><th id="71">71</th><td><u>#include <a href="kconfig.h.html">&lt;linux/kconfig.h&gt;</a></u></td></tr>
<tr><th id="72">72</th><td></td></tr>
<tr><th id="73">73</th><td><u>#<span data-ppcond="73">if</span> <a class="macro" href="kconfig.h.html#71" title="0" data-ref="_M/IS_ENABLED">IS_ENABLED</a>(CONFIG_HMM)</u></td></tr>
<tr><th id="74">74</th><td></td></tr>
<tr><th id="75">75</th><td><u>#include &lt;linux/device.h&gt;</u></td></tr>
<tr><th id="76">76</th><td><u>#include &lt;linux/migrate.h&gt;</u></td></tr>
<tr><th id="77">77</th><td><u>#include &lt;linux/memremap.h&gt;</u></td></tr>
<tr><th id="78">78</th><td><u>#include &lt;linux/completion.h&gt;</u></td></tr>
<tr><th id="79">79</th><td></td></tr>
<tr><th id="80">80</th><td><b>struct</b> hmm;</td></tr>
<tr><th id="81">81</th><td></td></tr>
<tr><th id="82">82</th><td><i>/*</i></td></tr>
<tr><th id="83">83</th><td><i> * hmm_pfn_t - HMM uses its own pfn type to keep several flags per page</i></td></tr>
<tr><th id="84">84</th><td><i> *</i></td></tr>
<tr><th id="85">85</th><td><i> * Flags:</i></td></tr>
<tr><th id="86">86</th><td><i> * HMM_PFN_VALID: pfn is valid</i></td></tr>
<tr><th id="87">87</th><td><i> * HMM_PFN_READ:  CPU page table has read permission set</i></td></tr>
<tr><th id="88">88</th><td><i> * HMM_PFN_WRITE: CPU page table has write permission set</i></td></tr>
<tr><th id="89">89</th><td><i> * HMM_PFN_ERROR: corresponding CPU page table entry points to poisoned memory</i></td></tr>
<tr><th id="90">90</th><td><i> * HMM_PFN_EMPTY: corresponding CPU page table entry is pte_none()</i></td></tr>
<tr><th id="91">91</th><td><i> * HMM_PFN_SPECIAL: corresponding CPU page table entry is special; i.e., the</i></td></tr>
<tr><th id="92">92</th><td><i> *      result of vm_insert_pfn() or vm_insert_page(). Therefore, it should not</i></td></tr>
<tr><th id="93">93</th><td><i> *      be mirrored by a device, because the entry will never have HMM_PFN_VALID</i></td></tr>
<tr><th id="94">94</th><td><i> *      set and the pfn value is undefined.</i></td></tr>
<tr><th id="95">95</th><td><i> * HMM_PFN_DEVICE_UNADDRESSABLE: unaddressable device memory (ZONE_DEVICE)</i></td></tr>
<tr><th id="96">96</th><td><i> */</i></td></tr>
<tr><th id="97">97</th><td><b>typedef</b> <em>unsigned</em> <em>long</em> hmm_pfn_t;</td></tr>
<tr><th id="98">98</th><td></td></tr>
<tr><th id="99">99</th><td><u>#define HMM_PFN_VALID (1 &lt;&lt; 0)</u></td></tr>
<tr><th id="100">100</th><td><u>#define HMM_PFN_READ (1 &lt;&lt; 1)</u></td></tr>
<tr><th id="101">101</th><td><u>#define HMM_PFN_WRITE (1 &lt;&lt; 2)</u></td></tr>
<tr><th id="102">102</th><td><u>#define HMM_PFN_ERROR (1 &lt;&lt; 3)</u></td></tr>
<tr><th id="103">103</th><td><u>#define HMM_PFN_EMPTY (1 &lt;&lt; 4)</u></td></tr>
<tr><th id="104">104</th><td><u>#define HMM_PFN_SPECIAL (1 &lt;&lt; 5)</u></td></tr>
<tr><th id="105">105</th><td><u>#define HMM_PFN_DEVICE_UNADDRESSABLE (1 &lt;&lt; 6)</u></td></tr>
<tr><th id="106">106</th><td><u>#define HMM_PFN_SHIFT 7</u></td></tr>
<tr><th id="107">107</th><td></td></tr>
<tr><th id="108">108</th><td><i>/*</i></td></tr>
<tr><th id="109">109</th><td><i> * hmm_pfn_t_to_page() - return struct page pointed to by a valid hmm_pfn_t</i></td></tr>
<tr><th id="110">110</th><td><i> * @pfn: hmm_pfn_t to convert to struct page</i></td></tr>
<tr><th id="111">111</th><td><i> * Returns: struct page pointer if pfn is a valid hmm_pfn_t, NULL otherwise</i></td></tr>
<tr><th id="112">112</th><td><i> *</i></td></tr>
<tr><th id="113">113</th><td><i> * If the hmm_pfn_t is valid (ie valid flag set) then return the struct page</i></td></tr>
<tr><th id="114">114</th><td><i> * matching the pfn value stored in the hmm_pfn_t. Otherwise return NULL.</i></td></tr>
<tr><th id="115">115</th><td><i> */</i></td></tr>
<tr><th id="116">116</th><td><em>static</em> <b>inline</b> <b>struct</b> page *hmm_pfn_t_to_page(hmm_pfn_t pfn)</td></tr>
<tr><th id="117">117</th><td>{</td></tr>
<tr><th id="118">118</th><td>	<b>if</b> (!(pfn &amp; HMM_PFN_VALID))</td></tr>
<tr><th id="119">119</th><td>		<b>return</b> NULL;</td></tr>
<tr><th id="120">120</th><td>	<b>return</b> pfn_to_page(pfn &gt;&gt; HMM_PFN_SHIFT);</td></tr>
<tr><th id="121">121</th><td>}</td></tr>
<tr><th id="122">122</th><td></td></tr>
<tr><th id="123">123</th><td><i>/*</i></td></tr>
<tr><th id="124">124</th><td><i> * hmm_pfn_t_to_pfn() - return pfn value store in a hmm_pfn_t</i></td></tr>
<tr><th id="125">125</th><td><i> * @pfn: hmm_pfn_t to extract pfn from</i></td></tr>
<tr><th id="126">126</th><td><i> * Returns: pfn value if hmm_pfn_t is valid, -1UL otherwise</i></td></tr>
<tr><th id="127">127</th><td><i> */</i></td></tr>
<tr><th id="128">128</th><td><em>static</em> <b>inline</b> <em>unsigned</em> <em>long</em> hmm_pfn_t_to_pfn(hmm_pfn_t pfn)</td></tr>
<tr><th id="129">129</th><td>{</td></tr>
<tr><th id="130">130</th><td>	<b>if</b> (!(pfn &amp; HMM_PFN_VALID))</td></tr>
<tr><th id="131">131</th><td>		<b>return</b> -<var>1UL</var>;</td></tr>
<tr><th id="132">132</th><td>	<b>return</b> (pfn &gt;&gt; HMM_PFN_SHIFT);</td></tr>
<tr><th id="133">133</th><td>}</td></tr>
<tr><th id="134">134</th><td></td></tr>
<tr><th id="135">135</th><td><i>/*</i></td></tr>
<tr><th id="136">136</th><td><i> * hmm_pfn_t_from_page() - create a valid hmm_pfn_t value from struct page</i></td></tr>
<tr><th id="137">137</th><td><i> * @page: struct page pointer for which to create the hmm_pfn_t</i></td></tr>
<tr><th id="138">138</th><td><i> * Returns: valid hmm_pfn_t for the page</i></td></tr>
<tr><th id="139">139</th><td><i> */</i></td></tr>
<tr><th id="140">140</th><td><em>static</em> <b>inline</b> hmm_pfn_t hmm_pfn_t_from_page(<b>struct</b> page *page)</td></tr>
<tr><th id="141">141</th><td>{</td></tr>
<tr><th id="142">142</th><td>	<b>return</b> (page_to_pfn(page) &lt;&lt; HMM_PFN_SHIFT) | HMM_PFN_VALID;</td></tr>
<tr><th id="143">143</th><td>}</td></tr>
<tr><th id="144">144</th><td></td></tr>
<tr><th id="145">145</th><td><i>/*</i></td></tr>
<tr><th id="146">146</th><td><i> * hmm_pfn_t_from_pfn() - create a valid hmm_pfn_t value from pfn</i></td></tr>
<tr><th id="147">147</th><td><i> * @pfn: pfn value for which to create the hmm_pfn_t</i></td></tr>
<tr><th id="148">148</th><td><i> * Returns: valid hmm_pfn_t for the pfn</i></td></tr>
<tr><th id="149">149</th><td><i> */</i></td></tr>
<tr><th id="150">150</th><td><em>static</em> <b>inline</b> hmm_pfn_t hmm_pfn_t_from_pfn(<em>unsigned</em> <em>long</em> pfn)</td></tr>
<tr><th id="151">151</th><td>{</td></tr>
<tr><th id="152">152</th><td>	<b>return</b> (pfn &lt;&lt; HMM_PFN_SHIFT) | HMM_PFN_VALID;</td></tr>
<tr><th id="153">153</th><td>}</td></tr>
<tr><th id="154">154</th><td></td></tr>
<tr><th id="155">155</th><td></td></tr>
<tr><th id="156">156</th><td><u>#if IS_ENABLED(CONFIG_HMM_MIRROR)</u></td></tr>
<tr><th id="157">157</th><td><i>/*</i></td></tr>
<tr><th id="158">158</th><td><i> * Mirroring: how to synchronize device page table with CPU page table.</i></td></tr>
<tr><th id="159">159</th><td><i> *</i></td></tr>
<tr><th id="160">160</th><td><i> * A device driver that is participating in HMM mirroring must always</i></td></tr>
<tr><th id="161">161</th><td><i> * synchronize with CPU page table updates. For this, device drivers can either</i></td></tr>
<tr><th id="162">162</th><td><i> * directly use mmu_notifier APIs or they can use the hmm_mirror API. Device</i></td></tr>
<tr><th id="163">163</th><td><i> * drivers can decide to register one mirror per device per process, or just</i></td></tr>
<tr><th id="164">164</th><td><i> * one mirror per process for a group of devices. The pattern is:</i></td></tr>
<tr><th id="165">165</th><td><i> *</i></td></tr>
<tr><th id="166">166</th><td><i> *      int device_bind_address_space(..., struct mm_struct *mm, ...)</i></td></tr>
<tr><th id="167">167</th><td><i> *      {</i></td></tr>
<tr><th id="168">168</th><td><i> *          struct device_address_space *das;</i></td></tr>
<tr><th id="169">169</th><td><i> *</i></td></tr>
<tr><th id="170">170</th><td><i> *          // Device driver specific initialization, and allocation of das</i></td></tr>
<tr><th id="171">171</th><td><i> *          // which contains an hmm_mirror struct as one of its fields.</i></td></tr>
<tr><th id="172">172</th><td><i> *          ...</i></td></tr>
<tr><th id="173">173</th><td><i> *</i></td></tr>
<tr><th id="174">174</th><td><i> *          ret = hmm_mirror_register(&amp;das-&gt;mirror, mm, &amp;device_mirror_ops);</i></td></tr>
<tr><th id="175">175</th><td><i> *          if (ret) {</i></td></tr>
<tr><th id="176">176</th><td><i> *              // Cleanup on error</i></td></tr>
<tr><th id="177">177</th><td><i> *              return ret;</i></td></tr>
<tr><th id="178">178</th><td><i> *          }</i></td></tr>
<tr><th id="179">179</th><td><i> *</i></td></tr>
<tr><th id="180">180</th><td><i> *          // Other device driver specific initialization</i></td></tr>
<tr><th id="181">181</th><td><i> *          ...</i></td></tr>
<tr><th id="182">182</th><td><i> *      }</i></td></tr>
<tr><th id="183">183</th><td><i> *</i></td></tr>
<tr><th id="184">184</th><td><i> * Once an hmm_mirror is registered for an address space, the device driver</i></td></tr>
<tr><th id="185">185</th><td><i> * will get callbacks through sync_cpu_device_pagetables() operation (see</i></td></tr>
<tr><th id="186">186</th><td><i> * hmm_mirror_ops struct).</i></td></tr>
<tr><th id="187">187</th><td><i> *</i></td></tr>
<tr><th id="188">188</th><td><i> * Device driver must not free the struct containing the hmm_mirror struct</i></td></tr>
<tr><th id="189">189</th><td><i> * before calling hmm_mirror_unregister(). The expected usage is to do that when</i></td></tr>
<tr><th id="190">190</th><td><i> * the device driver is unbinding from an address space.</i></td></tr>
<tr><th id="191">191</th><td><i> *</i></td></tr>
<tr><th id="192">192</th><td><i> *</i></td></tr>
<tr><th id="193">193</th><td><i> *      void device_unbind_address_space(struct device_address_space *das)</i></td></tr>
<tr><th id="194">194</th><td><i> *      {</i></td></tr>
<tr><th id="195">195</th><td><i> *          // Device driver specific cleanup</i></td></tr>
<tr><th id="196">196</th><td><i> *          ...</i></td></tr>
<tr><th id="197">197</th><td><i> *</i></td></tr>
<tr><th id="198">198</th><td><i> *          hmm_mirror_unregister(&amp;das-&gt;mirror);</i></td></tr>
<tr><th id="199">199</th><td><i> *</i></td></tr>
<tr><th id="200">200</th><td><i> *          // Other device driver specific cleanup, and now das can be freed</i></td></tr>
<tr><th id="201">201</th><td><i> *          ...</i></td></tr>
<tr><th id="202">202</th><td><i> *      }</i></td></tr>
<tr><th id="203">203</th><td><i> */</i></td></tr>
<tr><th id="204">204</th><td></td></tr>
<tr><th id="205">205</th><td><b>struct</b> hmm_mirror;</td></tr>
<tr><th id="206">206</th><td></td></tr>
<tr><th id="207">207</th><td><i>/*</i></td></tr>
<tr><th id="208">208</th><td><i> * enum hmm_update_type - type of update</i></td></tr>
<tr><th id="209">209</th><td><i> * @HMM_UPDATE_INVALIDATE: invalidate range (no indication as to why)</i></td></tr>
<tr><th id="210">210</th><td><i> */</i></td></tr>
<tr><th id="211">211</th><td><b>enum</b> hmm_update_type {</td></tr>
<tr><th id="212">212</th><td>	HMM_UPDATE_INVALIDATE,</td></tr>
<tr><th id="213">213</th><td>};</td></tr>
<tr><th id="214">214</th><td></td></tr>
<tr><th id="215">215</th><td><i>/*</i></td></tr>
<tr><th id="216">216</th><td><i> * struct hmm_mirror_ops - HMM mirror device operations callback</i></td></tr>
<tr><th id="217">217</th><td><i> *</i></td></tr>
<tr><th id="218">218</th><td><i> * @update: callback to update range on a device</i></td></tr>
<tr><th id="219">219</th><td><i> */</i></td></tr>
<tr><th id="220">220</th><td><b>struct</b> hmm_mirror_ops {</td></tr>
<tr><th id="221">221</th><td>	<i>/* sync_cpu_device_pagetables() - synchronize page tables</i></td></tr>
<tr><th id="222">222</th><td><i>	 *</i></td></tr>
<tr><th id="223">223</th><td><i>	 * @mirror: pointer to struct hmm_mirror</i></td></tr>
<tr><th id="224">224</th><td><i>	 * @update_type: type of update that occurred to the CPU page table</i></td></tr>
<tr><th id="225">225</th><td><i>	 * @start: virtual start address of the range to update</i></td></tr>
<tr><th id="226">226</th><td><i>	 * @end: virtual end address of the range to update</i></td></tr>
<tr><th id="227">227</th><td><i>	 *</i></td></tr>
<tr><th id="228">228</th><td><i>	 * This callback ultimately originates from mmu_notifiers when the CPU</i></td></tr>
<tr><th id="229">229</th><td><i>	 * page table is updated. The device driver must update its page table</i></td></tr>
<tr><th id="230">230</th><td><i>	 * in response to this callback. The update argument tells what action</i></td></tr>
<tr><th id="231">231</th><td><i>	 * to perform.</i></td></tr>
<tr><th id="232">232</th><td><i>	 *</i></td></tr>
<tr><th id="233">233</th><td><i>	 * The device driver must not return from this callback until the device</i></td></tr>
<tr><th id="234">234</th><td><i>	 * page tables are completely updated (TLBs flushed, etc); this is a</i></td></tr>
<tr><th id="235">235</th><td><i>	 * synchronous call.</i></td></tr>
<tr><th id="236">236</th><td><i>	 */</i></td></tr>
<tr><th id="237">237</th><td>	<em>void</em> (*sync_cpu_device_pagetables)(<b>struct</b> hmm_mirror *mirror,</td></tr>
<tr><th id="238">238</th><td>					   <b>enum</b> hmm_update_type update_type,</td></tr>
<tr><th id="239">239</th><td>					   <em>unsigned</em> <em>long</em> start,</td></tr>
<tr><th id="240">240</th><td>					   <em>unsigned</em> <em>long</em> end);</td></tr>
<tr><th id="241">241</th><td>};</td></tr>
<tr><th id="242">242</th><td></td></tr>
<tr><th id="243">243</th><td><i>/*</i></td></tr>
<tr><th id="244">244</th><td><i> * struct hmm_mirror - mirror struct for a device driver</i></td></tr>
<tr><th id="245">245</th><td><i> *</i></td></tr>
<tr><th id="246">246</th><td><i> * @hmm: pointer to struct hmm (which is unique per mm_struct)</i></td></tr>
<tr><th id="247">247</th><td><i> * @ops: device driver callback for HMM mirror operations</i></td></tr>
<tr><th id="248">248</th><td><i> * @list: for list of mirrors of a given mm</i></td></tr>
<tr><th id="249">249</th><td><i> *</i></td></tr>
<tr><th id="250">250</th><td><i> * Each address space (mm_struct) being mirrored by a device must register one</i></td></tr>
<tr><th id="251">251</th><td><i> * instance of an hmm_mirror struct with HMM. HMM will track the list of all</i></td></tr>
<tr><th id="252">252</th><td><i> * mirrors for each mm_struct.</i></td></tr>
<tr><th id="253">253</th><td><i> */</i></td></tr>
<tr><th id="254">254</th><td><b>struct</b> hmm_mirror {</td></tr>
<tr><th id="255">255</th><td>	<b>struct</b> hmm			*hmm;</td></tr>
<tr><th id="256">256</th><td>	<em>const</em> <b>struct</b> hmm_mirror_ops	*ops;</td></tr>
<tr><th id="257">257</th><td>	<b>struct</b> list_head		list;</td></tr>
<tr><th id="258">258</th><td>};</td></tr>
<tr><th id="259">259</th><td></td></tr>
<tr><th id="260">260</th><td><em>int</em> hmm_mirror_register(<b>struct</b> hmm_mirror *mirror, <b>struct</b> mm_struct *mm);</td></tr>
<tr><th id="261">261</th><td><em>void</em> hmm_mirror_unregister(<b>struct</b> hmm_mirror *mirror);</td></tr>
<tr><th id="262">262</th><td></td></tr>
<tr><th id="263">263</th><td></td></tr>
<tr><th id="264">264</th><td><i>/*</i></td></tr>
<tr><th id="265">265</th><td><i> * struct hmm_range - track invalidation lock on virtual address range</i></td></tr>
<tr><th id="266">266</th><td><i> *</i></td></tr>
<tr><th id="267">267</th><td><i> * @list: all range lock are on a list</i></td></tr>
<tr><th id="268">268</th><td><i> * @start: range virtual start address (inclusive)</i></td></tr>
<tr><th id="269">269</th><td><i> * @end: range virtual end address (exclusive)</i></td></tr>
<tr><th id="270">270</th><td><i> * @pfns: array of pfns (big enough for the range)</i></td></tr>
<tr><th id="271">271</th><td><i> * @valid: pfns array did not change since it has been fill by an HMM function</i></td></tr>
<tr><th id="272">272</th><td><i> */</i></td></tr>
<tr><th id="273">273</th><td><b>struct</b> hmm_range {</td></tr>
<tr><th id="274">274</th><td>	<b>struct</b> list_head	list;</td></tr>
<tr><th id="275">275</th><td>	<em>unsigned</em> <em>long</em>		start;</td></tr>
<tr><th id="276">276</th><td>	<em>unsigned</em> <em>long</em>		end;</td></tr>
<tr><th id="277">277</th><td>	hmm_pfn_t		*pfns;</td></tr>
<tr><th id="278">278</th><td>	bool			valid;</td></tr>
<tr><th id="279">279</th><td>};</td></tr>
<tr><th id="280">280</th><td></td></tr>
<tr><th id="281">281</th><td><i>/*</i></td></tr>
<tr><th id="282">282</th><td><i> * To snapshot the CPU page table, call hmm_vma_get_pfns(), then take a device</i></td></tr>
<tr><th id="283">283</th><td><i> * driver lock that serializes device page table updates, then call</i></td></tr>
<tr><th id="284">284</th><td><i> * hmm_vma_range_done(), to check if the snapshot is still valid. The same</i></td></tr>
<tr><th id="285">285</th><td><i> * device driver page table update lock must also be used in the</i></td></tr>
<tr><th id="286">286</th><td><i> * hmm_mirror_ops.sync_cpu_device_pagetables() callback, so that CPU page</i></td></tr>
<tr><th id="287">287</th><td><i> * table invalidation serializes on it.</i></td></tr>
<tr><th id="288">288</th><td><i> *</i></td></tr>
<tr><th id="289">289</th><td><i> * YOU MUST CALL hmm_vma_range_done() ONCE AND ONLY ONCE EACH TIME YOU CALL</i></td></tr>
<tr><th id="290">290</th><td><i> * hmm_vma_get_pfns() WITHOUT ERROR !</i></td></tr>
<tr><th id="291">291</th><td><i> *</i></td></tr>
<tr><th id="292">292</th><td><i> * IF YOU DO NOT FOLLOW THE ABOVE RULE THE SNAPSHOT CONTENT MIGHT BE INVALID !</i></td></tr>
<tr><th id="293">293</th><td><i> */</i></td></tr>
<tr><th id="294">294</th><td><em>int</em> hmm_vma_get_pfns(<b>struct</b> vm_area_struct *vma,</td></tr>
<tr><th id="295">295</th><td>		     <b>struct</b> hmm_range *range,</td></tr>
<tr><th id="296">296</th><td>		     <em>unsigned</em> <em>long</em> start,</td></tr>
<tr><th id="297">297</th><td>		     <em>unsigned</em> <em>long</em> end,</td></tr>
<tr><th id="298">298</th><td>		     hmm_pfn_t *pfns);</td></tr>
<tr><th id="299">299</th><td>bool hmm_vma_range_done(<b>struct</b> vm_area_struct *vma, <b>struct</b> hmm_range *range);</td></tr>
<tr><th id="300">300</th><td></td></tr>
<tr><th id="301">301</th><td></td></tr>
<tr><th id="302">302</th><td><i>/*</i></td></tr>
<tr><th id="303">303</th><td><i> * Fault memory on behalf of device driver. Unlike handle_mm_fault(), this will</i></td></tr>
<tr><th id="304">304</th><td><i> * not migrate any device memory back to system memory. The hmm_pfn_t array will</i></td></tr>
<tr><th id="305">305</th><td><i> * be updated with the fault result and current snapshot of the CPU page table</i></td></tr>
<tr><th id="306">306</th><td><i> * for the range.</i></td></tr>
<tr><th id="307">307</th><td><i> *</i></td></tr>
<tr><th id="308">308</th><td><i> * The mmap_sem must be taken in read mode before entering and it might be</i></td></tr>
<tr><th id="309">309</th><td><i> * dropped by the function if the block argument is false. In that case, the</i></td></tr>
<tr><th id="310">310</th><td><i> * function returns -EAGAIN.</i></td></tr>
<tr><th id="311">311</th><td><i> *</i></td></tr>
<tr><th id="312">312</th><td><i> * Return value does not reflect if the fault was successful for every single</i></td></tr>
<tr><th id="313">313</th><td><i> * address or not. Therefore, the caller must to inspect the hmm_pfn_t array to</i></td></tr>
<tr><th id="314">314</th><td><i> * determine fault status for each address.</i></td></tr>
<tr><th id="315">315</th><td><i> *</i></td></tr>
<tr><th id="316">316</th><td><i> * Trying to fault inside an invalid vma will result in -EINVAL.</i></td></tr>
<tr><th id="317">317</th><td><i> *</i></td></tr>
<tr><th id="318">318</th><td><i> * See the function description in mm/hmm.c for further documentation.</i></td></tr>
<tr><th id="319">319</th><td><i> */</i></td></tr>
<tr><th id="320">320</th><td><em>int</em> hmm_vma_fault(<b>struct</b> vm_area_struct *vma,</td></tr>
<tr><th id="321">321</th><td>		  <b>struct</b> hmm_range *range,</td></tr>
<tr><th id="322">322</th><td>		  <em>unsigned</em> <em>long</em> start,</td></tr>
<tr><th id="323">323</th><td>		  <em>unsigned</em> <em>long</em> end,</td></tr>
<tr><th id="324">324</th><td>		  hmm_pfn_t *pfns,</td></tr>
<tr><th id="325">325</th><td>		  bool write,</td></tr>
<tr><th id="326">326</th><td>		  bool block);</td></tr>
<tr><th id="327">327</th><td><u>#endif /* IS_ENABLED(CONFIG_HMM_MIRROR) */</u></td></tr>
<tr><th id="328">328</th><td></td></tr>
<tr><th id="329">329</th><td></td></tr>
<tr><th id="330">330</th><td><u>#if IS_ENABLED(CONFIG_DEVICE_PRIVATE) ||  IS_ENABLED(CONFIG_DEVICE_PUBLIC)</u></td></tr>
<tr><th id="331">331</th><td><b>struct</b> hmm_devmem;</td></tr>
<tr><th id="332">332</th><td></td></tr>
<tr><th id="333">333</th><td><b>struct</b> page *hmm_vma_alloc_locked_page(<b>struct</b> vm_area_struct *vma,</td></tr>
<tr><th id="334">334</th><td>				       <em>unsigned</em> <em>long</em> addr);</td></tr>
<tr><th id="335">335</th><td></td></tr>
<tr><th id="336">336</th><td><i>/*</i></td></tr>
<tr><th id="337">337</th><td><i> * struct hmm_devmem_ops - callback for ZONE_DEVICE memory events</i></td></tr>
<tr><th id="338">338</th><td><i> *</i></td></tr>
<tr><th id="339">339</th><td><i> * @free: call when refcount on page reach 1 and thus is no longer use</i></td></tr>
<tr><th id="340">340</th><td><i> * @fault: call when there is a page fault to unaddressable memory</i></td></tr>
<tr><th id="341">341</th><td><i> *</i></td></tr>
<tr><th id="342">342</th><td><i> * Both callback happens from page_free() and page_fault() callback of struct</i></td></tr>
<tr><th id="343">343</th><td><i> * dev_pagemap respectively. See include/linux/memremap.h for more details on</i></td></tr>
<tr><th id="344">344</th><td><i> * those.</i></td></tr>
<tr><th id="345">345</th><td><i> *</i></td></tr>
<tr><th id="346">346</th><td><i> * The hmm_devmem_ops callback are just here to provide a coherent and</i></td></tr>
<tr><th id="347">347</th><td><i> * uniq API to device driver and device driver should not register their</i></td></tr>
<tr><th id="348">348</th><td><i> * own page_free() or page_fault() but rely on the hmm_devmem_ops call-</i></td></tr>
<tr><th id="349">349</th><td><i> * back.</i></td></tr>
<tr><th id="350">350</th><td><i> */</i></td></tr>
<tr><th id="351">351</th><td><b>struct</b> hmm_devmem_ops {</td></tr>
<tr><th id="352">352</th><td>	<i>/*</i></td></tr>
<tr><th id="353">353</th><td><i>	 * free() - free a device page</i></td></tr>
<tr><th id="354">354</th><td><i>	 * @devmem: device memory structure (see struct hmm_devmem)</i></td></tr>
<tr><th id="355">355</th><td><i>	 * @page: pointer to struct page being freed</i></td></tr>
<tr><th id="356">356</th><td><i>	 *</i></td></tr>
<tr><th id="357">357</th><td><i>	 * Call back occurs whenever a device page refcount reach 1 which</i></td></tr>
<tr><th id="358">358</th><td><i>	 * means that no one is holding any reference on the page anymore</i></td></tr>
<tr><th id="359">359</th><td><i>	 * (ZONE_DEVICE page have an elevated refcount of 1 as default so</i></td></tr>
<tr><th id="360">360</th><td><i>	 * that they are not release to the general page allocator).</i></td></tr>
<tr><th id="361">361</th><td><i>	 *</i></td></tr>
<tr><th id="362">362</th><td><i>	 * Note that callback has exclusive ownership of the page (as no</i></td></tr>
<tr><th id="363">363</th><td><i>	 * one is holding any reference).</i></td></tr>
<tr><th id="364">364</th><td><i>	 */</i></td></tr>
<tr><th id="365">365</th><td>	<em>void</em> (*free)(<b>struct</b> hmm_devmem *devmem, <b>struct</b> page *page);</td></tr>
<tr><th id="366">366</th><td>	<i>/*</i></td></tr>
<tr><th id="367">367</th><td><i>	 * fault() - CPU page fault or get user page (GUP)</i></td></tr>
<tr><th id="368">368</th><td><i>	 * @devmem: device memory structure (see struct hmm_devmem)</i></td></tr>
<tr><th id="369">369</th><td><i>	 * @vma: virtual memory area containing the virtual address</i></td></tr>
<tr><th id="370">370</th><td><i>	 * @addr: virtual address that faulted or for which there is a GUP</i></td></tr>
<tr><th id="371">371</th><td><i>	 * @page: pointer to struct page backing virtual address (unreliable)</i></td></tr>
<tr><th id="372">372</th><td><i>	 * @flags: FAULT_FLAG_* (see include/linux/mm.h)</i></td></tr>
<tr><th id="373">373</th><td><i>	 * @pmdp: page middle directory</i></td></tr>
<tr><th id="374">374</th><td><i>	 * Returns: VM_FAULT_MINOR/MAJOR on success or one of VM_FAULT_ERROR</i></td></tr>
<tr><th id="375">375</th><td><i>	 *   on error</i></td></tr>
<tr><th id="376">376</th><td><i>	 *</i></td></tr>
<tr><th id="377">377</th><td><i>	 * The callback occurs whenever there is a CPU page fault or GUP on a</i></td></tr>
<tr><th id="378">378</th><td><i>	 * virtual address. This means that the device driver must migrate the</i></td></tr>
<tr><th id="379">379</th><td><i>	 * page back to regular memory (CPU accessible).</i></td></tr>
<tr><th id="380">380</th><td><i>	 *</i></td></tr>
<tr><th id="381">381</th><td><i>	 * The device driver is free to migrate more than one page from the</i></td></tr>
<tr><th id="382">382</th><td><i>	 * fault() callback as an optimization. However if device decide to</i></td></tr>
<tr><th id="383">383</th><td><i>	 * migrate more than one page it must always priotirize the faulting</i></td></tr>
<tr><th id="384">384</th><td><i>	 * address over the others.</i></td></tr>
<tr><th id="385">385</th><td><i>	 *</i></td></tr>
<tr><th id="386">386</th><td><i>	 * The struct page pointer is only given as an hint to allow quick</i></td></tr>
<tr><th id="387">387</th><td><i>	 * lookup of internal device driver data. A concurrent migration</i></td></tr>
<tr><th id="388">388</th><td><i>	 * might have already free that page and the virtual address might</i></td></tr>
<tr><th id="389">389</th><td><i>	 * not longer be back by it. So it should not be modified by the</i></td></tr>
<tr><th id="390">390</th><td><i>	 * callback.</i></td></tr>
<tr><th id="391">391</th><td><i>	 *</i></td></tr>
<tr><th id="392">392</th><td><i>	 * Note that mmap semaphore is held in read mode at least when this</i></td></tr>
<tr><th id="393">393</th><td><i>	 * callback occurs, hence the vma is valid upon callback entry.</i></td></tr>
<tr><th id="394">394</th><td><i>	 */</i></td></tr>
<tr><th id="395">395</th><td>	<em>int</em> (*fault)(<b>struct</b> hmm_devmem *devmem,</td></tr>
<tr><th id="396">396</th><td>		     <b>struct</b> vm_area_struct *vma,</td></tr>
<tr><th id="397">397</th><td>		     <em>unsigned</em> <em>long</em> addr,</td></tr>
<tr><th id="398">398</th><td>		     <em>const</em> <b>struct</b> page *page,</td></tr>
<tr><th id="399">399</th><td>		     <em>unsigned</em> <em>int</em> flags,</td></tr>
<tr><th id="400">400</th><td>		     pmd_t *pmdp);</td></tr>
<tr><th id="401">401</th><td>};</td></tr>
<tr><th id="402">402</th><td></td></tr>
<tr><th id="403">403</th><td><i>/*</i></td></tr>
<tr><th id="404">404</th><td><i> * struct hmm_devmem - track device memory</i></td></tr>
<tr><th id="405">405</th><td><i> *</i></td></tr>
<tr><th id="406">406</th><td><i> * @completion: completion object for device memory</i></td></tr>
<tr><th id="407">407</th><td><i> * @pfn_first: first pfn for this resource (set by hmm_devmem_add())</i></td></tr>
<tr><th id="408">408</th><td><i> * @pfn_last: last pfn for this resource (set by hmm_devmem_add())</i></td></tr>
<tr><th id="409">409</th><td><i> * @resource: IO resource reserved for this chunk of memory</i></td></tr>
<tr><th id="410">410</th><td><i> * @pagemap: device page map for that chunk</i></td></tr>
<tr><th id="411">411</th><td><i> * @device: device to bind resource to</i></td></tr>
<tr><th id="412">412</th><td><i> * @ops: memory operations callback</i></td></tr>
<tr><th id="413">413</th><td><i> * @ref: per CPU refcount</i></td></tr>
<tr><th id="414">414</th><td><i> *</i></td></tr>
<tr><th id="415">415</th><td><i> * This an helper structure for device drivers that do not wish to implement</i></td></tr>
<tr><th id="416">416</th><td><i> * the gory details related to hotplugging new memoy and allocating struct</i></td></tr>
<tr><th id="417">417</th><td><i> * pages.</i></td></tr>
<tr><th id="418">418</th><td><i> *</i></td></tr>
<tr><th id="419">419</th><td><i> * Device drivers can directly use ZONE_DEVICE memory on their own if they</i></td></tr>
<tr><th id="420">420</th><td><i> * wish to do so.</i></td></tr>
<tr><th id="421">421</th><td><i> */</i></td></tr>
<tr><th id="422">422</th><td><b>struct</b> hmm_devmem {</td></tr>
<tr><th id="423">423</th><td>	<b>struct</b> completion		completion;</td></tr>
<tr><th id="424">424</th><td>	<em>unsigned</em> <em>long</em>			pfn_first;</td></tr>
<tr><th id="425">425</th><td>	<em>unsigned</em> <em>long</em>			pfn_last;</td></tr>
<tr><th id="426">426</th><td>	<b>struct</b> resource			*resource;</td></tr>
<tr><th id="427">427</th><td>	<b>struct</b> device			*device;</td></tr>
<tr><th id="428">428</th><td>	<b>struct</b> dev_pagemap		pagemap;</td></tr>
<tr><th id="429">429</th><td>	<em>const</em> <b>struct</b> hmm_devmem_ops	*ops;</td></tr>
<tr><th id="430">430</th><td>	<b>struct</b> percpu_ref		ref;</td></tr>
<tr><th id="431">431</th><td>};</td></tr>
<tr><th id="432">432</th><td></td></tr>
<tr><th id="433">433</th><td><i>/*</i></td></tr>
<tr><th id="434">434</th><td><i> * To add (hotplug) device memory, HMM assumes that there is no real resource</i></td></tr>
<tr><th id="435">435</th><td><i> * that reserves a range in the physical address space (this is intended to be</i></td></tr>
<tr><th id="436">436</th><td><i> * use by unaddressable device memory). It will reserve a physical range big</i></td></tr>
<tr><th id="437">437</th><td><i> * enough and allocate struct page for it.</i></td></tr>
<tr><th id="438">438</th><td><i> *</i></td></tr>
<tr><th id="439">439</th><td><i> * The device driver can wrap the hmm_devmem struct inside a private device</i></td></tr>
<tr><th id="440">440</th><td><i> * driver struct. The device driver must call hmm_devmem_remove() before the</i></td></tr>
<tr><th id="441">441</th><td><i> * device goes away and before freeing the hmm_devmem struct memory.</i></td></tr>
<tr><th id="442">442</th><td><i> */</i></td></tr>
<tr><th id="443">443</th><td><b>struct</b> hmm_devmem *hmm_devmem_add(<em>const</em> <b>struct</b> hmm_devmem_ops *ops,</td></tr>
<tr><th id="444">444</th><td>				  <b>struct</b> device *device,</td></tr>
<tr><th id="445">445</th><td>				  <em>unsigned</em> <em>long</em> size);</td></tr>
<tr><th id="446">446</th><td><b>struct</b> hmm_devmem *hmm_devmem_add_resource(<em>const</em> <b>struct</b> hmm_devmem_ops *ops,</td></tr>
<tr><th id="447">447</th><td>					   <b>struct</b> device *device,</td></tr>
<tr><th id="448">448</th><td>					   <b>struct</b> resource *res);</td></tr>
<tr><th id="449">449</th><td><em>void</em> hmm_devmem_remove(<b>struct</b> hmm_devmem *devmem);</td></tr>
<tr><th id="450">450</th><td></td></tr>
<tr><th id="451">451</th><td><i>/*</i></td></tr>
<tr><th id="452">452</th><td><i> * hmm_devmem_page_set_drvdata - set per-page driver data field</i></td></tr>
<tr><th id="453">453</th><td><i> *</i></td></tr>
<tr><th id="454">454</th><td><i> * @page: pointer to struct page</i></td></tr>
<tr><th id="455">455</th><td><i> * @data: driver data value to set</i></td></tr>
<tr><th id="456">456</th><td><i> *</i></td></tr>
<tr><th id="457">457</th><td><i> * Because page can not be on lru we have an unsigned long that driver can use</i></td></tr>
<tr><th id="458">458</th><td><i> * to store a per page field. This just a simple helper to do that.</i></td></tr>
<tr><th id="459">459</th><td><i> */</i></td></tr>
<tr><th id="460">460</th><td><em>static</em> <b>inline</b> <em>void</em> hmm_devmem_page_set_drvdata(<b>struct</b> page *page,</td></tr>
<tr><th id="461">461</th><td>					       <em>unsigned</em> <em>long</em> data)</td></tr>
<tr><th id="462">462</th><td>{</td></tr>
<tr><th id="463">463</th><td>	<em>unsigned</em> <em>long</em> *drvdata = (<em>unsigned</em> <em>long</em> *)&amp;page-&gt;pgmap;</td></tr>
<tr><th id="464">464</th><td></td></tr>
<tr><th id="465">465</th><td>	drvdata[<var>1</var>] = data;</td></tr>
<tr><th id="466">466</th><td>}</td></tr>
<tr><th id="467">467</th><td></td></tr>
<tr><th id="468">468</th><td><i>/*</i></td></tr>
<tr><th id="469">469</th><td><i> * hmm_devmem_page_get_drvdata - get per page driver data field</i></td></tr>
<tr><th id="470">470</th><td><i> *</i></td></tr>
<tr><th id="471">471</th><td><i> * @page: pointer to struct page</i></td></tr>
<tr><th id="472">472</th><td><i> * Return: driver data value</i></td></tr>
<tr><th id="473">473</th><td><i> */</i></td></tr>
<tr><th id="474">474</th><td><em>static</em> <b>inline</b> <em>unsigned</em> <em>long</em> hmm_devmem_page_get_drvdata(<b>struct</b> page *page)</td></tr>
<tr><th id="475">475</th><td>{</td></tr>
<tr><th id="476">476</th><td>	<em>unsigned</em> <em>long</em> *drvdata = (<em>unsigned</em> <em>long</em> *)&amp;page-&gt;pgmap;</td></tr>
<tr><th id="477">477</th><td></td></tr>
<tr><th id="478">478</th><td>	<b>return</b> drvdata[<var>1</var>];</td></tr>
<tr><th id="479">479</th><td>}</td></tr>
<tr><th id="480">480</th><td></td></tr>
<tr><th id="481">481</th><td></td></tr>
<tr><th id="482">482</th><td><i>/*</i></td></tr>
<tr><th id="483">483</th><td><i> * struct hmm_device - fake device to hang device memory onto</i></td></tr>
<tr><th id="484">484</th><td><i> *</i></td></tr>
<tr><th id="485">485</th><td><i> * @device: device struct</i></td></tr>
<tr><th id="486">486</th><td><i> * @minor: device minor number</i></td></tr>
<tr><th id="487">487</th><td><i> */</i></td></tr>
<tr><th id="488">488</th><td><b>struct</b> hmm_device {</td></tr>
<tr><th id="489">489</th><td>	<b>struct</b> device		device;</td></tr>
<tr><th id="490">490</th><td>	<em>unsigned</em> <em>int</em>		minor;</td></tr>
<tr><th id="491">491</th><td>};</td></tr>
<tr><th id="492">492</th><td></td></tr>
<tr><th id="493">493</th><td><i>/*</i></td></tr>
<tr><th id="494">494</th><td><i> * A device driver that wants to handle multiple devices memory through a</i></td></tr>
<tr><th id="495">495</th><td><i> * single fake device can use hmm_device to do so. This is purely a helper and</i></td></tr>
<tr><th id="496">496</th><td><i> * it is not strictly needed, in order to make use of any HMM functionality.</i></td></tr>
<tr><th id="497">497</th><td><i> */</i></td></tr>
<tr><th id="498">498</th><td><b>struct</b> hmm_device *hmm_device_new(<em>void</em> *drvdata);</td></tr>
<tr><th id="499">499</th><td><em>void</em> hmm_device_put(<b>struct</b> hmm_device *hmm_device);</td></tr>
<tr><th id="500">500</th><td><u>#endif /* CONFIG_DEVICE_PRIVATE || CONFIG_DEVICE_PUBLIC */</u></td></tr>
<tr><th id="501">501</th><td><u>#<span data-ppcond="73">endif</span> /* IS_ENABLED(CONFIG_HMM) */</u></td></tr>
<tr><th id="502">502</th><td></td></tr>
<tr><th id="503">503</th><td><i>/* Below are for HMM internal use only! Not to be used by device driver! */</i></td></tr>
<tr><th id="504">504</th><td><u>#<span data-ppcond="504">if</span> <a class="macro" href="kconfig.h.html#71" title="0" data-ref="_M/IS_ENABLED">IS_ENABLED</a>(CONFIG_HMM_MIRROR)</u></td></tr>
<tr><th id="505">505</th><td><em>void</em> hmm_mm_destroy(<b>struct</b> mm_struct *mm);</td></tr>
<tr><th id="506">506</th><td></td></tr>
<tr><th id="507">507</th><td><em>static</em> <b>inline</b> <em>void</em> hmm_mm_init(<b>struct</b> mm_struct *mm)</td></tr>
<tr><th id="508">508</th><td>{</td></tr>
<tr><th id="509">509</th><td>	mm-&gt;hmm = NULL;</td></tr>
<tr><th id="510">510</th><td>}</td></tr>
<tr><th id="511">511</th><td><u>#<span data-ppcond="504">else</span> /* IS_ENABLED(CONFIG_HMM_MIRROR) */</u></td></tr>
<tr><th id="512">512</th><td><em>static</em> <a class="macro" href="compiler-gcc.h.html#95" title="inline __attribute__((always_inline, unused)) __attribute__((no_instrument_function))" data-ref="_M/inline"><b>inline</b></a> <em>void</em> <dfn class="decl def fn" id="hmm_mm_destroy" title='hmm_mm_destroy' data-ref="hmm_mm_destroy">hmm_mm_destroy</dfn>(<b>struct</b> <a class="type" href="mm_types.h.html#mm_struct" title='mm_struct' data-ref="mm_struct">mm_struct</a> *<dfn class="local col1 decl" id="1mm" title='mm' data-type='struct mm_struct *' data-ref="1mm">mm</dfn>) {}</td></tr>
<tr><th id="513">513</th><td><em>static</em> <a class="macro" href="compiler-gcc.h.html#95" title="inline __attribute__((always_inline, unused)) __attribute__((no_instrument_function))" data-ref="_M/inline"><b>inline</b></a> <em>void</em> <dfn class="decl def fn" id="hmm_mm_init" title='hmm_mm_init' data-ref="hmm_mm_init">hmm_mm_init</dfn>(<b>struct</b> <a class="type" href="mm_types.h.html#mm_struct" title='mm_struct' data-ref="mm_struct">mm_struct</a> *<dfn class="local col2 decl" id="2mm" title='mm' data-type='struct mm_struct *' data-ref="2mm">mm</dfn>) {}</td></tr>
<tr><th id="514">514</th><td><u>#<span data-ppcond="504">endif</span> /* IS_ENABLED(CONFIG_HMM_MIRROR) */</u></td></tr>
<tr><th id="515">515</th><td></td></tr>
<tr><th id="516">516</th><td></td></tr>
<tr><th id="517">517</th><td><u>#<span data-ppcond="68">else</span> /* IS_ENABLED(CONFIG_HMM) */</u></td></tr>
<tr><th id="518">518</th><td><em>static</em> <b>inline</b> <em>void</em> hmm_mm_destroy(<b>struct</b> mm_struct *mm) {}</td></tr>
<tr><th id="519">519</th><td><em>static</em> <b>inline</b> <em>void</em> hmm_mm_init(<b>struct</b> mm_struct *mm) {}</td></tr>
<tr><th id="520">520</th><td><u>#<span data-ppcond="68">endif</span> /* LINUX_HMM_H */</u></td></tr>
<tr><th id="521">521</th><td></td></tr>
</table><hr/><p id='footer'>
Generated while processing <a href='../../kernel/fork.c.html'>linux-4.14.y/kernel/fork.c</a><br/>Generated on <em>2018-Aug-02</em> from project linux-4.14.y revision <em>linux-4.14.y</em><br />Powered by <a href='https://woboq.com'><img alt='Woboq' src='https://code.woboq.org/woboq-16.png' width='41' height='16' /></a> <a href='https://code.woboq.org'>Code Browser</a> 2.1
<br/>Generator usage only permitted with license.</p>
</div></body></html>
