<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>tree_plugin.h source code [linux-4.14.y/kernel/rcu/tree_plugin.h] - Woboq Code Browser</title>
<link rel="stylesheet" href="../../../../data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="../../../../data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="../../../../data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="../../../../data/jquery/jquery-ui.min.js"></script>
<script>var file = 'linux-4.14.y/kernel/rcu/tree_plugin.h'; var root_path = '../../..'; var data_path = '../../../../data';</script>
<script src='../../../../data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../..'>linux-4.14.y</a>/<a href='..'>kernel</a>/<a href='./'>rcu</a>/<a href='tree_plugin.h.html'>tree_plugin.h</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>/*</i></td></tr>
<tr><th id="2">2</th><td><i> * Read-Copy Update mechanism for mutual exclusion (tree-based version)</i></td></tr>
<tr><th id="3">3</th><td><i> * Internal non-public definitions that provide either classic</i></td></tr>
<tr><th id="4">4</th><td><i> * or preemptible semantics.</i></td></tr>
<tr><th id="5">5</th><td><i> *</i></td></tr>
<tr><th id="6">6</th><td><i> * This program is free software; you can redistribute it and/or modify</i></td></tr>
<tr><th id="7">7</th><td><i> * it under the terms of the GNU General Public License as published by</i></td></tr>
<tr><th id="8">8</th><td><i> * the Free Software Foundation; either version 2 of the License, or</i></td></tr>
<tr><th id="9">9</th><td><i> * (at your option) any later version.</i></td></tr>
<tr><th id="10">10</th><td><i> *</i></td></tr>
<tr><th id="11">11</th><td><i> * This program is distributed in the hope that it will be useful,</i></td></tr>
<tr><th id="12">12</th><td><i> * but WITHOUT ANY WARRANTY; without even the implied warranty of</i></td></tr>
<tr><th id="13">13</th><td><i> * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</i></td></tr>
<tr><th id="14">14</th><td><i> * GNU General Public License for more details.</i></td></tr>
<tr><th id="15">15</th><td><i> *</i></td></tr>
<tr><th id="16">16</th><td><i> * You should have received a copy of the GNU General Public License</i></td></tr>
<tr><th id="17">17</th><td><i> * along with this program; if not, you can access it online at</i></td></tr>
<tr><th id="18">18</th><td><i> * <a href="http://www.gnu.org/licenses/gpl-2.0.html">http://www.gnu.org/licenses/gpl-2.0.html</a>.</i></td></tr>
<tr><th id="19">19</th><td><i> *</i></td></tr>
<tr><th id="20">20</th><td><i> * Copyright Red Hat, 2009</i></td></tr>
<tr><th id="21">21</th><td><i> * Copyright IBM Corporation, 2009</i></td></tr>
<tr><th id="22">22</th><td><i> *</i></td></tr>
<tr><th id="23">23</th><td><i> * Author: Ingo Molnar &lt;mingo@elte.hu&gt;</i></td></tr>
<tr><th id="24">24</th><td><i> *	   Paul E. McKenney &lt;paulmck@linux.vnet.ibm.com&gt;</i></td></tr>
<tr><th id="25">25</th><td><i> */</i></td></tr>
<tr><th id="26">26</th><td></td></tr>
<tr><th id="27">27</th><td><u>#include <a href="../../include/linux/delay.h.html">&lt;linux/delay.h&gt;</a></u></td></tr>
<tr><th id="28">28</th><td><u>#include <a href="../../include/linux/gfp.h.html">&lt;linux/gfp.h&gt;</a></u></td></tr>
<tr><th id="29">29</th><td><u>#include <a href="../../include/linux/oom.h.html">&lt;linux/oom.h&gt;</a></u></td></tr>
<tr><th id="30">30</th><td><u>#include <a href="../../include/linux/sched/debug.h.html">&lt;linux/sched/debug.h&gt;</a></u></td></tr>
<tr><th id="31">31</th><td><u>#include <a href="../../include/linux/smpboot.h.html">&lt;linux/smpboot.h&gt;</a></u></td></tr>
<tr><th id="32">32</th><td><u>#include <a href="../../include/uapi/linux/sched/types.h.html">&lt;uapi/linux/sched/types.h&gt;</a></u></td></tr>
<tr><th id="33">33</th><td><u>#include <a href="../time/tick-internal.h.html">"../time/tick-internal.h"</a></u></td></tr>
<tr><th id="34">34</th><td></td></tr>
<tr><th id="35">35</th><td><u>#<span data-ppcond="35">ifdef</span> <span class="macro" data-ref="_M/CONFIG_RCU_BOOST">CONFIG_RCU_BOOST</span></u></td></tr>
<tr><th id="36">36</th><td></td></tr>
<tr><th id="37">37</th><td><u>#include "../locking/rtmutex_common.h"</u></td></tr>
<tr><th id="38">38</th><td></td></tr>
<tr><th id="39">39</th><td><i>/*</i></td></tr>
<tr><th id="40">40</th><td><i> * Control variables for per-CPU and per-rcu_node kthreads.  These</i></td></tr>
<tr><th id="41">41</th><td><i> * handle all flavors of RCU.</i></td></tr>
<tr><th id="42">42</th><td><i> */</i></td></tr>
<tr><th id="43">43</th><td><em>static</em> DEFINE_PER_CPU(<b>struct</b> task_struct *, rcu_cpu_kthread_task);</td></tr>
<tr><th id="44">44</th><td>DEFINE_PER_CPU(<em>unsigned</em> <em>int</em>, rcu_cpu_kthread_status);</td></tr>
<tr><th id="45">45</th><td>DEFINE_PER_CPU(<em>unsigned</em> <em>int</em>, rcu_cpu_kthread_loops);</td></tr>
<tr><th id="46">46</th><td>DEFINE_PER_CPU(<em>char</em>, rcu_cpu_has_work);</td></tr>
<tr><th id="47">47</th><td></td></tr>
<tr><th id="48">48</th><td><u>#<span data-ppcond="35">else</span> /* #ifdef CONFIG_RCU_BOOST */</u></td></tr>
<tr><th id="49">49</th><td></td></tr>
<tr><th id="50">50</th><td><i>/*</i></td></tr>
<tr><th id="51">51</th><td><i> * Some architectures do not define rt_mutexes, but if !CONFIG_RCU_BOOST,</i></td></tr>
<tr><th id="52">52</th><td><i> * all uses are in dead code.  Provide a definition to keep the compiler</i></td></tr>
<tr><th id="53">53</th><td><i> * happy, but add WARN_ON_ONCE() to complain if used in the wrong place.</i></td></tr>
<tr><th id="54">54</th><td><i> * This probably needs to be excluded from -rt builds.</i></td></tr>
<tr><th id="55">55</th><td><i> */</i></td></tr>
<tr><th id="56">56</th><td><u>#define <dfn class="macro" id="_M/rt_mutex_owner" data-ref="_M/rt_mutex_owner">rt_mutex_owner</dfn>(a) ({ WARN_ON_ONCE(1); NULL; })</u></td></tr>
<tr><th id="57">57</th><td></td></tr>
<tr><th id="58">58</th><td><u>#<span data-ppcond="35">endif</span> /* #else #ifdef CONFIG_RCU_BOOST */</u></td></tr>
<tr><th id="59">59</th><td></td></tr>
<tr><th id="60">60</th><td><u>#<span data-ppcond="60">ifdef</span> <span class="macro" data-ref="_M/CONFIG_RCU_NOCB_CPU">CONFIG_RCU_NOCB_CPU</span></u></td></tr>
<tr><th id="61">61</th><td><em>static</em> cpumask_var_t rcu_nocb_mask; <i>/* CPUs to have callbacks offloaded. */</i></td></tr>
<tr><th id="62">62</th><td><em>static</em> bool have_rcu_nocb_mask;	    <i>/* Was rcu_nocb_mask allocated? */</i></td></tr>
<tr><th id="63">63</th><td><em>static</em> bool __read_mostly rcu_nocb_poll;    <i>/* Offload kthread are to poll. */</i></td></tr>
<tr><th id="64">64</th><td><u>#<span data-ppcond="60">endif</span> /* #ifdef CONFIG_RCU_NOCB_CPU */</u></td></tr>
<tr><th id="65">65</th><td></td></tr>
<tr><th id="66">66</th><td><i>/*</i></td></tr>
<tr><th id="67">67</th><td><i> * Check the RCU kernel configuration parameters and print informative</i></td></tr>
<tr><th id="68">68</th><td><i> * messages about anything out of the ordinary.</i></td></tr>
<tr><th id="69">69</th><td><i> */</i></td></tr>
<tr><th id="70">70</th><td><em>static</em> <em>void</em> <a class="macro" href="../../include/linux/init.h.html#50" title="__attribute__ ((__section__(&quot;.init.text&quot;)))" data-ref="_M/__init">__init</a> <dfn class="decl def fn" id="rcu_bootup_announce_oddness" title='rcu_bootup_announce_oddness' data-ref="rcu_bootup_announce_oddness">rcu_bootup_announce_oddness</dfn>(<em>void</em>)</td></tr>
<tr><th id="71">71</th><td>{</td></tr>
<tr><th id="72">72</th><td>	<b>if</b> (<a class="macro" href="../../include/linux/kconfig.h.html#71" title="0" data-ref="_M/IS_ENABLED">IS_ENABLED</a>(CONFIG_RCU_TRACE))</td></tr>
<tr><th id="73">73</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tRCU event tracing is enabled.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU event tracing is enabled.\n"</q>);</td></tr>
<tr><th id="74">74</th><td>	<b>if</b> ((<a class="macro" href="../../include/linux/kconfig.h.html#71" title="1" data-ref="_M/IS_ENABLED">IS_ENABLED</a>(<a class="macro" href="../../include/generated/autoconf.h.html#1218" title="1" data-ref="_M/CONFIG_64BIT">CONFIG_64BIT</a>) &amp;&amp; <a class="macro" href="../../include/linux/rcu_node_tree.h.html#48" title="64" data-ref="_M/RCU_FANOUT">RCU_FANOUT</a> != <var>64</var>) ||</td></tr>
<tr><th id="75">75</th><td>	    (!<a class="macro" href="../../include/linux/kconfig.h.html#71" title="1" data-ref="_M/IS_ENABLED">IS_ENABLED</a>(<a class="macro" href="../../include/generated/autoconf.h.html#1218" title="1" data-ref="_M/CONFIG_64BIT">CONFIG_64BIT</a>) &amp;&amp; <a class="macro" href="../../include/linux/rcu_node_tree.h.html#48" title="64" data-ref="_M/RCU_FANOUT">RCU_FANOUT</a> != <var>32</var>))</td></tr>
<tr><th id="76">76</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tCONFIG_RCU_FANOUT set to non-default value of %d\n&quot;, 64)" data-ref="_M/pr_info">pr_info</a>(<q>"\tCONFIG_RCU_FANOUT set to non-default value of %d\n"</q>,</td></tr>
<tr><th id="77">77</th><td>		       RCU_FANOUT);</td></tr>
<tr><th id="78">78</th><td>	<b>if</b> (<a class="tu ref" href="rcu.h.html#rcu_fanout_exact" title='rcu_fanout_exact' data-use='r' data-ref="rcu_fanout_exact">rcu_fanout_exact</a>)</td></tr>
<tr><th id="79">79</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tHierarchical RCU autobalancing is disabled.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"\tHierarchical RCU autobalancing is disabled.\n"</q>);</td></tr>
<tr><th id="80">80</th><td>	<b>if</b> (<a class="macro" href="../../include/linux/kconfig.h.html#71" title="0" data-ref="_M/IS_ENABLED">IS_ENABLED</a>(CONFIG_RCU_FAST_NO_HZ))</td></tr>
<tr><th id="81">81</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tRCU dyntick-idle grace-period acceleration is enabled.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU dyntick-idle grace-period acceleration is enabled.\n"</q>);</td></tr>
<tr><th id="82">82</th><td>	<b>if</b> (<a class="macro" href="../../include/linux/kconfig.h.html#71" title="0" data-ref="_M/IS_ENABLED">IS_ENABLED</a>(CONFIG_PROVE_RCU))</td></tr>
<tr><th id="83">83</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tRCU lockdep checking is enabled.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU lockdep checking is enabled.\n"</q>);</td></tr>
<tr><th id="84">84</th><td>	<b>if</b> (<a class="macro" href="../../include/linux/rcu_node_tree.h.html#73" title="2" data-ref="_M/RCU_NUM_LVLS">RCU_NUM_LVLS</a> &gt;= <var>4</var>)</td></tr>
<tr><th id="85">85</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tFour(or more)-level hierarchy is enabled.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"\tFour(or more)-level hierarchy is enabled.\n"</q>);</td></tr>
<tr><th id="86">86</th><td>	<b>if</b> (<a class="macro" href="../../include/linux/rcu_node_tree.h.html#57" title="16" data-ref="_M/RCU_FANOUT_LEAF">RCU_FANOUT_LEAF</a> != <var>16</var>)</td></tr>
<tr><th id="87">87</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tBuild-time adjustment of leaf fanout to %d.\n&quot;, 16)" data-ref="_M/pr_info">pr_info</a>(<q>"\tBuild-time adjustment of leaf fanout to %d.\n"</q>,</td></tr>
<tr><th id="88">88</th><td>			RCU_FANOUT_LEAF);</td></tr>
<tr><th id="89">89</th><td>	<b>if</b> (<a class="tu ref" href="rcu.h.html#rcu_fanout_leaf" title='rcu_fanout_leaf' data-use='r' data-ref="rcu_fanout_leaf">rcu_fanout_leaf</a> != <a class="macro" href="../../include/linux/rcu_node_tree.h.html#57" title="16" data-ref="_M/RCU_FANOUT_LEAF">RCU_FANOUT_LEAF</a>)</td></tr>
<tr><th id="90">90</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tBoot-time adjustment of leaf fanout to %d.\n&quot;, rcu_fanout_leaf)" data-ref="_M/pr_info">pr_info</a>(<q>"\tBoot-time adjustment of leaf fanout to %d.\n"</q>, <a class="tu ref" href="rcu.h.html#rcu_fanout_leaf" title='rcu_fanout_leaf' data-use='r' data-ref="rcu_fanout_leaf">rcu_fanout_leaf</a>);</td></tr>
<tr><th id="91">91</th><td>	<b>if</b> (<a class="ref" href="../../include/linux/cpumask.h.html#nr_cpu_ids" title='nr_cpu_ids' data-ref="nr_cpu_ids">nr_cpu_ids</a> != <a class="macro" href="../../include/linux/threads.h.html#21" title="64" data-ref="_M/NR_CPUS">NR_CPUS</a>)</td></tr>
<tr><th id="92">92</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tRCU restricting CPUs from NR_CPUS=%d to nr_cpu_ids=%u.\n&quot;, 64, nr_cpu_ids)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU restricting CPUs from NR_CPUS=%d to nr_cpu_ids=%u.\n"</q>, NR_CPUS, <a class="ref" href="../../include/linux/cpumask.h.html#nr_cpu_ids" title='nr_cpu_ids' data-ref="nr_cpu_ids">nr_cpu_ids</a>);</td></tr>
<tr><th id="93">93</th><td><u>#<span data-ppcond="93">ifdef</span> <span class="macro" data-ref="_M/CONFIG_RCU_BOOST">CONFIG_RCU_BOOST</span></u></td></tr>
<tr><th id="94">94</th><td>	pr_info(<q>"\tRCU priority boosting: priority %d delay %d ms.\n"</q>, kthread_prio, CONFIG_RCU_BOOST_DELAY);</td></tr>
<tr><th id="95">95</th><td><u>#<span data-ppcond="93">endif</span></u></td></tr>
<tr><th id="96">96</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#blimit" title='blimit' data-use='r' data-ref="blimit">blimit</a> != <a class="macro" href="tree.c.html#514" title="10" data-ref="_M/DEFAULT_RCU_BLIMIT">DEFAULT_RCU_BLIMIT</a>)</td></tr>
<tr><th id="97">97</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tBoot-time adjustment of callback invocation limit to %ld.\n&quot;, blimit)" data-ref="_M/pr_info">pr_info</a>(<q>"\tBoot-time adjustment of callback invocation limit to %ld.\n"</q>, <a class="tu ref" href="tree.c.html#blimit" title='blimit' data-use='r' data-ref="blimit">blimit</a>);</td></tr>
<tr><th id="98">98</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#qhimark" title='qhimark' data-use='r' data-ref="qhimark">qhimark</a> != <a class="macro" href="tree.c.html#516" title="10000" data-ref="_M/DEFAULT_RCU_QHIMARK">DEFAULT_RCU_QHIMARK</a>)</td></tr>
<tr><th id="99">99</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tBoot-time adjustment of callback high-water mark to %ld.\n&quot;, qhimark)" data-ref="_M/pr_info">pr_info</a>(<q>"\tBoot-time adjustment of callback high-water mark to %ld.\n"</q>, <a class="tu ref" href="tree.c.html#qhimark" title='qhimark' data-use='r' data-ref="qhimark">qhimark</a>);</td></tr>
<tr><th id="100">100</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#qlowmark" title='qlowmark' data-use='r' data-ref="qlowmark">qlowmark</a> != <a class="macro" href="tree.c.html#518" title="100" data-ref="_M/DEFAULT_RCU_QLOMARK">DEFAULT_RCU_QLOMARK</a>)</td></tr>
<tr><th id="101">101</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tBoot-time adjustment of callback low-water mark to %ld.\n&quot;, qlowmark)" data-ref="_M/pr_info">pr_info</a>(<q>"\tBoot-time adjustment of callback low-water mark to %ld.\n"</q>, <a class="tu ref" href="tree.c.html#qlowmark" title='qlowmark' data-use='r' data-ref="qlowmark">qlowmark</a>);</td></tr>
<tr><th id="102">102</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#jiffies_till_first_fqs" title='jiffies_till_first_fqs' data-use='r' data-ref="jiffies_till_first_fqs">jiffies_till_first_fqs</a> != <a class="macro" href="../../include/linux/kernel.h.html#27" title="(~0UL)" data-ref="_M/ULONG_MAX">ULONG_MAX</a>)</td></tr>
<tr><th id="103">103</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tBoot-time adjustment of first FQS scan delay to %ld jiffies.\n&quot;, jiffies_till_first_fqs)" data-ref="_M/pr_info">pr_info</a>(<q>"\tBoot-time adjustment of first FQS scan delay to %ld jiffies.\n"</q>, <a class="tu ref" href="tree.c.html#jiffies_till_first_fqs" title='jiffies_till_first_fqs' data-use='r' data-ref="jiffies_till_first_fqs">jiffies_till_first_fqs</a>);</td></tr>
<tr><th id="104">104</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#jiffies_till_next_fqs" title='jiffies_till_next_fqs' data-use='r' data-ref="jiffies_till_next_fqs">jiffies_till_next_fqs</a> != <a class="macro" href="../../include/linux/kernel.h.html#27" title="(~0UL)" data-ref="_M/ULONG_MAX">ULONG_MAX</a>)</td></tr>
<tr><th id="105">105</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tBoot-time adjustment of subsequent FQS scan delay to %ld jiffies.\n&quot;, jiffies_till_next_fqs)" data-ref="_M/pr_info">pr_info</a>(<q>"\tBoot-time adjustment of subsequent FQS scan delay to %ld jiffies.\n"</q>, <a class="tu ref" href="tree.c.html#jiffies_till_next_fqs" title='jiffies_till_next_fqs' data-use='r' data-ref="jiffies_till_next_fqs">jiffies_till_next_fqs</a>);</td></tr>
<tr><th id="106">106</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#rcu_kick_kthreads" title='rcu_kick_kthreads' data-use='r' data-ref="rcu_kick_kthreads">rcu_kick_kthreads</a>)</td></tr>
<tr><th id="107">107</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tKick kthreads if too-long grace period.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"\tKick kthreads if too-long grace period.\n"</q>);</td></tr>
<tr><th id="108">108</th><td>	<b>if</b> (<a class="macro" href="../../include/linux/kconfig.h.html#71" title="0" data-ref="_M/IS_ENABLED">IS_ENABLED</a>(CONFIG_DEBUG_OBJECTS_RCU_HEAD))</td></tr>
<tr><th id="109">109</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tRCU callback double-/use-after-free debug enabled.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU callback double-/use-after-free debug enabled.\n"</q>);</td></tr>
<tr><th id="110">110</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#gp_preinit_delay" title='gp_preinit_delay' data-use='r' data-ref="gp_preinit_delay">gp_preinit_delay</a>)</td></tr>
<tr><th id="111">111</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tRCU debug GP pre-init slowdown %d jiffies.\n&quot;, gp_preinit_delay)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU debug GP pre-init slowdown %d jiffies.\n"</q>, <a class="tu ref" href="tree.c.html#gp_preinit_delay" title='gp_preinit_delay' data-use='r' data-ref="gp_preinit_delay">gp_preinit_delay</a>);</td></tr>
<tr><th id="112">112</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#gp_init_delay" title='gp_init_delay' data-use='r' data-ref="gp_init_delay">gp_init_delay</a>)</td></tr>
<tr><th id="113">113</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tRCU debug GP init slowdown %d jiffies.\n&quot;, gp_init_delay)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU debug GP init slowdown %d jiffies.\n"</q>, <a class="tu ref" href="tree.c.html#gp_init_delay" title='gp_init_delay' data-use='r' data-ref="gp_init_delay">gp_init_delay</a>);</td></tr>
<tr><th id="114">114</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#gp_cleanup_delay" title='gp_cleanup_delay' data-use='r' data-ref="gp_cleanup_delay">gp_cleanup_delay</a>)</td></tr>
<tr><th id="115">115</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tRCU debug GP init slowdown %d jiffies.\n&quot;, gp_cleanup_delay)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU debug GP init slowdown %d jiffies.\n"</q>, <a class="tu ref" href="tree.c.html#gp_cleanup_delay" title='gp_cleanup_delay' data-use='r' data-ref="gp_cleanup_delay">gp_cleanup_delay</a>);</td></tr>
<tr><th id="116">116</th><td>	<b>if</b> (<a class="macro" href="../../include/linux/kconfig.h.html#71" title="0" data-ref="_M/IS_ENABLED">IS_ENABLED</a>(CONFIG_RCU_EQS_DEBUG))</td></tr>
<tr><th id="117">117</th><td>		<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;\tRCU debug extended QS entry/exit.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU debug extended QS entry/exit.\n"</q>);</td></tr>
<tr><th id="118">118</th><td>	<a class="ref fn" href="rcu.h.html#rcupdate_announce_bootup_oddness" title='rcupdate_announce_bootup_oddness' data-ref="rcupdate_announce_bootup_oddness">rcupdate_announce_bootup_oddness</a>();</td></tr>
<tr><th id="119">119</th><td>}</td></tr>
<tr><th id="120">120</th><td></td></tr>
<tr><th id="121">121</th><td><u>#<span data-ppcond="121">ifdef</span> <span class="macro" data-ref="_M/CONFIG_PREEMPT_RCU">CONFIG_PREEMPT_RCU</span></u></td></tr>
<tr><th id="122">122</th><td></td></tr>
<tr><th id="123">123</th><td>RCU_STATE_INITIALIZER(rcu_preempt, <kbd>'p'</kbd>, call_rcu);</td></tr>
<tr><th id="124">124</th><td><em>static</em> <b>struct</b> rcu_state *<em>const</em> rcu_state_p = &amp;rcu_preempt_state;</td></tr>
<tr><th id="125">125</th><td><em>static</em> <b>struct</b> rcu_data __percpu *<em>const</em> rcu_data_p = &amp;rcu_preempt_data;</td></tr>
<tr><th id="126">126</th><td></td></tr>
<tr><th id="127">127</th><td><em>static</em> <em>void</em> rcu_report_exp_rnp(<b>struct</b> rcu_state *rsp, <b>struct</b> rcu_node *rnp,</td></tr>
<tr><th id="128">128</th><td>			       bool wake);</td></tr>
<tr><th id="129">129</th><td></td></tr>
<tr><th id="130">130</th><td><i>/*</i></td></tr>
<tr><th id="131">131</th><td><i> * Tell them what RCU they are running.</i></td></tr>
<tr><th id="132">132</th><td><i> */</i></td></tr>
<tr><th id="133">133</th><td><em>static</em> <em>void</em> __init rcu_bootup_announce(<em>void</em>)</td></tr>
<tr><th id="134">134</th><td>{</td></tr>
<tr><th id="135">135</th><td>	pr_info(<q>"Preemptible hierarchical RCU implementation.\n"</q>);</td></tr>
<tr><th id="136">136</th><td>	rcu_bootup_announce_oddness();</td></tr>
<tr><th id="137">137</th><td>}</td></tr>
<tr><th id="138">138</th><td></td></tr>
<tr><th id="139">139</th><td><i>/* Flags for rcu_preempt_ctxt_queue() decision table. */</i></td></tr>
<tr><th id="140">140</th><td><u>#define RCU_GP_TASKS	0x8</u></td></tr>
<tr><th id="141">141</th><td><u>#define RCU_EXP_TASKS	0x4</u></td></tr>
<tr><th id="142">142</th><td><u>#define RCU_GP_BLKD	0x2</u></td></tr>
<tr><th id="143">143</th><td><u>#define RCU_EXP_BLKD	0x1</u></td></tr>
<tr><th id="144">144</th><td></td></tr>
<tr><th id="145">145</th><td><i>/*</i></td></tr>
<tr><th id="146">146</th><td><i> * Queues a task preempted within an RCU-preempt read-side critical</i></td></tr>
<tr><th id="147">147</th><td><i> * section into the appropriate location within the -&gt;blkd_tasks list,</i></td></tr>
<tr><th id="148">148</th><td><i> * depending on the states of any ongoing normal and expedited grace</i></td></tr>
<tr><th id="149">149</th><td><i> * periods.  The -&gt;gp_tasks pointer indicates which element the normal</i></td></tr>
<tr><th id="150">150</th><td><i> * grace period is waiting on (NULL if none), and the -&gt;exp_tasks pointer</i></td></tr>
<tr><th id="151">151</th><td><i> * indicates which element the expedited grace period is waiting on (again,</i></td></tr>
<tr><th id="152">152</th><td><i> * NULL if none).  If a grace period is waiting on a given element in the</i></td></tr>
<tr><th id="153">153</th><td><i> * -&gt;blkd_tasks list, it also waits on all subsequent elements.  Thus,</i></td></tr>
<tr><th id="154">154</th><td><i> * adding a task to the tail of the list blocks any grace period that is</i></td></tr>
<tr><th id="155">155</th><td><i> * already waiting on one of the elements.  In contrast, adding a task</i></td></tr>
<tr><th id="156">156</th><td><i> * to the head of the list won't block any grace period that is already</i></td></tr>
<tr><th id="157">157</th><td><i> * waiting on one of the elements.</i></td></tr>
<tr><th id="158">158</th><td><i> *</i></td></tr>
<tr><th id="159">159</th><td><i> * This queuing is imprecise, and can sometimes make an ongoing grace</i></td></tr>
<tr><th id="160">160</th><td><i> * period wait for a task that is not strictly speaking blocking it.</i></td></tr>
<tr><th id="161">161</th><td><i> * Given the choice, we needlessly block a normal grace period rather than</i></td></tr>
<tr><th id="162">162</th><td><i> * blocking an expedited grace period.</i></td></tr>
<tr><th id="163">163</th><td><i> *</i></td></tr>
<tr><th id="164">164</th><td><i> * Note that an endless sequence of expedited grace periods still cannot</i></td></tr>
<tr><th id="165">165</th><td><i> * indefinitely postpone a normal grace period.  Eventually, all of the</i></td></tr>
<tr><th id="166">166</th><td><i> * fixed number of preempted tasks blocking the normal grace period that are</i></td></tr>
<tr><th id="167">167</th><td><i> * not also blocking the expedited grace period will resume and complete</i></td></tr>
<tr><th id="168">168</th><td><i> * their RCU read-side critical sections.  At that point, the -&gt;gp_tasks</i></td></tr>
<tr><th id="169">169</th><td><i> * pointer will equal the -&gt;exp_tasks pointer, at which point the end of</i></td></tr>
<tr><th id="170">170</th><td><i> * the corresponding expedited grace period will also be the end of the</i></td></tr>
<tr><th id="171">171</th><td><i> * normal grace period.</i></td></tr>
<tr><th id="172">172</th><td><i> */</i></td></tr>
<tr><th id="173">173</th><td><em>static</em> <em>void</em> rcu_preempt_ctxt_queue(<b>struct</b> rcu_node *rnp, <b>struct</b> rcu_data *rdp)</td></tr>
<tr><th id="174">174</th><td>	__releases(rnp-&gt;lock) <i>/* But leaves rrupts disabled. */</i></td></tr>
<tr><th id="175">175</th><td>{</td></tr>
<tr><th id="176">176</th><td>	<em>int</em> blkd_state = (rnp-&gt;gp_tasks ? RCU_GP_TASKS : <var>0</var>) +</td></tr>
<tr><th id="177">177</th><td>			 (rnp-&gt;exp_tasks ? RCU_EXP_TASKS : <var>0</var>) +</td></tr>
<tr><th id="178">178</th><td>			 (rnp-&gt;qsmask &amp; rdp-&gt;grpmask ? RCU_GP_BLKD : <var>0</var>) +</td></tr>
<tr><th id="179">179</th><td>			 (rnp-&gt;expmask &amp; rdp-&gt;grpmask ? RCU_EXP_BLKD : <var>0</var>);</td></tr>
<tr><th id="180">180</th><td>	<b>struct</b> task_struct *t = current;</td></tr>
<tr><th id="181">181</th><td></td></tr>
<tr><th id="182">182</th><td>	lockdep_assert_held(&amp;rnp-&gt;lock);</td></tr>
<tr><th id="183">183</th><td>	WARN_ON_ONCE(rdp-&gt;mynode != rnp);</td></tr>
<tr><th id="184">184</th><td>	WARN_ON_ONCE(rnp-&gt;level != rcu_num_lvls - <var>1</var>);</td></tr>
<tr><th id="185">185</th><td></td></tr>
<tr><th id="186">186</th><td>	<i>/*</i></td></tr>
<tr><th id="187">187</th><td><i>	 * Decide where to queue the newly blocked task.  In theory,</i></td></tr>
<tr><th id="188">188</th><td><i>	 * this could be an if-statement.  In practice, when I tried</i></td></tr>
<tr><th id="189">189</th><td><i>	 * that, it was quite messy.</i></td></tr>
<tr><th id="190">190</th><td><i>	 */</i></td></tr>
<tr><th id="191">191</th><td>	<b>switch</b> (blkd_state) {</td></tr>
<tr><th id="192">192</th><td>	<b>case</b> <var>0</var>:</td></tr>
<tr><th id="193">193</th><td>	<b>case</b>                RCU_EXP_TASKS:</td></tr>
<tr><th id="194">194</th><td>	<b>case</b>                RCU_EXP_TASKS + RCU_GP_BLKD:</td></tr>
<tr><th id="195">195</th><td>	<b>case</b> RCU_GP_TASKS:</td></tr>
<tr><th id="196">196</th><td>	<b>case</b> RCU_GP_TASKS + RCU_EXP_TASKS:</td></tr>
<tr><th id="197">197</th><td></td></tr>
<tr><th id="198">198</th><td>		<i>/*</i></td></tr>
<tr><th id="199">199</th><td><i>		 * Blocking neither GP, or first task blocking the normal</i></td></tr>
<tr><th id="200">200</th><td><i>		 * GP but not blocking the already-waiting expedited GP.</i></td></tr>
<tr><th id="201">201</th><td><i>		 * Queue at the head of the list to avoid unnecessarily</i></td></tr>
<tr><th id="202">202</th><td><i>		 * blocking the already-waiting GPs.</i></td></tr>
<tr><th id="203">203</th><td><i>		 */</i></td></tr>
<tr><th id="204">204</th><td>		list_add(&amp;t-&gt;rcu_node_entry, &amp;rnp-&gt;blkd_tasks);</td></tr>
<tr><th id="205">205</th><td>		<b>break</b>;</td></tr>
<tr><th id="206">206</th><td></td></tr>
<tr><th id="207">207</th><td>	<b>case</b>                                              RCU_EXP_BLKD:</td></tr>
<tr><th id="208">208</th><td>	<b>case</b>                                RCU_GP_BLKD:</td></tr>
<tr><th id="209">209</th><td>	<b>case</b>                                RCU_GP_BLKD + RCU_EXP_BLKD:</td></tr>
<tr><th id="210">210</th><td>	<b>case</b> RCU_GP_TASKS +                               RCU_EXP_BLKD:</td></tr>
<tr><th id="211">211</th><td>	<b>case</b> RCU_GP_TASKS +                 RCU_GP_BLKD + RCU_EXP_BLKD:</td></tr>
<tr><th id="212">212</th><td>	<b>case</b> RCU_GP_TASKS + RCU_EXP_TASKS + RCU_GP_BLKD + RCU_EXP_BLKD:</td></tr>
<tr><th id="213">213</th><td></td></tr>
<tr><th id="214">214</th><td>		<i>/*</i></td></tr>
<tr><th id="215">215</th><td><i>		 * First task arriving that blocks either GP, or first task</i></td></tr>
<tr><th id="216">216</th><td><i>		 * arriving that blocks the expedited GP (with the normal</i></td></tr>
<tr><th id="217">217</th><td><i>		 * GP already waiting), or a task arriving that blocks</i></td></tr>
<tr><th id="218">218</th><td><i>		 * both GPs with both GPs already waiting.  Queue at the</i></td></tr>
<tr><th id="219">219</th><td><i>		 * tail of the list to avoid any GP waiting on any of the</i></td></tr>
<tr><th id="220">220</th><td><i>		 * already queued tasks that are not blocking it.</i></td></tr>
<tr><th id="221">221</th><td><i>		 */</i></td></tr>
<tr><th id="222">222</th><td>		list_add_tail(&amp;t-&gt;rcu_node_entry, &amp;rnp-&gt;blkd_tasks);</td></tr>
<tr><th id="223">223</th><td>		<b>break</b>;</td></tr>
<tr><th id="224">224</th><td></td></tr>
<tr><th id="225">225</th><td>	<b>case</b>                RCU_EXP_TASKS +               RCU_EXP_BLKD:</td></tr>
<tr><th id="226">226</th><td>	<b>case</b>                RCU_EXP_TASKS + RCU_GP_BLKD + RCU_EXP_BLKD:</td></tr>
<tr><th id="227">227</th><td>	<b>case</b> RCU_GP_TASKS + RCU_EXP_TASKS +               RCU_EXP_BLKD:</td></tr>
<tr><th id="228">228</th><td></td></tr>
<tr><th id="229">229</th><td>		<i>/*</i></td></tr>
<tr><th id="230">230</th><td><i>		 * Second or subsequent task blocking the expedited GP.</i></td></tr>
<tr><th id="231">231</th><td><i>		 * The task either does not block the normal GP, or is the</i></td></tr>
<tr><th id="232">232</th><td><i>		 * first task blocking the normal GP.  Queue just after</i></td></tr>
<tr><th id="233">233</th><td><i>		 * the first task blocking the expedited GP.</i></td></tr>
<tr><th id="234">234</th><td><i>		 */</i></td></tr>
<tr><th id="235">235</th><td>		list_add(&amp;t-&gt;rcu_node_entry, rnp-&gt;exp_tasks);</td></tr>
<tr><th id="236">236</th><td>		<b>break</b>;</td></tr>
<tr><th id="237">237</th><td></td></tr>
<tr><th id="238">238</th><td>	<b>case</b> RCU_GP_TASKS +                 RCU_GP_BLKD:</td></tr>
<tr><th id="239">239</th><td>	<b>case</b> RCU_GP_TASKS + RCU_EXP_TASKS + RCU_GP_BLKD:</td></tr>
<tr><th id="240">240</th><td></td></tr>
<tr><th id="241">241</th><td>		<i>/*</i></td></tr>
<tr><th id="242">242</th><td><i>		 * Second or subsequent task blocking the normal GP.</i></td></tr>
<tr><th id="243">243</th><td><i>		 * The task does not block the expedited GP. Queue just</i></td></tr>
<tr><th id="244">244</th><td><i>		 * after the first task blocking the normal GP.</i></td></tr>
<tr><th id="245">245</th><td><i>		 */</i></td></tr>
<tr><th id="246">246</th><td>		list_add(&amp;t-&gt;rcu_node_entry, rnp-&gt;gp_tasks);</td></tr>
<tr><th id="247">247</th><td>		<b>break</b>;</td></tr>
<tr><th id="248">248</th><td></td></tr>
<tr><th id="249">249</th><td>	<b>default</b>:</td></tr>
<tr><th id="250">250</th><td></td></tr>
<tr><th id="251">251</th><td>		<i>/* Yet another exercise in excessive paranoia. */</i></td></tr>
<tr><th id="252">252</th><td>		WARN_ON_ONCE(<var>1</var>);</td></tr>
<tr><th id="253">253</th><td>		<b>break</b>;</td></tr>
<tr><th id="254">254</th><td>	}</td></tr>
<tr><th id="255">255</th><td></td></tr>
<tr><th id="256">256</th><td>	<i>/*</i></td></tr>
<tr><th id="257">257</th><td><i>	 * We have now queued the task.  If it was the first one to</i></td></tr>
<tr><th id="258">258</th><td><i>	 * block either grace period, update the -&gt;gp_tasks and/or</i></td></tr>
<tr><th id="259">259</th><td><i>	 * -&gt;exp_tasks pointers, respectively, to reference the newly</i></td></tr>
<tr><th id="260">260</th><td><i>	 * blocked tasks.</i></td></tr>
<tr><th id="261">261</th><td><i>	 */</i></td></tr>
<tr><th id="262">262</th><td>	<b>if</b> (!rnp-&gt;gp_tasks &amp;&amp; (blkd_state &amp; RCU_GP_BLKD))</td></tr>
<tr><th id="263">263</th><td>		rnp-&gt;gp_tasks = &amp;t-&gt;rcu_node_entry;</td></tr>
<tr><th id="264">264</th><td>	<b>if</b> (!rnp-&gt;exp_tasks &amp;&amp; (blkd_state &amp; RCU_EXP_BLKD))</td></tr>
<tr><th id="265">265</th><td>		rnp-&gt;exp_tasks = &amp;t-&gt;rcu_node_entry;</td></tr>
<tr><th id="266">266</th><td>	WARN_ON_ONCE(!(blkd_state &amp; RCU_GP_BLKD) !=</td></tr>
<tr><th id="267">267</th><td>		     !(rnp-&gt;qsmask &amp; rdp-&gt;grpmask));</td></tr>
<tr><th id="268">268</th><td>	WARN_ON_ONCE(!(blkd_state &amp; RCU_EXP_BLKD) !=</td></tr>
<tr><th id="269">269</th><td>		     !(rnp-&gt;expmask &amp; rdp-&gt;grpmask));</td></tr>
<tr><th id="270">270</th><td>	raw_spin_unlock_rcu_node(rnp); <i>/* interrupts remain disabled. */</i></td></tr>
<tr><th id="271">271</th><td></td></tr>
<tr><th id="272">272</th><td>	<i>/*</i></td></tr>
<tr><th id="273">273</th><td><i>	 * Report the quiescent state for the expedited GP.  This expedited</i></td></tr>
<tr><th id="274">274</th><td><i>	 * GP should not be able to end until we report, so there should be</i></td></tr>
<tr><th id="275">275</th><td><i>	 * no need to check for a subsequent expedited GP.  (Though we are</i></td></tr>
<tr><th id="276">276</th><td><i>	 * still in a quiescent state in any case.)</i></td></tr>
<tr><th id="277">277</th><td><i>	 */</i></td></tr>
<tr><th id="278">278</th><td>	<b>if</b> (blkd_state &amp; RCU_EXP_BLKD &amp;&amp;</td></tr>
<tr><th id="279">279</th><td>	    t-&gt;rcu_read_unlock_special.b.exp_need_qs) {</td></tr>
<tr><th id="280">280</th><td>		t-&gt;rcu_read_unlock_special.b.exp_need_qs = false;</td></tr>
<tr><th id="281">281</th><td>		rcu_report_exp_rdp(rdp-&gt;rsp, rdp, true);</td></tr>
<tr><th id="282">282</th><td>	} <b>else</b> {</td></tr>
<tr><th id="283">283</th><td>		WARN_ON_ONCE(t-&gt;rcu_read_unlock_special.b.exp_need_qs);</td></tr>
<tr><th id="284">284</th><td>	}</td></tr>
<tr><th id="285">285</th><td>}</td></tr>
<tr><th id="286">286</th><td></td></tr>
<tr><th id="287">287</th><td><i>/*</i></td></tr>
<tr><th id="288">288</th><td><i> * Record a preemptible-RCU quiescent state for the specified CPU.  Note</i></td></tr>
<tr><th id="289">289</th><td><i> * that this just means that the task currently running on the CPU is</i></td></tr>
<tr><th id="290">290</th><td><i> * not in a quiescent state.  There might be any number of tasks blocked</i></td></tr>
<tr><th id="291">291</th><td><i> * while in an RCU read-side critical section.</i></td></tr>
<tr><th id="292">292</th><td><i> *</i></td></tr>
<tr><th id="293">293</th><td><i> * As with the other rcu_*_qs() functions, callers to this function</i></td></tr>
<tr><th id="294">294</th><td><i> * must disable preemption.</i></td></tr>
<tr><th id="295">295</th><td><i> */</i></td></tr>
<tr><th id="296">296</th><td><em>static</em> <em>void</em> rcu_preempt_qs(<em>void</em>)</td></tr>
<tr><th id="297">297</th><td>{</td></tr>
<tr><th id="298">298</th><td>	RCU_LOCKDEP_WARN(preemptible(), <q>"rcu_preempt_qs() invoked with preemption enabled!!!\n"</q>);</td></tr>
<tr><th id="299">299</th><td>	<b>if</b> (__this_cpu_read(rcu_data_p-&gt;cpu_no_qs.s)) {</td></tr>
<tr><th id="300">300</th><td>		trace_rcu_grace_period(TPS(<q>"rcu_preempt"</q>),</td></tr>
<tr><th id="301">301</th><td>				       __this_cpu_read(rcu_data_p-&gt;gpnum),</td></tr>
<tr><th id="302">302</th><td>				       TPS(<q>"cpuqs"</q>));</td></tr>
<tr><th id="303">303</th><td>		__this_cpu_write(rcu_data_p-&gt;cpu_no_qs.b.norm, false);</td></tr>
<tr><th id="304">304</th><td>		barrier(); <i>/* Coordinate with rcu_preempt_check_callbacks(). */</i></td></tr>
<tr><th id="305">305</th><td>		current-&gt;rcu_read_unlock_special.b.need_qs = false;</td></tr>
<tr><th id="306">306</th><td>	}</td></tr>
<tr><th id="307">307</th><td>}</td></tr>
<tr><th id="308">308</th><td></td></tr>
<tr><th id="309">309</th><td><i>/*</i></td></tr>
<tr><th id="310">310</th><td><i> * We have entered the scheduler, and the current task might soon be</i></td></tr>
<tr><th id="311">311</th><td><i> * context-switched away from.  If this task is in an RCU read-side</i></td></tr>
<tr><th id="312">312</th><td><i> * critical section, we will no longer be able to rely on the CPU to</i></td></tr>
<tr><th id="313">313</th><td><i> * record that fact, so we enqueue the task on the blkd_tasks list.</i></td></tr>
<tr><th id="314">314</th><td><i> * The task will dequeue itself when it exits the outermost enclosing</i></td></tr>
<tr><th id="315">315</th><td><i> * RCU read-side critical section.  Therefore, the current grace period</i></td></tr>
<tr><th id="316">316</th><td><i> * cannot be permitted to complete until the blkd_tasks list entries</i></td></tr>
<tr><th id="317">317</th><td><i> * predating the current grace period drain, in other words, until</i></td></tr>
<tr><th id="318">318</th><td><i> * rnp-&gt;gp_tasks becomes NULL.</i></td></tr>
<tr><th id="319">319</th><td><i> *</i></td></tr>
<tr><th id="320">320</th><td><i> * Caller must disable interrupts.</i></td></tr>
<tr><th id="321">321</th><td><i> */</i></td></tr>
<tr><th id="322">322</th><td><em>static</em> <em>void</em> rcu_preempt_note_context_switch(bool preempt)</td></tr>
<tr><th id="323">323</th><td>{</td></tr>
<tr><th id="324">324</th><td>	<b>struct</b> task_struct *t = current;</td></tr>
<tr><th id="325">325</th><td>	<b>struct</b> rcu_data *rdp;</td></tr>
<tr><th id="326">326</th><td>	<b>struct</b> rcu_node *rnp;</td></tr>
<tr><th id="327">327</th><td></td></tr>
<tr><th id="328">328</th><td>	RCU_LOCKDEP_WARN(!irqs_disabled(), <q>"rcu_preempt_note_context_switch() invoked with interrupts enabled!!!\n"</q>);</td></tr>
<tr><th id="329">329</th><td>	WARN_ON_ONCE(!preempt &amp;&amp; t-&gt;rcu_read_lock_nesting &gt; <var>0</var>);</td></tr>
<tr><th id="330">330</th><td>	<b>if</b> (t-&gt;rcu_read_lock_nesting &gt; <var>0</var> &amp;&amp;</td></tr>
<tr><th id="331">331</th><td>	    !t-&gt;rcu_read_unlock_special.b.blocked) {</td></tr>
<tr><th id="332">332</th><td></td></tr>
<tr><th id="333">333</th><td>		<i>/* Possibly blocking in an RCU read-side critical section. */</i></td></tr>
<tr><th id="334">334</th><td>		rdp = this_cpu_ptr(rcu_state_p-&gt;rda);</td></tr>
<tr><th id="335">335</th><td>		rnp = rdp-&gt;mynode;</td></tr>
<tr><th id="336">336</th><td>		raw_spin_lock_rcu_node(rnp);</td></tr>
<tr><th id="337">337</th><td>		t-&gt;rcu_read_unlock_special.b.blocked = true;</td></tr>
<tr><th id="338">338</th><td>		t-&gt;rcu_blocked_node = rnp;</td></tr>
<tr><th id="339">339</th><td></td></tr>
<tr><th id="340">340</th><td>		<i>/*</i></td></tr>
<tr><th id="341">341</th><td><i>		 * Verify the CPU's sanity, trace the preemption, and</i></td></tr>
<tr><th id="342">342</th><td><i>		 * then queue the task as required based on the states</i></td></tr>
<tr><th id="343">343</th><td><i>		 * of any ongoing and expedited grace periods.</i></td></tr>
<tr><th id="344">344</th><td><i>		 */</i></td></tr>
<tr><th id="345">345</th><td>		WARN_ON_ONCE((rdp-&gt;grpmask &amp; rcu_rnp_online_cpus(rnp)) == <var>0</var>);</td></tr>
<tr><th id="346">346</th><td>		WARN_ON_ONCE(!list_empty(&amp;t-&gt;rcu_node_entry));</td></tr>
<tr><th id="347">347</th><td>		trace_rcu_preempt_task(rdp-&gt;rsp-&gt;name,</td></tr>
<tr><th id="348">348</th><td>				       t-&gt;pid,</td></tr>
<tr><th id="349">349</th><td>				       (rnp-&gt;qsmask &amp; rdp-&gt;grpmask)</td></tr>
<tr><th id="350">350</th><td>				       ? rnp-&gt;gpnum</td></tr>
<tr><th id="351">351</th><td>				       : rnp-&gt;gpnum + <var>1</var>);</td></tr>
<tr><th id="352">352</th><td>		rcu_preempt_ctxt_queue(rnp, rdp);</td></tr>
<tr><th id="353">353</th><td>	} <b>else</b> <b>if</b> (t-&gt;rcu_read_lock_nesting &lt; <var>0</var> &amp;&amp;</td></tr>
<tr><th id="354">354</th><td>		   t-&gt;rcu_read_unlock_special.s) {</td></tr>
<tr><th id="355">355</th><td></td></tr>
<tr><th id="356">356</th><td>		<i>/*</i></td></tr>
<tr><th id="357">357</th><td><i>		 * Complete exit from RCU read-side critical section on</i></td></tr>
<tr><th id="358">358</th><td><i>		 * behalf of preempted instance of __rcu_read_unlock().</i></td></tr>
<tr><th id="359">359</th><td><i>		 */</i></td></tr>
<tr><th id="360">360</th><td>		rcu_read_unlock_special(t);</td></tr>
<tr><th id="361">361</th><td>	}</td></tr>
<tr><th id="362">362</th><td></td></tr>
<tr><th id="363">363</th><td>	<i>/*</i></td></tr>
<tr><th id="364">364</th><td><i>	 * Either we were not in an RCU read-side critical section to</i></td></tr>
<tr><th id="365">365</th><td><i>	 * begin with, or we have now recorded that critical section</i></td></tr>
<tr><th id="366">366</th><td><i>	 * globally.  Either way, we can now note a quiescent state</i></td></tr>
<tr><th id="367">367</th><td><i>	 * for this CPU.  Again, if we were in an RCU read-side critical</i></td></tr>
<tr><th id="368">368</th><td><i>	 * section, and if that critical section was blocking the current</i></td></tr>
<tr><th id="369">369</th><td><i>	 * grace period, then the fact that the task has been enqueued</i></td></tr>
<tr><th id="370">370</th><td><i>	 * means that we continue to block the current grace period.</i></td></tr>
<tr><th id="371">371</th><td><i>	 */</i></td></tr>
<tr><th id="372">372</th><td>	rcu_preempt_qs();</td></tr>
<tr><th id="373">373</th><td>}</td></tr>
<tr><th id="374">374</th><td></td></tr>
<tr><th id="375">375</th><td><i>/*</i></td></tr>
<tr><th id="376">376</th><td><i> * Check for preempted RCU readers blocking the current grace period</i></td></tr>
<tr><th id="377">377</th><td><i> * for the specified rcu_node structure.  If the caller needs a reliable</i></td></tr>
<tr><th id="378">378</th><td><i> * answer, it must hold the rcu_node's -&gt;lock.</i></td></tr>
<tr><th id="379">379</th><td><i> */</i></td></tr>
<tr><th id="380">380</th><td><em>static</em> <em>int</em> rcu_preempt_blocked_readers_cgp(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="381">381</th><td>{</td></tr>
<tr><th id="382">382</th><td>	<b>return</b> rnp-&gt;gp_tasks != NULL;</td></tr>
<tr><th id="383">383</th><td>}</td></tr>
<tr><th id="384">384</th><td></td></tr>
<tr><th id="385">385</th><td><i>/*</i></td></tr>
<tr><th id="386">386</th><td><i> * Advance a -&gt;blkd_tasks-list pointer to the next entry, instead</i></td></tr>
<tr><th id="387">387</th><td><i> * returning NULL if at the end of the list.</i></td></tr>
<tr><th id="388">388</th><td><i> */</i></td></tr>
<tr><th id="389">389</th><td><em>static</em> <b>struct</b> list_head *rcu_next_node_entry(<b>struct</b> task_struct *t,</td></tr>
<tr><th id="390">390</th><td>					     <b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="391">391</th><td>{</td></tr>
<tr><th id="392">392</th><td>	<b>struct</b> list_head *np;</td></tr>
<tr><th id="393">393</th><td></td></tr>
<tr><th id="394">394</th><td>	np = t-&gt;rcu_node_entry.next;</td></tr>
<tr><th id="395">395</th><td>	<b>if</b> (np == &amp;rnp-&gt;blkd_tasks)</td></tr>
<tr><th id="396">396</th><td>		np = NULL;</td></tr>
<tr><th id="397">397</th><td>	<b>return</b> np;</td></tr>
<tr><th id="398">398</th><td>}</td></tr>
<tr><th id="399">399</th><td></td></tr>
<tr><th id="400">400</th><td><i>/*</i></td></tr>
<tr><th id="401">401</th><td><i> * Return true if the specified rcu_node structure has tasks that were</i></td></tr>
<tr><th id="402">402</th><td><i> * preempted within an RCU read-side critical section.</i></td></tr>
<tr><th id="403">403</th><td><i> */</i></td></tr>
<tr><th id="404">404</th><td><em>static</em> bool rcu_preempt_has_tasks(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="405">405</th><td>{</td></tr>
<tr><th id="406">406</th><td>	<b>return</b> !list_empty(&amp;rnp-&gt;blkd_tasks);</td></tr>
<tr><th id="407">407</th><td>}</td></tr>
<tr><th id="408">408</th><td></td></tr>
<tr><th id="409">409</th><td><i>/*</i></td></tr>
<tr><th id="410">410</th><td><i> * Handle special cases during rcu_read_unlock(), such as needing to</i></td></tr>
<tr><th id="411">411</th><td><i> * notify RCU core processing or task having blocked during the RCU</i></td></tr>
<tr><th id="412">412</th><td><i> * read-side critical section.</i></td></tr>
<tr><th id="413">413</th><td><i> */</i></td></tr>
<tr><th id="414">414</th><td><em>void</em> rcu_read_unlock_special(<b>struct</b> task_struct *t)</td></tr>
<tr><th id="415">415</th><td>{</td></tr>
<tr><th id="416">416</th><td>	bool empty_exp;</td></tr>
<tr><th id="417">417</th><td>	bool empty_norm;</td></tr>
<tr><th id="418">418</th><td>	bool empty_exp_now;</td></tr>
<tr><th id="419">419</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="420">420</th><td>	<b>struct</b> list_head *np;</td></tr>
<tr><th id="421">421</th><td>	bool drop_boost_mutex = false;</td></tr>
<tr><th id="422">422</th><td>	<b>struct</b> rcu_data *rdp;</td></tr>
<tr><th id="423">423</th><td>	<b>struct</b> rcu_node *rnp;</td></tr>
<tr><th id="424">424</th><td>	<b>union</b> rcu_special special;</td></tr>
<tr><th id="425">425</th><td></td></tr>
<tr><th id="426">426</th><td>	<i>/* NMI handlers cannot block and cannot safely manipulate state. */</i></td></tr>
<tr><th id="427">427</th><td>	<b>if</b> (in_nmi())</td></tr>
<tr><th id="428">428</th><td>		<b>return</b>;</td></tr>
<tr><th id="429">429</th><td></td></tr>
<tr><th id="430">430</th><td>	local_irq_save(flags);</td></tr>
<tr><th id="431">431</th><td></td></tr>
<tr><th id="432">432</th><td>	<i>/*</i></td></tr>
<tr><th id="433">433</th><td><i>	 * If RCU core is waiting for this CPU to exit its critical section,</i></td></tr>
<tr><th id="434">434</th><td><i>	 * report the fact that it has exited.  Because irqs are disabled,</i></td></tr>
<tr><th id="435">435</th><td><i>	 * t-&gt;rcu_read_unlock_special cannot change.</i></td></tr>
<tr><th id="436">436</th><td><i>	 */</i></td></tr>
<tr><th id="437">437</th><td>	special = t-&gt;rcu_read_unlock_special;</td></tr>
<tr><th id="438">438</th><td>	<b>if</b> (special.b.need_qs) {</td></tr>
<tr><th id="439">439</th><td>		rcu_preempt_qs();</td></tr>
<tr><th id="440">440</th><td>		t-&gt;rcu_read_unlock_special.b.need_qs = false;</td></tr>
<tr><th id="441">441</th><td>		<b>if</b> (!t-&gt;rcu_read_unlock_special.s) {</td></tr>
<tr><th id="442">442</th><td>			local_irq_restore(flags);</td></tr>
<tr><th id="443">443</th><td>			<b>return</b>;</td></tr>
<tr><th id="444">444</th><td>		}</td></tr>
<tr><th id="445">445</th><td>	}</td></tr>
<tr><th id="446">446</th><td></td></tr>
<tr><th id="447">447</th><td>	<i>/*</i></td></tr>
<tr><th id="448">448</th><td><i>	 * Respond to a request for an expedited grace period, but only if</i></td></tr>
<tr><th id="449">449</th><td><i>	 * we were not preempted, meaning that we were running on the same</i></td></tr>
<tr><th id="450">450</th><td><i>	 * CPU throughout.  If we were preempted, the exp_need_qs flag</i></td></tr>
<tr><th id="451">451</th><td><i>	 * would have been cleared at the time of the first preemption,</i></td></tr>
<tr><th id="452">452</th><td><i>	 * and the quiescent state would be reported when we were dequeued.</i></td></tr>
<tr><th id="453">453</th><td><i>	 */</i></td></tr>
<tr><th id="454">454</th><td>	<b>if</b> (special.b.exp_need_qs) {</td></tr>
<tr><th id="455">455</th><td>		WARN_ON_ONCE(special.b.blocked);</td></tr>
<tr><th id="456">456</th><td>		t-&gt;rcu_read_unlock_special.b.exp_need_qs = false;</td></tr>
<tr><th id="457">457</th><td>		rdp = this_cpu_ptr(rcu_state_p-&gt;rda);</td></tr>
<tr><th id="458">458</th><td>		rcu_report_exp_rdp(rcu_state_p, rdp, true);</td></tr>
<tr><th id="459">459</th><td>		<b>if</b> (!t-&gt;rcu_read_unlock_special.s) {</td></tr>
<tr><th id="460">460</th><td>			local_irq_restore(flags);</td></tr>
<tr><th id="461">461</th><td>			<b>return</b>;</td></tr>
<tr><th id="462">462</th><td>		}</td></tr>
<tr><th id="463">463</th><td>	}</td></tr>
<tr><th id="464">464</th><td></td></tr>
<tr><th id="465">465</th><td>	<i>/* Hardware IRQ handlers cannot block, complain if they get here. */</i></td></tr>
<tr><th id="466">466</th><td>	<b>if</b> (in_irq() || in_serving_softirq()) {</td></tr>
<tr><th id="467">467</th><td>		lockdep_rcu_suspicious(__FILE__, __LINE__,</td></tr>
<tr><th id="468">468</th><td>				       <q>"rcu_read_unlock() from irq or softirq with blocking in critical section!!!\n"</q>);</td></tr>
<tr><th id="469">469</th><td>		pr_alert(<q>"-&gt;rcu_read_unlock_special: %#x (b: %d, enq: %d nq: %d)\n"</q>,</td></tr>
<tr><th id="470">470</th><td>			 t-&gt;rcu_read_unlock_special.s,</td></tr>
<tr><th id="471">471</th><td>			 t-&gt;rcu_read_unlock_special.b.blocked,</td></tr>
<tr><th id="472">472</th><td>			 t-&gt;rcu_read_unlock_special.b.exp_need_qs,</td></tr>
<tr><th id="473">473</th><td>			 t-&gt;rcu_read_unlock_special.b.need_qs);</td></tr>
<tr><th id="474">474</th><td>		local_irq_restore(flags);</td></tr>
<tr><th id="475">475</th><td>		<b>return</b>;</td></tr>
<tr><th id="476">476</th><td>	}</td></tr>
<tr><th id="477">477</th><td></td></tr>
<tr><th id="478">478</th><td>	<i>/* Clean up if blocked during RCU read-side critical section. */</i></td></tr>
<tr><th id="479">479</th><td>	<b>if</b> (special.b.blocked) {</td></tr>
<tr><th id="480">480</th><td>		t-&gt;rcu_read_unlock_special.b.blocked = false;</td></tr>
<tr><th id="481">481</th><td></td></tr>
<tr><th id="482">482</th><td>		<i>/*</i></td></tr>
<tr><th id="483">483</th><td><i>		 * Remove this task from the list it blocked on.  The task</i></td></tr>
<tr><th id="484">484</th><td><i>		 * now remains queued on the rcu_node corresponding to the</i></td></tr>
<tr><th id="485">485</th><td><i>		 * CPU it first blocked on, so there is no longer any need</i></td></tr>
<tr><th id="486">486</th><td><i>		 * to loop.  Retain a WARN_ON_ONCE() out of sheer paranoia.</i></td></tr>
<tr><th id="487">487</th><td><i>		 */</i></td></tr>
<tr><th id="488">488</th><td>		rnp = t-&gt;rcu_blocked_node;</td></tr>
<tr><th id="489">489</th><td>		raw_spin_lock_rcu_node(rnp); <i>/* irqs already disabled. */</i></td></tr>
<tr><th id="490">490</th><td>		WARN_ON_ONCE(rnp != t-&gt;rcu_blocked_node);</td></tr>
<tr><th id="491">491</th><td>		WARN_ON_ONCE(rnp-&gt;level != rcu_num_lvls - <var>1</var>);</td></tr>
<tr><th id="492">492</th><td>		empty_norm = !rcu_preempt_blocked_readers_cgp(rnp);</td></tr>
<tr><th id="493">493</th><td>		empty_exp = sync_rcu_preempt_exp_done(rnp);</td></tr>
<tr><th id="494">494</th><td>		smp_mb(); <i>/* ensure expedited fastpath sees end of RCU c-s. */</i></td></tr>
<tr><th id="495">495</th><td>		np = rcu_next_node_entry(t, rnp);</td></tr>
<tr><th id="496">496</th><td>		list_del_init(&amp;t-&gt;rcu_node_entry);</td></tr>
<tr><th id="497">497</th><td>		t-&gt;rcu_blocked_node = NULL;</td></tr>
<tr><th id="498">498</th><td>		trace_rcu_unlock_preempted_task(TPS(<q>"rcu_preempt"</q>),</td></tr>
<tr><th id="499">499</th><td>						rnp-&gt;gpnum, t-&gt;pid);</td></tr>
<tr><th id="500">500</th><td>		<b>if</b> (&amp;t-&gt;rcu_node_entry == rnp-&gt;gp_tasks)</td></tr>
<tr><th id="501">501</th><td>			rnp-&gt;gp_tasks = np;</td></tr>
<tr><th id="502">502</th><td>		<b>if</b> (&amp;t-&gt;rcu_node_entry == rnp-&gt;exp_tasks)</td></tr>
<tr><th id="503">503</th><td>			rnp-&gt;exp_tasks = np;</td></tr>
<tr><th id="504">504</th><td>		<b>if</b> (IS_ENABLED(CONFIG_RCU_BOOST)) {</td></tr>
<tr><th id="505">505</th><td>			<i>/* Snapshot -&gt;boost_mtx ownership w/rnp-&gt;lock held. */</i></td></tr>
<tr><th id="506">506</th><td>			drop_boost_mutex = rt_mutex_owner(&amp;rnp-&gt;boost_mtx) == t;</td></tr>
<tr><th id="507">507</th><td>			<b>if</b> (&amp;t-&gt;rcu_node_entry == rnp-&gt;boost_tasks)</td></tr>
<tr><th id="508">508</th><td>				rnp-&gt;boost_tasks = np;</td></tr>
<tr><th id="509">509</th><td>		}</td></tr>
<tr><th id="510">510</th><td></td></tr>
<tr><th id="511">511</th><td>		<i>/*</i></td></tr>
<tr><th id="512">512</th><td><i>		 * If this was the last task on the current list, and if</i></td></tr>
<tr><th id="513">513</th><td><i>		 * we aren't waiting on any CPUs, report the quiescent state.</i></td></tr>
<tr><th id="514">514</th><td><i>		 * Note that rcu_report_unblock_qs_rnp() releases rnp-&gt;lock,</i></td></tr>
<tr><th id="515">515</th><td><i>		 * so we must take a snapshot of the expedited state.</i></td></tr>
<tr><th id="516">516</th><td><i>		 */</i></td></tr>
<tr><th id="517">517</th><td>		empty_exp_now = sync_rcu_preempt_exp_done(rnp);</td></tr>
<tr><th id="518">518</th><td>		<b>if</b> (!empty_norm &amp;&amp; !rcu_preempt_blocked_readers_cgp(rnp)) {</td></tr>
<tr><th id="519">519</th><td>			trace_rcu_quiescent_state_report(TPS(<q>"preempt_rcu"</q>),</td></tr>
<tr><th id="520">520</th><td>							 rnp-&gt;gpnum,</td></tr>
<tr><th id="521">521</th><td>							 <var>0</var>, rnp-&gt;qsmask,</td></tr>
<tr><th id="522">522</th><td>							 rnp-&gt;level,</td></tr>
<tr><th id="523">523</th><td>							 rnp-&gt;grplo,</td></tr>
<tr><th id="524">524</th><td>							 rnp-&gt;grphi,</td></tr>
<tr><th id="525">525</th><td>							 !!rnp-&gt;gp_tasks);</td></tr>
<tr><th id="526">526</th><td>			rcu_report_unblock_qs_rnp(rcu_state_p, rnp, flags);</td></tr>
<tr><th id="527">527</th><td>		} <b>else</b> {</td></tr>
<tr><th id="528">528</th><td>			raw_spin_unlock_irqrestore_rcu_node(rnp, flags);</td></tr>
<tr><th id="529">529</th><td>		}</td></tr>
<tr><th id="530">530</th><td></td></tr>
<tr><th id="531">531</th><td>		<i>/* Unboost if we were boosted. */</i></td></tr>
<tr><th id="532">532</th><td>		<b>if</b> (IS_ENABLED(CONFIG_RCU_BOOST) &amp;&amp; drop_boost_mutex)</td></tr>
<tr><th id="533">533</th><td>			rt_mutex_unlock(&amp;rnp-&gt;boost_mtx);</td></tr>
<tr><th id="534">534</th><td></td></tr>
<tr><th id="535">535</th><td>		<i>/*</i></td></tr>
<tr><th id="536">536</th><td><i>		 * If this was the last task on the expedited lists,</i></td></tr>
<tr><th id="537">537</th><td><i>		 * then we need to report up the rcu_node hierarchy.</i></td></tr>
<tr><th id="538">538</th><td><i>		 */</i></td></tr>
<tr><th id="539">539</th><td>		<b>if</b> (!empty_exp &amp;&amp; empty_exp_now)</td></tr>
<tr><th id="540">540</th><td>			rcu_report_exp_rnp(rcu_state_p, rnp, true);</td></tr>
<tr><th id="541">541</th><td>	} <b>else</b> {</td></tr>
<tr><th id="542">542</th><td>		local_irq_restore(flags);</td></tr>
<tr><th id="543">543</th><td>	}</td></tr>
<tr><th id="544">544</th><td>}</td></tr>
<tr><th id="545">545</th><td></td></tr>
<tr><th id="546">546</th><td><i>/*</i></td></tr>
<tr><th id="547">547</th><td><i> * Dump detailed information for all tasks blocking the current RCU</i></td></tr>
<tr><th id="548">548</th><td><i> * grace period on the specified rcu_node structure.</i></td></tr>
<tr><th id="549">549</th><td><i> */</i></td></tr>
<tr><th id="550">550</th><td><em>static</em> <em>void</em> rcu_print_detail_task_stall_rnp(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="551">551</th><td>{</td></tr>
<tr><th id="552">552</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="553">553</th><td>	<b>struct</b> task_struct *t;</td></tr>
<tr><th id="554">554</th><td></td></tr>
<tr><th id="555">555</th><td>	raw_spin_lock_irqsave_rcu_node(rnp, flags);</td></tr>
<tr><th id="556">556</th><td>	<b>if</b> (!rcu_preempt_blocked_readers_cgp(rnp)) {</td></tr>
<tr><th id="557">557</th><td>		raw_spin_unlock_irqrestore_rcu_node(rnp, flags);</td></tr>
<tr><th id="558">558</th><td>		<b>return</b>;</td></tr>
<tr><th id="559">559</th><td>	}</td></tr>
<tr><th id="560">560</th><td>	t = list_entry(rnp-&gt;gp_tasks-&gt;prev,</td></tr>
<tr><th id="561">561</th><td>		       <b>struct</b> task_struct, rcu_node_entry);</td></tr>
<tr><th id="562">562</th><td>	list_for_each_entry_continue(t, &amp;rnp-&gt;blkd_tasks, rcu_node_entry) {</td></tr>
<tr><th id="563">563</th><td>		<i>/*</i></td></tr>
<tr><th id="564">564</th><td><i>		 * We could be printing a lot while holding a spinlock.</i></td></tr>
<tr><th id="565">565</th><td><i>		 * Avoid triggering hard lockup.</i></td></tr>
<tr><th id="566">566</th><td><i>		 */</i></td></tr>
<tr><th id="567">567</th><td>		touch_nmi_watchdog();</td></tr>
<tr><th id="568">568</th><td>		sched_show_task(t);</td></tr>
<tr><th id="569">569</th><td>	}</td></tr>
<tr><th id="570">570</th><td>	raw_spin_unlock_irqrestore_rcu_node(rnp, flags);</td></tr>
<tr><th id="571">571</th><td>}</td></tr>
<tr><th id="572">572</th><td></td></tr>
<tr><th id="573">573</th><td><i>/*</i></td></tr>
<tr><th id="574">574</th><td><i> * Dump detailed information for all tasks blocking the current RCU</i></td></tr>
<tr><th id="575">575</th><td><i> * grace period.</i></td></tr>
<tr><th id="576">576</th><td><i> */</i></td></tr>
<tr><th id="577">577</th><td><em>static</em> <em>void</em> rcu_print_detail_task_stall(<b>struct</b> rcu_state *rsp)</td></tr>
<tr><th id="578">578</th><td>{</td></tr>
<tr><th id="579">579</th><td>	<b>struct</b> rcu_node *rnp = rcu_get_root(rsp);</td></tr>
<tr><th id="580">580</th><td></td></tr>
<tr><th id="581">581</th><td>	rcu_print_detail_task_stall_rnp(rnp);</td></tr>
<tr><th id="582">582</th><td>	rcu_for_each_leaf_node(rsp, rnp)</td></tr>
<tr><th id="583">583</th><td>		rcu_print_detail_task_stall_rnp(rnp);</td></tr>
<tr><th id="584">584</th><td>}</td></tr>
<tr><th id="585">585</th><td></td></tr>
<tr><th id="586">586</th><td><em>static</em> <em>void</em> rcu_print_task_stall_begin(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="587">587</th><td>{</td></tr>
<tr><th id="588">588</th><td>	pr_err(<q>"\tTasks blocked on level-%d rcu_node (CPUs %d-%d):"</q>,</td></tr>
<tr><th id="589">589</th><td>	       rnp-&gt;level, rnp-&gt;grplo, rnp-&gt;grphi);</td></tr>
<tr><th id="590">590</th><td>}</td></tr>
<tr><th id="591">591</th><td></td></tr>
<tr><th id="592">592</th><td><em>static</em> <em>void</em> rcu_print_task_stall_end(<em>void</em>)</td></tr>
<tr><th id="593">593</th><td>{</td></tr>
<tr><th id="594">594</th><td>	pr_cont(<q>"\n"</q>);</td></tr>
<tr><th id="595">595</th><td>}</td></tr>
<tr><th id="596">596</th><td></td></tr>
<tr><th id="597">597</th><td><i>/*</i></td></tr>
<tr><th id="598">598</th><td><i> * Scan the current list of tasks blocked within RCU read-side critical</i></td></tr>
<tr><th id="599">599</th><td><i> * sections, printing out the tid of each.</i></td></tr>
<tr><th id="600">600</th><td><i> */</i></td></tr>
<tr><th id="601">601</th><td><em>static</em> <em>int</em> rcu_print_task_stall(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="602">602</th><td>{</td></tr>
<tr><th id="603">603</th><td>	<b>struct</b> task_struct *t;</td></tr>
<tr><th id="604">604</th><td>	<em>int</em> ndetected = <var>0</var>;</td></tr>
<tr><th id="605">605</th><td></td></tr>
<tr><th id="606">606</th><td>	<b>if</b> (!rcu_preempt_blocked_readers_cgp(rnp))</td></tr>
<tr><th id="607">607</th><td>		<b>return</b> <var>0</var>;</td></tr>
<tr><th id="608">608</th><td>	rcu_print_task_stall_begin(rnp);</td></tr>
<tr><th id="609">609</th><td>	t = list_entry(rnp-&gt;gp_tasks-&gt;prev,</td></tr>
<tr><th id="610">610</th><td>		       <b>struct</b> task_struct, rcu_node_entry);</td></tr>
<tr><th id="611">611</th><td>	list_for_each_entry_continue(t, &amp;rnp-&gt;blkd_tasks, rcu_node_entry) {</td></tr>
<tr><th id="612">612</th><td>		pr_cont(<q>" P%d"</q>, t-&gt;pid);</td></tr>
<tr><th id="613">613</th><td>		ndetected++;</td></tr>
<tr><th id="614">614</th><td>	}</td></tr>
<tr><th id="615">615</th><td>	rcu_print_task_stall_end();</td></tr>
<tr><th id="616">616</th><td>	<b>return</b> ndetected;</td></tr>
<tr><th id="617">617</th><td>}</td></tr>
<tr><th id="618">618</th><td></td></tr>
<tr><th id="619">619</th><td><i>/*</i></td></tr>
<tr><th id="620">620</th><td><i> * Scan the current list of tasks blocked within RCU read-side critical</i></td></tr>
<tr><th id="621">621</th><td><i> * sections, printing out the tid of each that is blocking the current</i></td></tr>
<tr><th id="622">622</th><td><i> * expedited grace period.</i></td></tr>
<tr><th id="623">623</th><td><i> */</i></td></tr>
<tr><th id="624">624</th><td><em>static</em> <em>int</em> rcu_print_task_exp_stall(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="625">625</th><td>{</td></tr>
<tr><th id="626">626</th><td>	<b>struct</b> task_struct *t;</td></tr>
<tr><th id="627">627</th><td>	<em>int</em> ndetected = <var>0</var>;</td></tr>
<tr><th id="628">628</th><td></td></tr>
<tr><th id="629">629</th><td>	<b>if</b> (!rnp-&gt;exp_tasks)</td></tr>
<tr><th id="630">630</th><td>		<b>return</b> <var>0</var>;</td></tr>
<tr><th id="631">631</th><td>	t = list_entry(rnp-&gt;exp_tasks-&gt;prev,</td></tr>
<tr><th id="632">632</th><td>		       <b>struct</b> task_struct, rcu_node_entry);</td></tr>
<tr><th id="633">633</th><td>	list_for_each_entry_continue(t, &amp;rnp-&gt;blkd_tasks, rcu_node_entry) {</td></tr>
<tr><th id="634">634</th><td>		pr_cont(<q>" P%d"</q>, t-&gt;pid);</td></tr>
<tr><th id="635">635</th><td>		ndetected++;</td></tr>
<tr><th id="636">636</th><td>	}</td></tr>
<tr><th id="637">637</th><td>	<b>return</b> ndetected;</td></tr>
<tr><th id="638">638</th><td>}</td></tr>
<tr><th id="639">639</th><td></td></tr>
<tr><th id="640">640</th><td><i>/*</i></td></tr>
<tr><th id="641">641</th><td><i> * Check that the list of blocked tasks for the newly completed grace</i></td></tr>
<tr><th id="642">642</th><td><i> * period is in fact empty.  It is a serious bug to complete a grace</i></td></tr>
<tr><th id="643">643</th><td><i> * period that still has RCU readers blocked!  This function must be</i></td></tr>
<tr><th id="644">644</th><td><i> * invoked -before- updating this rnp's -&gt;gpnum, and the rnp's -&gt;lock</i></td></tr>
<tr><th id="645">645</th><td><i> * must be held by the caller.</i></td></tr>
<tr><th id="646">646</th><td><i> *</i></td></tr>
<tr><th id="647">647</th><td><i> * Also, if there are blocked tasks on the list, they automatically</i></td></tr>
<tr><th id="648">648</th><td><i> * block the newly created grace period, so set up -&gt;gp_tasks accordingly.</i></td></tr>
<tr><th id="649">649</th><td><i> */</i></td></tr>
<tr><th id="650">650</th><td><em>static</em> <em>void</em> rcu_preempt_check_blocked_tasks(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="651">651</th><td>{</td></tr>
<tr><th id="652">652</th><td>	<b>struct</b> task_struct *t;</td></tr>
<tr><th id="653">653</th><td></td></tr>
<tr><th id="654">654</th><td>	RCU_LOCKDEP_WARN(preemptible(), <q>"rcu_preempt_check_blocked_tasks() invoked with preemption enabled!!!\n"</q>);</td></tr>
<tr><th id="655">655</th><td>	WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp));</td></tr>
<tr><th id="656">656</th><td>	<b>if</b> (rcu_preempt_has_tasks(rnp)) {</td></tr>
<tr><th id="657">657</th><td>		rnp-&gt;gp_tasks = rnp-&gt;blkd_tasks.next;</td></tr>
<tr><th id="658">658</th><td>		t = container_of(rnp-&gt;gp_tasks, <b>struct</b> task_struct,</td></tr>
<tr><th id="659">659</th><td>				 rcu_node_entry);</td></tr>
<tr><th id="660">660</th><td>		trace_rcu_unlock_preempted_task(TPS(<q>"rcu_preempt-GPS"</q>),</td></tr>
<tr><th id="661">661</th><td>						rnp-&gt;gpnum, t-&gt;pid);</td></tr>
<tr><th id="662">662</th><td>	}</td></tr>
<tr><th id="663">663</th><td>	WARN_ON_ONCE(rnp-&gt;qsmask);</td></tr>
<tr><th id="664">664</th><td>}</td></tr>
<tr><th id="665">665</th><td></td></tr>
<tr><th id="666">666</th><td><i>/*</i></td></tr>
<tr><th id="667">667</th><td><i> * Check for a quiescent state from the current CPU.  When a task blocks,</i></td></tr>
<tr><th id="668">668</th><td><i> * the task is recorded in the corresponding CPU's rcu_node structure,</i></td></tr>
<tr><th id="669">669</th><td><i> * which is checked elsewhere.</i></td></tr>
<tr><th id="670">670</th><td><i> *</i></td></tr>
<tr><th id="671">671</th><td><i> * Caller must disable hard irqs.</i></td></tr>
<tr><th id="672">672</th><td><i> */</i></td></tr>
<tr><th id="673">673</th><td><em>static</em> <em>void</em> rcu_preempt_check_callbacks(<em>void</em>)</td></tr>
<tr><th id="674">674</th><td>{</td></tr>
<tr><th id="675">675</th><td>	<b>struct</b> task_struct *t = current;</td></tr>
<tr><th id="676">676</th><td></td></tr>
<tr><th id="677">677</th><td>	<b>if</b> (t-&gt;rcu_read_lock_nesting == <var>0</var>) {</td></tr>
<tr><th id="678">678</th><td>		rcu_preempt_qs();</td></tr>
<tr><th id="679">679</th><td>		<b>return</b>;</td></tr>
<tr><th id="680">680</th><td>	}</td></tr>
<tr><th id="681">681</th><td>	<b>if</b> (t-&gt;rcu_read_lock_nesting &gt; <var>0</var> &amp;&amp;</td></tr>
<tr><th id="682">682</th><td>	    __this_cpu_read(rcu_data_p-&gt;core_needs_qs) &amp;&amp;</td></tr>
<tr><th id="683">683</th><td>	    __this_cpu_read(rcu_data_p-&gt;cpu_no_qs.b.norm))</td></tr>
<tr><th id="684">684</th><td>		t-&gt;rcu_read_unlock_special.b.need_qs = true;</td></tr>
<tr><th id="685">685</th><td>}</td></tr>
<tr><th id="686">686</th><td></td></tr>
<tr><th id="687">687</th><td><u>#ifdef CONFIG_RCU_BOOST</u></td></tr>
<tr><th id="688">688</th><td></td></tr>
<tr><th id="689">689</th><td><em>static</em> <em>void</em> rcu_preempt_do_callbacks(<em>void</em>)</td></tr>
<tr><th id="690">690</th><td>{</td></tr>
<tr><th id="691">691</th><td>	rcu_do_batch(rcu_state_p, this_cpu_ptr(rcu_data_p));</td></tr>
<tr><th id="692">692</th><td>}</td></tr>
<tr><th id="693">693</th><td></td></tr>
<tr><th id="694">694</th><td><u>#endif /* #ifdef CONFIG_RCU_BOOST */</u></td></tr>
<tr><th id="695">695</th><td></td></tr>
<tr><th id="696">696</th><td><i class="doc">/**</i></td></tr>
<tr><th id="697">697</th><td><i class="doc"> * call_rcu() - Queue an RCU callback for invocation after a grace period.</i></td></tr>
<tr><th id="698">698</th><td><i class="doc"> *<span class="command"> @head</span>: structure to be used for queueing the RCU updates.</i></td></tr>
<tr><th id="699">699</th><td><i class="doc"> *<span class="command"> @func</span>: actual callback function to be invoked after the grace period</i></td></tr>
<tr><th id="700">700</th><td><i class="doc"> *</i></td></tr>
<tr><th id="701">701</th><td><i class="doc"> * The callback function will be invoked some time after a full grace</i></td></tr>
<tr><th id="702">702</th><td><i class="doc"> * period elapses, in other words after all pre-existing RCU read-side</i></td></tr>
<tr><th id="703">703</th><td><i class="doc"> * critical sections have completed.  However, the callback function</i></td></tr>
<tr><th id="704">704</th><td><i class="doc"> * might well execute concurrently with RCU read-side critical sections</i></td></tr>
<tr><th id="705">705</th><td><i class="doc"> * that started after call_rcu() was invoked.  RCU read-side critical</i></td></tr>
<tr><th id="706">706</th><td><i class="doc"> * sections are delimited by rcu_read_lock() and rcu_read_unlock(),</i></td></tr>
<tr><th id="707">707</th><td><i class="doc"> * and may be nested.</i></td></tr>
<tr><th id="708">708</th><td><i class="doc"> *</i></td></tr>
<tr><th id="709">709</th><td><i class="doc"> * Note that all CPUs must agree that the grace period extended beyond</i></td></tr>
<tr><th id="710">710</th><td><i class="doc"> * all pre-existing RCU read-side critical section.  On systems with more</i></td></tr>
<tr><th id="711">711</th><td><i class="doc"> * than one CPU, this means that when "func()" is invoked, each CPU is</i></td></tr>
<tr><th id="712">712</th><td><i class="doc"> * guaranteed to have executed a full memory barrier since the end of its</i></td></tr>
<tr><th id="713">713</th><td><i class="doc"> * last RCU read-side critical section whose beginning preceded the call</i></td></tr>
<tr><th id="714">714</th><td><i class="doc"> * to call_rcu().  It also means that each CPU executing an RCU read-side</i></td></tr>
<tr><th id="715">715</th><td><i class="doc"> * critical section that continues beyond the start of "func()" must have</i></td></tr>
<tr><th id="716">716</th><td><i class="doc"> * executed a memory barrier after the call_rcu() but before the beginning</i></td></tr>
<tr><th id="717">717</th><td><i class="doc"> * of that RCU read-side critical section.  Note that these guarantees</i></td></tr>
<tr><th id="718">718</th><td><i class="doc"> * include CPUs that are offline, idle, or executing in user mode, as</i></td></tr>
<tr><th id="719">719</th><td><i class="doc"> * well as CPUs that are executing in the kernel.</i></td></tr>
<tr><th id="720">720</th><td><i class="doc"> *</i></td></tr>
<tr><th id="721">721</th><td><i class="doc"> * Furthermore, if CPU A invoked call_rcu() and CPU B invoked the</i></td></tr>
<tr><th id="722">722</th><td><i class="doc"> * resulting RCU callback function "func()", then both CPU A and CPU B are</i></td></tr>
<tr><th id="723">723</th><td><i class="doc"> * guaranteed to execute a full memory barrier during the time interval</i></td></tr>
<tr><th id="724">724</th><td><i class="doc"> * between the call to call_rcu() and the invocation of "func()" -- even</i></td></tr>
<tr><th id="725">725</th><td><i class="doc"> * if CPU A and CPU B are the same CPU (but again only if the system has</i></td></tr>
<tr><th id="726">726</th><td><i class="doc"> * more than one CPU).</i></td></tr>
<tr><th id="727">727</th><td><i class="doc"> */</i></td></tr>
<tr><th id="728">728</th><td><em>void</em> call_rcu(<b>struct</b> rcu_head *head, rcu_callback_t func)</td></tr>
<tr><th id="729">729</th><td>{</td></tr>
<tr><th id="730">730</th><td>	__call_rcu(head, func, rcu_state_p, -<var>1</var>, <var>0</var>);</td></tr>
<tr><th id="731">731</th><td>}</td></tr>
<tr><th id="732">732</th><td>EXPORT_SYMBOL_GPL(call_rcu);</td></tr>
<tr><th id="733">733</th><td></td></tr>
<tr><th id="734">734</th><td><i class="doc">/**</i></td></tr>
<tr><th id="735">735</th><td><i class="doc"> * synchronize_rcu - wait until a grace period has elapsed.</i></td></tr>
<tr><th id="736">736</th><td><i class="doc"> *</i></td></tr>
<tr><th id="737">737</th><td><i class="doc"> * Control will return to the caller some time after a full grace</i></td></tr>
<tr><th id="738">738</th><td><i class="doc"> * period has elapsed, in other words after all currently executing RCU</i></td></tr>
<tr><th id="739">739</th><td><i class="doc"> * read-side critical sections have completed.  Note, however, that</i></td></tr>
<tr><th id="740">740</th><td><i class="doc"> * upon return from synchronize_rcu(), the caller might well be executing</i></td></tr>
<tr><th id="741">741</th><td><i class="doc"> * concurrently with new RCU read-side critical sections that began while</i></td></tr>
<tr><th id="742">742</th><td><i class="doc"> * synchronize_rcu() was waiting.  RCU read-side critical sections are</i></td></tr>
<tr><th id="743">743</th><td><i class="doc"> * delimited by rcu_read_lock() and rcu_read_unlock(), and may be nested.</i></td></tr>
<tr><th id="744">744</th><td><i class="doc"> *</i></td></tr>
<tr><th id="745">745</th><td><i class="doc"> * See the description of synchronize_sched() for more detailed</i></td></tr>
<tr><th id="746">746</th><td><i class="doc"> * information on memory-ordering guarantees.  However, please note</i></td></tr>
<tr><th id="747">747</th><td><i class="doc"> * that -only- the memory-ordering guarantees apply.  For example,</i></td></tr>
<tr><th id="748">748</th><td><i class="doc"> * synchronize_rcu() is -not- guaranteed to wait on things like code</i></td></tr>
<tr><th id="749">749</th><td><i class="doc"> * protected by preempt_disable(), instead, synchronize_rcu() is -only-</i></td></tr>
<tr><th id="750">750</th><td><i class="doc"> * guaranteed to wait on RCU read-side critical sections, that is, sections</i></td></tr>
<tr><th id="751">751</th><td><i class="doc"> * of code protected by rcu_read_lock().</i></td></tr>
<tr><th id="752">752</th><td><i class="doc"> */</i></td></tr>
<tr><th id="753">753</th><td><em>void</em> synchronize_rcu(<em>void</em>)</td></tr>
<tr><th id="754">754</th><td>{</td></tr>
<tr><th id="755">755</th><td>	RCU_LOCKDEP_WARN(lock_is_held(&amp;rcu_bh_lock_map) ||</td></tr>
<tr><th id="756">756</th><td>			 lock_is_held(&amp;rcu_lock_map) ||</td></tr>
<tr><th id="757">757</th><td>			 lock_is_held(&amp;rcu_sched_lock_map),</td></tr>
<tr><th id="758">758</th><td>			 <q>"Illegal synchronize_rcu() in RCU read-side critical section"</q>);</td></tr>
<tr><th id="759">759</th><td>	<b>if</b> (rcu_scheduler_active == RCU_SCHEDULER_INACTIVE)</td></tr>
<tr><th id="760">760</th><td>		<b>return</b>;</td></tr>
<tr><th id="761">761</th><td>	<b>if</b> (rcu_gp_is_expedited())</td></tr>
<tr><th id="762">762</th><td>		synchronize_rcu_expedited();</td></tr>
<tr><th id="763">763</th><td>	<b>else</b></td></tr>
<tr><th id="764">764</th><td>		wait_rcu_gp(call_rcu);</td></tr>
<tr><th id="765">765</th><td>}</td></tr>
<tr><th id="766">766</th><td>EXPORT_SYMBOL_GPL(synchronize_rcu);</td></tr>
<tr><th id="767">767</th><td></td></tr>
<tr><th id="768">768</th><td><i class="doc">/**</i></td></tr>
<tr><th id="769">769</th><td><i class="doc"> * rcu_barrier - Wait until all in-flight call_rcu() callbacks complete.</i></td></tr>
<tr><th id="770">770</th><td><i class="doc"> *</i></td></tr>
<tr><th id="771">771</th><td><i class="doc"> * Note that this primitive does not necessarily wait for an RCU grace period</i></td></tr>
<tr><th id="772">772</th><td><i class="doc"> * to complete.  For example, if there are no RCU callbacks queued anywhere</i></td></tr>
<tr><th id="773">773</th><td><i class="doc"> * in the system, then rcu_barrier() is within its rights to return</i></td></tr>
<tr><th id="774">774</th><td><i class="doc"> * immediately, without waiting for anything, much less an RCU grace period.</i></td></tr>
<tr><th id="775">775</th><td><i class="doc"> */</i></td></tr>
<tr><th id="776">776</th><td><em>void</em> rcu_barrier(<em>void</em>)</td></tr>
<tr><th id="777">777</th><td>{</td></tr>
<tr><th id="778">778</th><td>	_rcu_barrier(rcu_state_p);</td></tr>
<tr><th id="779">779</th><td>}</td></tr>
<tr><th id="780">780</th><td>EXPORT_SYMBOL_GPL(rcu_barrier);</td></tr>
<tr><th id="781">781</th><td></td></tr>
<tr><th id="782">782</th><td><i>/*</i></td></tr>
<tr><th id="783">783</th><td><i> * Initialize preemptible RCU's state structures.</i></td></tr>
<tr><th id="784">784</th><td><i> */</i></td></tr>
<tr><th id="785">785</th><td><em>static</em> <em>void</em> __init __rcu_init_preempt(<em>void</em>)</td></tr>
<tr><th id="786">786</th><td>{</td></tr>
<tr><th id="787">787</th><td>	rcu_init_one(rcu_state_p);</td></tr>
<tr><th id="788">788</th><td>}</td></tr>
<tr><th id="789">789</th><td></td></tr>
<tr><th id="790">790</th><td><i>/*</i></td></tr>
<tr><th id="791">791</th><td><i> * Check for a task exiting while in a preemptible-RCU read-side</i></td></tr>
<tr><th id="792">792</th><td><i> * critical section, clean up if so.  No need to issue warnings,</i></td></tr>
<tr><th id="793">793</th><td><i> * as debug_check_no_locks_held() already does this if lockdep</i></td></tr>
<tr><th id="794">794</th><td><i> * is enabled.</i></td></tr>
<tr><th id="795">795</th><td><i> */</i></td></tr>
<tr><th id="796">796</th><td><em>void</em> exit_rcu(<em>void</em>)</td></tr>
<tr><th id="797">797</th><td>{</td></tr>
<tr><th id="798">798</th><td>	<b>struct</b> task_struct *t = current;</td></tr>
<tr><th id="799">799</th><td></td></tr>
<tr><th id="800">800</th><td>	<b>if</b> (likely(list_empty(&amp;current-&gt;rcu_node_entry)))</td></tr>
<tr><th id="801">801</th><td>		<b>return</b>;</td></tr>
<tr><th id="802">802</th><td>	t-&gt;rcu_read_lock_nesting = <var>1</var>;</td></tr>
<tr><th id="803">803</th><td>	barrier();</td></tr>
<tr><th id="804">804</th><td>	t-&gt;rcu_read_unlock_special.b.blocked = true;</td></tr>
<tr><th id="805">805</th><td>	__rcu_read_unlock();</td></tr>
<tr><th id="806">806</th><td>}</td></tr>
<tr><th id="807">807</th><td></td></tr>
<tr><th id="808">808</th><td><u>#<span data-ppcond="121">else</span> /* #ifdef CONFIG_PREEMPT_RCU */</u></td></tr>
<tr><th id="809">809</th><td></td></tr>
<tr><th id="810">810</th><td><em>static</em> <b>struct</b> <a class="type" href="tree.h.html#rcu_state" title='rcu_state' data-ref="rcu_state">rcu_state</a> *<em>const</em> <dfn class="decl def" id="rcu_state_p" title='rcu_state_p' data-ref="rcu_state_p">rcu_state_p</dfn> = &amp;<a class="ref" href="tree.h.html#rcu_sched_state" title='rcu_sched_state' data-ref="rcu_sched_state">rcu_sched_state</a>;</td></tr>
<tr><th id="811">811</th><td></td></tr>
<tr><th id="812">812</th><td><i>/*</i></td></tr>
<tr><th id="813">813</th><td><i> * Tell them what RCU they are running.</i></td></tr>
<tr><th id="814">814</th><td><i> */</i></td></tr>
<tr><th id="815">815</th><td><em>static</em> <em>void</em> <a class="macro" href="../../include/linux/init.h.html#50" title="__attribute__ ((__section__(&quot;.init.text&quot;)))" data-ref="_M/__init">__init</a> <dfn class="decl def fn" id="rcu_bootup_announce" title='rcu_bootup_announce' data-ref="rcu_bootup_announce">rcu_bootup_announce</dfn>(<em>void</em>)</td></tr>
<tr><th id="816">816</th><td>{</td></tr>
<tr><th id="817">817</th><td>	<a class="macro" href="../../include/linux/printk.h.html#308" title="printk(&quot;\001&quot; &quot;6&quot; &quot;Hierarchical RCU implementation.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"Hierarchical RCU implementation.\n"</q>);</td></tr>
<tr><th id="818">818</th><td>	<a class="ref fn" href="#rcu_bootup_announce_oddness" title='rcu_bootup_announce_oddness' data-ref="rcu_bootup_announce_oddness">rcu_bootup_announce_oddness</a>();</td></tr>
<tr><th id="819">819</th><td>}</td></tr>
<tr><th id="820">820</th><td></td></tr>
<tr><th id="821">821</th><td><i>/*</i></td></tr>
<tr><th id="822">822</th><td><i> * Because preemptible RCU does not exist, we never have to check for</i></td></tr>
<tr><th id="823">823</th><td><i> * CPUs being in quiescent states.</i></td></tr>
<tr><th id="824">824</th><td><i> */</i></td></tr>
<tr><th id="825">825</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_preempt_note_context_switch" title='rcu_preempt_note_context_switch' data-ref="rcu_preempt_note_context_switch">rcu_preempt_note_context_switch</dfn>(<a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool">bool</a> <dfn class="local col0 decl" id="550preempt" title='preempt' data-type='bool' data-ref="550preempt">preempt</dfn>)</td></tr>
<tr><th id="826">826</th><td>{</td></tr>
<tr><th id="827">827</th><td>}</td></tr>
<tr><th id="828">828</th><td></td></tr>
<tr><th id="829">829</th><td><i>/*</i></td></tr>
<tr><th id="830">830</th><td><i> * Because preemptible RCU does not exist, there are never any preempted</i></td></tr>
<tr><th id="831">831</th><td><i> * RCU readers.</i></td></tr>
<tr><th id="832">832</th><td><i> */</i></td></tr>
<tr><th id="833">833</th><td><em>static</em> <em>int</em> <dfn class="decl def fn" id="rcu_preempt_blocked_readers_cgp" title='rcu_preempt_blocked_readers_cgp' data-ref="rcu_preempt_blocked_readers_cgp">rcu_preempt_blocked_readers_cgp</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node">rcu_node</a> *<dfn class="local col1 decl" id="551rnp" title='rnp' data-type='struct rcu_node *' data-ref="551rnp">rnp</dfn>)</td></tr>
<tr><th id="834">834</th><td>{</td></tr>
<tr><th id="835">835</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="836">836</th><td>}</td></tr>
<tr><th id="837">837</th><td></td></tr>
<tr><th id="838">838</th><td><i>/*</i></td></tr>
<tr><th id="839">839</th><td><i> * Because there is no preemptible RCU, there can be no readers blocked.</i></td></tr>
<tr><th id="840">840</th><td><i> */</i></td></tr>
<tr><th id="841">841</th><td><em>static</em> <a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool">bool</a> <dfn class="decl def fn" id="rcu_preempt_has_tasks" title='rcu_preempt_has_tasks' data-ref="rcu_preempt_has_tasks">rcu_preempt_has_tasks</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node">rcu_node</a> *<dfn class="local col2 decl" id="552rnp" title='rnp' data-type='struct rcu_node *' data-ref="552rnp">rnp</dfn>)</td></tr>
<tr><th id="842">842</th><td>{</td></tr>
<tr><th id="843">843</th><td>	<b>return</b> <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false">false</a>;</td></tr>
<tr><th id="844">844</th><td>}</td></tr>
<tr><th id="845">845</th><td></td></tr>
<tr><th id="846">846</th><td><i>/*</i></td></tr>
<tr><th id="847">847</th><td><i> * Because preemptible RCU does not exist, we never have to check for</i></td></tr>
<tr><th id="848">848</th><td><i> * tasks blocked within RCU read-side critical sections.</i></td></tr>
<tr><th id="849">849</th><td><i> */</i></td></tr>
<tr><th id="850">850</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_print_detail_task_stall" title='rcu_print_detail_task_stall' data-ref="rcu_print_detail_task_stall">rcu_print_detail_task_stall</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_state" title='rcu_state' data-ref="rcu_state">rcu_state</a> *<dfn class="local col3 decl" id="553rsp" title='rsp' data-type='struct rcu_state *' data-ref="553rsp">rsp</dfn>)</td></tr>
<tr><th id="851">851</th><td>{</td></tr>
<tr><th id="852">852</th><td>}</td></tr>
<tr><th id="853">853</th><td></td></tr>
<tr><th id="854">854</th><td><i>/*</i></td></tr>
<tr><th id="855">855</th><td><i> * Because preemptible RCU does not exist, we never have to check for</i></td></tr>
<tr><th id="856">856</th><td><i> * tasks blocked within RCU read-side critical sections.</i></td></tr>
<tr><th id="857">857</th><td><i> */</i></td></tr>
<tr><th id="858">858</th><td><em>static</em> <em>int</em> <dfn class="decl def fn" id="rcu_print_task_stall" title='rcu_print_task_stall' data-ref="rcu_print_task_stall">rcu_print_task_stall</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node">rcu_node</a> *<dfn class="local col4 decl" id="554rnp" title='rnp' data-type='struct rcu_node *' data-ref="554rnp">rnp</dfn>)</td></tr>
<tr><th id="859">859</th><td>{</td></tr>
<tr><th id="860">860</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="861">861</th><td>}</td></tr>
<tr><th id="862">862</th><td></td></tr>
<tr><th id="863">863</th><td><i>/*</i></td></tr>
<tr><th id="864">864</th><td><i> * Because preemptible RCU does not exist, we never have to check for</i></td></tr>
<tr><th id="865">865</th><td><i> * tasks blocked within RCU read-side critical sections that are</i></td></tr>
<tr><th id="866">866</th><td><i> * blocking the current expedited grace period.</i></td></tr>
<tr><th id="867">867</th><td><i> */</i></td></tr>
<tr><th id="868">868</th><td><em>static</em> <em>int</em> <dfn class="decl def fn" id="rcu_print_task_exp_stall" title='rcu_print_task_exp_stall' data-ref="rcu_print_task_exp_stall">rcu_print_task_exp_stall</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node">rcu_node</a> *<dfn class="local col5 decl" id="555rnp" title='rnp' data-type='struct rcu_node *' data-ref="555rnp">rnp</dfn>)</td></tr>
<tr><th id="869">869</th><td>{</td></tr>
<tr><th id="870">870</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="871">871</th><td>}</td></tr>
<tr><th id="872">872</th><td></td></tr>
<tr><th id="873">873</th><td><i>/*</i></td></tr>
<tr><th id="874">874</th><td><i> * Because there is no preemptible RCU, there can be no readers blocked,</i></td></tr>
<tr><th id="875">875</th><td><i> * so there is no need to check for blocked tasks.  So check only for</i></td></tr>
<tr><th id="876">876</th><td><i> * bogus qsmask values.</i></td></tr>
<tr><th id="877">877</th><td><i> */</i></td></tr>
<tr><th id="878">878</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_preempt_check_blocked_tasks" title='rcu_preempt_check_blocked_tasks' data-ref="rcu_preempt_check_blocked_tasks">rcu_preempt_check_blocked_tasks</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node">rcu_node</a> *<dfn class="local col6 decl" id="556rnp" title='rnp' data-type='struct rcu_node *' data-ref="556rnp">rnp</dfn>)</td></tr>
<tr><th id="879">879</th><td>{</td></tr>
<tr><th id="880">880</th><td>	<a class="macro" href="../../include/asm-generic/bug.h.html#66" title="({ int __ret_warn_on = !!(rnp-&gt;qsmask); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { asm volatile(&quot;1:\t&quot; &quot;.byte 0x0f, 0x0b&quot; &quot;\n&quot; &quot;.pushsection __bug_table,\&quot;aw\&quot;\n&quot; &quot;2:\t&quot; &quot;.long &quot; &quot;1b&quot; &quot; - 2b&quot; &quot;\t# bug_entry::bug_addr\n&quot; &quot;\t&quot; &quot;.long &quot; &quot;%c0&quot; &quot; - 2b&quot; &quot;\t# bug_entry::file\n&quot; &quot;\t.word %c1&quot; &quot;\t# bug_entry::line\n&quot; &quot;\t.word %c2&quot; &quot;\t# bug_entry::flags\n&quot; &quot;\t.org 2b+%c3\n&quot; &quot;.popsection&quot; : : &quot;i&quot; (&quot;/home/tempdban/kernel/stable/kernel/rcu/tree_plugin.h&quot;), &quot;i&quot; (880), &quot;i&quot; ((1 &lt;&lt; 0)|((1 &lt;&lt; 1)|((9) &lt;&lt; 8))), &quot;i&quot; (sizeof(struct bug_entry))); } while (0); ({ asm(&quot;%c0:\n\t&quot; &quot;.pushsection .discard.reachable\n\t&quot; &quot;.long %c0b - .\n\t&quot; &quot;.popsection\n\t&quot; : : &quot;i&quot; (178)); }); } while (0); __builtin_expect(!!(__ret_warn_on), 0); })" data-ref="_M/WARN_ON_ONCE">WARN_ON_ONCE</a>(<a class="local col6 ref" href="#556rnp" title='rnp' data-ref="556rnp">rnp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_node::qsmask" title='rcu_node::qsmask' data-ref="rcu_node::qsmask">qsmask</a>);</td></tr>
<tr><th id="881">881</th><td>}</td></tr>
<tr><th id="882">882</th><td></td></tr>
<tr><th id="883">883</th><td><i>/*</i></td></tr>
<tr><th id="884">884</th><td><i> * Because preemptible RCU does not exist, it never has any callbacks</i></td></tr>
<tr><th id="885">885</th><td><i> * to check.</i></td></tr>
<tr><th id="886">886</th><td><i> */</i></td></tr>
<tr><th id="887">887</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_preempt_check_callbacks" title='rcu_preempt_check_callbacks' data-ref="rcu_preempt_check_callbacks">rcu_preempt_check_callbacks</dfn>(<em>void</em>)</td></tr>
<tr><th id="888">888</th><td>{</td></tr>
<tr><th id="889">889</th><td>}</td></tr>
<tr><th id="890">890</th><td></td></tr>
<tr><th id="891">891</th><td><i>/*</i></td></tr>
<tr><th id="892">892</th><td><i> * Because preemptible RCU does not exist, rcu_barrier() is just</i></td></tr>
<tr><th id="893">893</th><td><i> * another name for rcu_barrier_sched().</i></td></tr>
<tr><th id="894">894</th><td><i> */</i></td></tr>
<tr><th id="895">895</th><td><em>void</em> <dfn class="decl def fn" id="rcu_barrier" title='rcu_barrier' data-ref="rcu_barrier">rcu_barrier</dfn>(<em>void</em>)</td></tr>
<tr><th id="896">896</th><td>{</td></tr>
<tr><th id="897">897</th><td>	<a class="ref fn" href="tree.c.html#rcu_barrier_sched" title='rcu_barrier_sched' data-ref="rcu_barrier_sched">rcu_barrier_sched</a>();</td></tr>
<tr><th id="898">898</th><td>}</td></tr>
<tr><th id="899">899</th><td><a class="macro" href="../../include/linux/export.h.html#106" title="extern typeof(rcu_barrier) rcu_barrier; static const char __kstrtab_rcu_barrier[] __attribute__((section(&quot;__ksymtab_strings&quot;), aligned(1))) = &quot;rcu_barrier&quot;; static const struct kernel_symbol __ksymtab_rcu_barrier __attribute__((__used__)) __attribute__((section(&quot;___ksymtab&quot; &quot;_gpl&quot; &quot;+&quot; &quot;rcu_barrier&quot;), used)) = { (unsigned long)&amp;rcu_barrier, __kstrtab_rcu_barrier }" data-ref="_M/EXPORT_SYMBOL_GPL">EXPORT_SYMBOL_GPL</a>(<a class="decl fn" href="#rcu_barrier" title='rcu_barrier' data-ref="rcu_barrier"><a class="ref fn" href="#rcu_barrier" title='rcu_barrier' data-ref="rcu_barrier"><a class="ref fn" href="#rcu_barrier" title='rcu_barrier' data-ref="rcu_barrier"><a class="ref fn" href="#rcu_barrier" title='rcu_barrier' data-ref="rcu_barrier">rcu_barrier</a></a></a></a>);</td></tr>
<tr><th id="900">900</th><td></td></tr>
<tr><th id="901">901</th><td><i>/*</i></td></tr>
<tr><th id="902">902</th><td><i> * Because preemptible RCU does not exist, it need not be initialized.</i></td></tr>
<tr><th id="903">903</th><td><i> */</i></td></tr>
<tr><th id="904">904</th><td><em>static</em> <em>void</em> <a class="macro" href="../../include/linux/init.h.html#50" title="__attribute__ ((__section__(&quot;.init.text&quot;)))" data-ref="_M/__init">__init</a> <dfn class="decl def fn" id="__rcu_init_preempt" title='__rcu_init_preempt' data-ref="__rcu_init_preempt">__rcu_init_preempt</dfn>(<em>void</em>)</td></tr>
<tr><th id="905">905</th><td>{</td></tr>
<tr><th id="906">906</th><td>}</td></tr>
<tr><th id="907">907</th><td></td></tr>
<tr><th id="908">908</th><td><i>/*</i></td></tr>
<tr><th id="909">909</th><td><i> * Because preemptible RCU does not exist, tasks cannot possibly exit</i></td></tr>
<tr><th id="910">910</th><td><i> * while in preemptible RCU read-side critical sections.</i></td></tr>
<tr><th id="911">911</th><td><i> */</i></td></tr>
<tr><th id="912">912</th><td><em>void</em> <dfn class="decl def fn" id="exit_rcu" title='exit_rcu' data-ref="exit_rcu">exit_rcu</dfn>(<em>void</em>)</td></tr>
<tr><th id="913">913</th><td>{</td></tr>
<tr><th id="914">914</th><td>}</td></tr>
<tr><th id="915">915</th><td></td></tr>
<tr><th id="916">916</th><td><u>#<span data-ppcond="121">endif</span> /* #else #ifdef CONFIG_PREEMPT_RCU */</u></td></tr>
<tr><th id="917">917</th><td></td></tr>
<tr><th id="918">918</th><td><u>#<span data-ppcond="918">ifdef</span> <span class="macro" data-ref="_M/CONFIG_RCU_BOOST">CONFIG_RCU_BOOST</span></u></td></tr>
<tr><th id="919">919</th><td></td></tr>
<tr><th id="920">920</th><td><u>#include "../locking/rtmutex_common.h"</u></td></tr>
<tr><th id="921">921</th><td></td></tr>
<tr><th id="922">922</th><td><em>static</em> <em>void</em> rcu_wake_cond(<b>struct</b> task_struct *t, <em>int</em> status)</td></tr>
<tr><th id="923">923</th><td>{</td></tr>
<tr><th id="924">924</th><td>	<i>/*</i></td></tr>
<tr><th id="925">925</th><td><i>	 * If the thread is yielding, only wake it when this</i></td></tr>
<tr><th id="926">926</th><td><i>	 * is invoked from idle</i></td></tr>
<tr><th id="927">927</th><td><i>	 */</i></td></tr>
<tr><th id="928">928</th><td>	<b>if</b> (status != RCU_KTHREAD_YIELDING || is_idle_task(current))</td></tr>
<tr><th id="929">929</th><td>		wake_up_process(t);</td></tr>
<tr><th id="930">930</th><td>}</td></tr>
<tr><th id="931">931</th><td></td></tr>
<tr><th id="932">932</th><td><i>/*</i></td></tr>
<tr><th id="933">933</th><td><i> * Carry out RCU priority boosting on the task indicated by -&gt;exp_tasks</i></td></tr>
<tr><th id="934">934</th><td><i> * or -&gt;boost_tasks, advancing the pointer to the next task in the</i></td></tr>
<tr><th id="935">935</th><td><i> * -&gt;blkd_tasks list.</i></td></tr>
<tr><th id="936">936</th><td><i> *</i></td></tr>
<tr><th id="937">937</th><td><i> * Note that irqs must be enabled: boosting the task can block.</i></td></tr>
<tr><th id="938">938</th><td><i> * Returns 1 if there are more tasks needing to be boosted.</i></td></tr>
<tr><th id="939">939</th><td><i> */</i></td></tr>
<tr><th id="940">940</th><td><em>static</em> <em>int</em> rcu_boost(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="941">941</th><td>{</td></tr>
<tr><th id="942">942</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="943">943</th><td>	<b>struct</b> task_struct *t;</td></tr>
<tr><th id="944">944</th><td>	<b>struct</b> list_head *tb;</td></tr>
<tr><th id="945">945</th><td></td></tr>
<tr><th id="946">946</th><td>	<b>if</b> (READ_ONCE(rnp-&gt;exp_tasks) == NULL &amp;&amp;</td></tr>
<tr><th id="947">947</th><td>	    READ_ONCE(rnp-&gt;boost_tasks) == NULL)</td></tr>
<tr><th id="948">948</th><td>		<b>return</b> <var>0</var>;  <i>/* Nothing left to boost. */</i></td></tr>
<tr><th id="949">949</th><td></td></tr>
<tr><th id="950">950</th><td>	raw_spin_lock_irqsave_rcu_node(rnp, flags);</td></tr>
<tr><th id="951">951</th><td></td></tr>
<tr><th id="952">952</th><td>	<i>/*</i></td></tr>
<tr><th id="953">953</th><td><i>	 * Recheck under the lock: all tasks in need of boosting</i></td></tr>
<tr><th id="954">954</th><td><i>	 * might exit their RCU read-side critical sections on their own.</i></td></tr>
<tr><th id="955">955</th><td><i>	 */</i></td></tr>
<tr><th id="956">956</th><td>	<b>if</b> (rnp-&gt;exp_tasks == NULL &amp;&amp; rnp-&gt;boost_tasks == NULL) {</td></tr>
<tr><th id="957">957</th><td>		raw_spin_unlock_irqrestore_rcu_node(rnp, flags);</td></tr>
<tr><th id="958">958</th><td>		<b>return</b> <var>0</var>;</td></tr>
<tr><th id="959">959</th><td>	}</td></tr>
<tr><th id="960">960</th><td></td></tr>
<tr><th id="961">961</th><td>	<i>/*</i></td></tr>
<tr><th id="962">962</th><td><i>	 * Preferentially boost tasks blocking expedited grace periods.</i></td></tr>
<tr><th id="963">963</th><td><i>	 * This cannot starve the normal grace periods because a second</i></td></tr>
<tr><th id="964">964</th><td><i>	 * expedited grace period must boost all blocked tasks, including</i></td></tr>
<tr><th id="965">965</th><td><i>	 * those blocking the pre-existing normal grace period.</i></td></tr>
<tr><th id="966">966</th><td><i>	 */</i></td></tr>
<tr><th id="967">967</th><td>	<b>if</b> (rnp-&gt;exp_tasks != NULL) {</td></tr>
<tr><th id="968">968</th><td>		tb = rnp-&gt;exp_tasks;</td></tr>
<tr><th id="969">969</th><td>		rnp-&gt;n_exp_boosts++;</td></tr>
<tr><th id="970">970</th><td>	} <b>else</b> {</td></tr>
<tr><th id="971">971</th><td>		tb = rnp-&gt;boost_tasks;</td></tr>
<tr><th id="972">972</th><td>		rnp-&gt;n_normal_boosts++;</td></tr>
<tr><th id="973">973</th><td>	}</td></tr>
<tr><th id="974">974</th><td>	rnp-&gt;n_tasks_boosted++;</td></tr>
<tr><th id="975">975</th><td></td></tr>
<tr><th id="976">976</th><td>	<i>/*</i></td></tr>
<tr><th id="977">977</th><td><i>	 * We boost task t by manufacturing an rt_mutex that appears to</i></td></tr>
<tr><th id="978">978</th><td><i>	 * be held by task t.  We leave a pointer to that rt_mutex where</i></td></tr>
<tr><th id="979">979</th><td><i>	 * task t can find it, and task t will release the mutex when it</i></td></tr>
<tr><th id="980">980</th><td><i>	 * exits its outermost RCU read-side critical section.  Then</i></td></tr>
<tr><th id="981">981</th><td><i>	 * simply acquiring this artificial rt_mutex will boost task</i></td></tr>
<tr><th id="982">982</th><td><i>	 * t's priority.  (Thanks to tglx for suggesting this approach!)</i></td></tr>
<tr><th id="983">983</th><td><i>	 *</i></td></tr>
<tr><th id="984">984</th><td><i>	 * Note that task t must acquire rnp-&gt;lock to remove itself from</i></td></tr>
<tr><th id="985">985</th><td><i>	 * the -&gt;blkd_tasks list, which it will do from exit() if from</i></td></tr>
<tr><th id="986">986</th><td><i>	 * nowhere else.  We therefore are guaranteed that task t will</i></td></tr>
<tr><th id="987">987</th><td><i>	 * stay around at least until we drop rnp-&gt;lock.  Note that</i></td></tr>
<tr><th id="988">988</th><td><i>	 * rnp-&gt;lock also resolves races between our priority boosting</i></td></tr>
<tr><th id="989">989</th><td><i>	 * and task t's exiting its outermost RCU read-side critical</i></td></tr>
<tr><th id="990">990</th><td><i>	 * section.</i></td></tr>
<tr><th id="991">991</th><td><i>	 */</i></td></tr>
<tr><th id="992">992</th><td>	t = container_of(tb, <b>struct</b> task_struct, rcu_node_entry);</td></tr>
<tr><th id="993">993</th><td>	rt_mutex_init_proxy_locked(&amp;rnp-&gt;boost_mtx, t);</td></tr>
<tr><th id="994">994</th><td>	raw_spin_unlock_irqrestore_rcu_node(rnp, flags);</td></tr>
<tr><th id="995">995</th><td>	<i>/* Lock only for side effect: boosts task t's priority. */</i></td></tr>
<tr><th id="996">996</th><td>	rt_mutex_lock(&amp;rnp-&gt;boost_mtx);</td></tr>
<tr><th id="997">997</th><td>	rt_mutex_unlock(&amp;rnp-&gt;boost_mtx);  <i>/* Then keep lockdep happy. */</i></td></tr>
<tr><th id="998">998</th><td></td></tr>
<tr><th id="999">999</th><td>	<b>return</b> READ_ONCE(rnp-&gt;exp_tasks) != NULL ||</td></tr>
<tr><th id="1000">1000</th><td>	       READ_ONCE(rnp-&gt;boost_tasks) != NULL;</td></tr>
<tr><th id="1001">1001</th><td>}</td></tr>
<tr><th id="1002">1002</th><td></td></tr>
<tr><th id="1003">1003</th><td><i>/*</i></td></tr>
<tr><th id="1004">1004</th><td><i> * Priority-boosting kthread, one per leaf rcu_node.</i></td></tr>
<tr><th id="1005">1005</th><td><i> */</i></td></tr>
<tr><th id="1006">1006</th><td><em>static</em> <em>int</em> rcu_boost_kthread(<em>void</em> *arg)</td></tr>
<tr><th id="1007">1007</th><td>{</td></tr>
<tr><th id="1008">1008</th><td>	<b>struct</b> rcu_node *rnp = (<b>struct</b> rcu_node *)arg;</td></tr>
<tr><th id="1009">1009</th><td>	<em>int</em> spincnt = <var>0</var>;</td></tr>
<tr><th id="1010">1010</th><td>	<em>int</em> more2boost;</td></tr>
<tr><th id="1011">1011</th><td></td></tr>
<tr><th id="1012">1012</th><td>	trace_rcu_utilization(TPS(<q>"Start boost kthread@init"</q>));</td></tr>
<tr><th id="1013">1013</th><td>	<b>for</b> (;;) {</td></tr>
<tr><th id="1014">1014</th><td>		rnp-&gt;boost_kthread_status = RCU_KTHREAD_WAITING;</td></tr>
<tr><th id="1015">1015</th><td>		trace_rcu_utilization(TPS(<q>"End boost kthread@rcu_wait"</q>));</td></tr>
<tr><th id="1016">1016</th><td>		rcu_wait(rnp-&gt;boost_tasks || rnp-&gt;exp_tasks);</td></tr>
<tr><th id="1017">1017</th><td>		trace_rcu_utilization(TPS(<q>"Start boost kthread@rcu_wait"</q>));</td></tr>
<tr><th id="1018">1018</th><td>		rnp-&gt;boost_kthread_status = RCU_KTHREAD_RUNNING;</td></tr>
<tr><th id="1019">1019</th><td>		more2boost = rcu_boost(rnp);</td></tr>
<tr><th id="1020">1020</th><td>		<b>if</b> (more2boost)</td></tr>
<tr><th id="1021">1021</th><td>			spincnt++;</td></tr>
<tr><th id="1022">1022</th><td>		<b>else</b></td></tr>
<tr><th id="1023">1023</th><td>			spincnt = <var>0</var>;</td></tr>
<tr><th id="1024">1024</th><td>		<b>if</b> (spincnt &gt; <var>10</var>) {</td></tr>
<tr><th id="1025">1025</th><td>			rnp-&gt;boost_kthread_status = RCU_KTHREAD_YIELDING;</td></tr>
<tr><th id="1026">1026</th><td>			trace_rcu_utilization(TPS(<q>"End boost kthread@rcu_yield"</q>));</td></tr>
<tr><th id="1027">1027</th><td>			schedule_timeout_interruptible(<var>2</var>);</td></tr>
<tr><th id="1028">1028</th><td>			trace_rcu_utilization(TPS(<q>"Start boost kthread@rcu_yield"</q>));</td></tr>
<tr><th id="1029">1029</th><td>			spincnt = <var>0</var>;</td></tr>
<tr><th id="1030">1030</th><td>		}</td></tr>
<tr><th id="1031">1031</th><td>	}</td></tr>
<tr><th id="1032">1032</th><td>	<i>/* NOTREACHED */</i></td></tr>
<tr><th id="1033">1033</th><td>	trace_rcu_utilization(TPS(<q>"End boost kthread@notreached"</q>));</td></tr>
<tr><th id="1034">1034</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1035">1035</th><td>}</td></tr>
<tr><th id="1036">1036</th><td></td></tr>
<tr><th id="1037">1037</th><td><i>/*</i></td></tr>
<tr><th id="1038">1038</th><td><i> * Check to see if it is time to start boosting RCU readers that are</i></td></tr>
<tr><th id="1039">1039</th><td><i> * blocking the current grace period, and, if so, tell the per-rcu_node</i></td></tr>
<tr><th id="1040">1040</th><td><i> * kthread to start boosting them.  If there is an expedited grace</i></td></tr>
<tr><th id="1041">1041</th><td><i> * period in progress, it is always time to boost.</i></td></tr>
<tr><th id="1042">1042</th><td><i> *</i></td></tr>
<tr><th id="1043">1043</th><td><i> * The caller must hold rnp-&gt;lock, which this function releases.</i></td></tr>
<tr><th id="1044">1044</th><td><i> * The -&gt;boost_kthread_task is immortal, so we don't need to worry</i></td></tr>
<tr><th id="1045">1045</th><td><i> * about it going away.</i></td></tr>
<tr><th id="1046">1046</th><td><i> */</i></td></tr>
<tr><th id="1047">1047</th><td><em>static</em> <em>void</em> rcu_initiate_boost(<b>struct</b> rcu_node *rnp, <em>unsigned</em> <em>long</em> flags)</td></tr>
<tr><th id="1048">1048</th><td>	__releases(rnp-&gt;lock)</td></tr>
<tr><th id="1049">1049</th><td>{</td></tr>
<tr><th id="1050">1050</th><td>	<b>struct</b> task_struct *t;</td></tr>
<tr><th id="1051">1051</th><td></td></tr>
<tr><th id="1052">1052</th><td>	lockdep_assert_held(&amp;rnp-&gt;lock);</td></tr>
<tr><th id="1053">1053</th><td>	<b>if</b> (!rcu_preempt_blocked_readers_cgp(rnp) &amp;&amp; rnp-&gt;exp_tasks == NULL) {</td></tr>
<tr><th id="1054">1054</th><td>		raw_spin_unlock_irqrestore_rcu_node(rnp, flags);</td></tr>
<tr><th id="1055">1055</th><td>		<b>return</b>;</td></tr>
<tr><th id="1056">1056</th><td>	}</td></tr>
<tr><th id="1057">1057</th><td>	<b>if</b> (rnp-&gt;exp_tasks != NULL ||</td></tr>
<tr><th id="1058">1058</th><td>	    (rnp-&gt;gp_tasks != NULL &amp;&amp;</td></tr>
<tr><th id="1059">1059</th><td>	     rnp-&gt;boost_tasks == NULL &amp;&amp;</td></tr>
<tr><th id="1060">1060</th><td>	     rnp-&gt;qsmask == <var>0</var> &amp;&amp;</td></tr>
<tr><th id="1061">1061</th><td>	     ULONG_CMP_GE(jiffies, rnp-&gt;boost_time))) {</td></tr>
<tr><th id="1062">1062</th><td>		<b>if</b> (rnp-&gt;exp_tasks == NULL)</td></tr>
<tr><th id="1063">1063</th><td>			rnp-&gt;boost_tasks = rnp-&gt;gp_tasks;</td></tr>
<tr><th id="1064">1064</th><td>		raw_spin_unlock_irqrestore_rcu_node(rnp, flags);</td></tr>
<tr><th id="1065">1065</th><td>		t = rnp-&gt;boost_kthread_task;</td></tr>
<tr><th id="1066">1066</th><td>		<b>if</b> (t)</td></tr>
<tr><th id="1067">1067</th><td>			rcu_wake_cond(t, rnp-&gt;boost_kthread_status);</td></tr>
<tr><th id="1068">1068</th><td>	} <b>else</b> {</td></tr>
<tr><th id="1069">1069</th><td>		raw_spin_unlock_irqrestore_rcu_node(rnp, flags);</td></tr>
<tr><th id="1070">1070</th><td>	}</td></tr>
<tr><th id="1071">1071</th><td>}</td></tr>
<tr><th id="1072">1072</th><td></td></tr>
<tr><th id="1073">1073</th><td><i>/*</i></td></tr>
<tr><th id="1074">1074</th><td><i> * Wake up the per-CPU kthread to invoke RCU callbacks.</i></td></tr>
<tr><th id="1075">1075</th><td><i> */</i></td></tr>
<tr><th id="1076">1076</th><td><em>static</em> <em>void</em> invoke_rcu_callbacks_kthread(<em>void</em>)</td></tr>
<tr><th id="1077">1077</th><td>{</td></tr>
<tr><th id="1078">1078</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="1079">1079</th><td></td></tr>
<tr><th id="1080">1080</th><td>	local_irq_save(flags);</td></tr>
<tr><th id="1081">1081</th><td>	__this_cpu_write(rcu_cpu_has_work, <var>1</var>);</td></tr>
<tr><th id="1082">1082</th><td>	<b>if</b> (__this_cpu_read(rcu_cpu_kthread_task) != NULL &amp;&amp;</td></tr>
<tr><th id="1083">1083</th><td>	    current != __this_cpu_read(rcu_cpu_kthread_task)) {</td></tr>
<tr><th id="1084">1084</th><td>		rcu_wake_cond(__this_cpu_read(rcu_cpu_kthread_task),</td></tr>
<tr><th id="1085">1085</th><td>			      __this_cpu_read(rcu_cpu_kthread_status));</td></tr>
<tr><th id="1086">1086</th><td>	}</td></tr>
<tr><th id="1087">1087</th><td>	local_irq_restore(flags);</td></tr>
<tr><th id="1088">1088</th><td>}</td></tr>
<tr><th id="1089">1089</th><td></td></tr>
<tr><th id="1090">1090</th><td><i>/*</i></td></tr>
<tr><th id="1091">1091</th><td><i> * Is the current CPU running the RCU-callbacks kthread?</i></td></tr>
<tr><th id="1092">1092</th><td><i> * Caller must have preemption disabled.</i></td></tr>
<tr><th id="1093">1093</th><td><i> */</i></td></tr>
<tr><th id="1094">1094</th><td><em>static</em> bool rcu_is_callbacks_kthread(<em>void</em>)</td></tr>
<tr><th id="1095">1095</th><td>{</td></tr>
<tr><th id="1096">1096</th><td>	<b>return</b> __this_cpu_read(rcu_cpu_kthread_task) == current;</td></tr>
<tr><th id="1097">1097</th><td>}</td></tr>
<tr><th id="1098">1098</th><td></td></tr>
<tr><th id="1099">1099</th><td><u>#define RCU_BOOST_DELAY_JIFFIES DIV_ROUND_UP(CONFIG_RCU_BOOST_DELAY * HZ, 1000)</u></td></tr>
<tr><th id="1100">1100</th><td></td></tr>
<tr><th id="1101">1101</th><td><i>/*</i></td></tr>
<tr><th id="1102">1102</th><td><i> * Do priority-boost accounting for the start of a new grace period.</i></td></tr>
<tr><th id="1103">1103</th><td><i> */</i></td></tr>
<tr><th id="1104">1104</th><td><em>static</em> <em>void</em> rcu_preempt_boost_start_gp(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="1105">1105</th><td>{</td></tr>
<tr><th id="1106">1106</th><td>	rnp-&gt;boost_time = jiffies + RCU_BOOST_DELAY_JIFFIES;</td></tr>
<tr><th id="1107">1107</th><td>}</td></tr>
<tr><th id="1108">1108</th><td></td></tr>
<tr><th id="1109">1109</th><td><i>/*</i></td></tr>
<tr><th id="1110">1110</th><td><i> * Create an RCU-boost kthread for the specified node if one does not</i></td></tr>
<tr><th id="1111">1111</th><td><i> * already exist.  We only create this kthread for preemptible RCU.</i></td></tr>
<tr><th id="1112">1112</th><td><i> * Returns zero if all is well, a negated errno otherwise.</i></td></tr>
<tr><th id="1113">1113</th><td><i> */</i></td></tr>
<tr><th id="1114">1114</th><td><em>static</em> <em>int</em> rcu_spawn_one_boost_kthread(<b>struct</b> rcu_state *rsp,</td></tr>
<tr><th id="1115">1115</th><td>				       <b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="1116">1116</th><td>{</td></tr>
<tr><th id="1117">1117</th><td>	<em>int</em> rnp_index = rnp - &amp;rsp-&gt;node[<var>0</var>];</td></tr>
<tr><th id="1118">1118</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="1119">1119</th><td>	<b>struct</b> sched_param sp;</td></tr>
<tr><th id="1120">1120</th><td>	<b>struct</b> task_struct *t;</td></tr>
<tr><th id="1121">1121</th><td></td></tr>
<tr><th id="1122">1122</th><td>	<b>if</b> (rcu_state_p != rsp)</td></tr>
<tr><th id="1123">1123</th><td>		<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1124">1124</th><td></td></tr>
<tr><th id="1125">1125</th><td>	<b>if</b> (!rcu_scheduler_fully_active || rcu_rnp_online_cpus(rnp) == <var>0</var>)</td></tr>
<tr><th id="1126">1126</th><td>		<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1127">1127</th><td></td></tr>
<tr><th id="1128">1128</th><td>	rsp-&gt;boost = <var>1</var>;</td></tr>
<tr><th id="1129">1129</th><td>	<b>if</b> (rnp-&gt;boost_kthread_task != NULL)</td></tr>
<tr><th id="1130">1130</th><td>		<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1131">1131</th><td>	t = kthread_create(rcu_boost_kthread, (<em>void</em> *)rnp,</td></tr>
<tr><th id="1132">1132</th><td>			   <q>"rcub/%d"</q>, rnp_index);</td></tr>
<tr><th id="1133">1133</th><td>	<b>if</b> (IS_ERR(t))</td></tr>
<tr><th id="1134">1134</th><td>		<b>return</b> PTR_ERR(t);</td></tr>
<tr><th id="1135">1135</th><td>	raw_spin_lock_irqsave_rcu_node(rnp, flags);</td></tr>
<tr><th id="1136">1136</th><td>	rnp-&gt;boost_kthread_task = t;</td></tr>
<tr><th id="1137">1137</th><td>	raw_spin_unlock_irqrestore_rcu_node(rnp, flags);</td></tr>
<tr><th id="1138">1138</th><td>	sp.sched_priority = kthread_prio;</td></tr>
<tr><th id="1139">1139</th><td>	sched_setscheduler_nocheck(t, SCHED_FIFO, &amp;sp);</td></tr>
<tr><th id="1140">1140</th><td>	wake_up_process(t); <i>/* get to TASK_INTERRUPTIBLE quickly. */</i></td></tr>
<tr><th id="1141">1141</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1142">1142</th><td>}</td></tr>
<tr><th id="1143">1143</th><td></td></tr>
<tr><th id="1144">1144</th><td><em>static</em> <em>void</em> rcu_kthread_do_work(<em>void</em>)</td></tr>
<tr><th id="1145">1145</th><td>{</td></tr>
<tr><th id="1146">1146</th><td>	rcu_do_batch(&amp;rcu_sched_state, this_cpu_ptr(&amp;rcu_sched_data));</td></tr>
<tr><th id="1147">1147</th><td>	rcu_do_batch(&amp;rcu_bh_state, this_cpu_ptr(&amp;rcu_bh_data));</td></tr>
<tr><th id="1148">1148</th><td>	rcu_preempt_do_callbacks();</td></tr>
<tr><th id="1149">1149</th><td>}</td></tr>
<tr><th id="1150">1150</th><td></td></tr>
<tr><th id="1151">1151</th><td><em>static</em> <em>void</em> rcu_cpu_kthread_setup(<em>unsigned</em> <em>int</em> cpu)</td></tr>
<tr><th id="1152">1152</th><td>{</td></tr>
<tr><th id="1153">1153</th><td>	<b>struct</b> sched_param sp;</td></tr>
<tr><th id="1154">1154</th><td></td></tr>
<tr><th id="1155">1155</th><td>	sp.sched_priority = kthread_prio;</td></tr>
<tr><th id="1156">1156</th><td>	sched_setscheduler_nocheck(current, SCHED_FIFO, &amp;sp);</td></tr>
<tr><th id="1157">1157</th><td>}</td></tr>
<tr><th id="1158">1158</th><td></td></tr>
<tr><th id="1159">1159</th><td><em>static</em> <em>void</em> rcu_cpu_kthread_park(<em>unsigned</em> <em>int</em> cpu)</td></tr>
<tr><th id="1160">1160</th><td>{</td></tr>
<tr><th id="1161">1161</th><td>	per_cpu(rcu_cpu_kthread_status, cpu) = RCU_KTHREAD_OFFCPU;</td></tr>
<tr><th id="1162">1162</th><td>}</td></tr>
<tr><th id="1163">1163</th><td></td></tr>
<tr><th id="1164">1164</th><td><em>static</em> <em>int</em> rcu_cpu_kthread_should_run(<em>unsigned</em> <em>int</em> cpu)</td></tr>
<tr><th id="1165">1165</th><td>{</td></tr>
<tr><th id="1166">1166</th><td>	<b>return</b> __this_cpu_read(rcu_cpu_has_work);</td></tr>
<tr><th id="1167">1167</th><td>}</td></tr>
<tr><th id="1168">1168</th><td></td></tr>
<tr><th id="1169">1169</th><td><i>/*</i></td></tr>
<tr><th id="1170">1170</th><td><i> * Per-CPU kernel thread that invokes RCU callbacks.  This replaces the</i></td></tr>
<tr><th id="1171">1171</th><td><i> * RCU softirq used in flavors and configurations of RCU that do not</i></td></tr>
<tr><th id="1172">1172</th><td><i> * support RCU priority boosting.</i></td></tr>
<tr><th id="1173">1173</th><td><i> */</i></td></tr>
<tr><th id="1174">1174</th><td><em>static</em> <em>void</em> rcu_cpu_kthread(<em>unsigned</em> <em>int</em> cpu)</td></tr>
<tr><th id="1175">1175</th><td>{</td></tr>
<tr><th id="1176">1176</th><td>	<em>unsigned</em> <em>int</em> *statusp = this_cpu_ptr(&amp;rcu_cpu_kthread_status);</td></tr>
<tr><th id="1177">1177</th><td>	<em>char</em> work, *workp = this_cpu_ptr(&amp;rcu_cpu_has_work);</td></tr>
<tr><th id="1178">1178</th><td>	<em>int</em> spincnt;</td></tr>
<tr><th id="1179">1179</th><td></td></tr>
<tr><th id="1180">1180</th><td>	<b>for</b> (spincnt = <var>0</var>; spincnt &lt; <var>10</var>; spincnt++) {</td></tr>
<tr><th id="1181">1181</th><td>		trace_rcu_utilization(TPS(<q>"Start CPU kthread@rcu_wait"</q>));</td></tr>
<tr><th id="1182">1182</th><td>		local_bh_disable();</td></tr>
<tr><th id="1183">1183</th><td>		*statusp = RCU_KTHREAD_RUNNING;</td></tr>
<tr><th id="1184">1184</th><td>		this_cpu_inc(rcu_cpu_kthread_loops);</td></tr>
<tr><th id="1185">1185</th><td>		local_irq_disable();</td></tr>
<tr><th id="1186">1186</th><td>		work = *workp;</td></tr>
<tr><th id="1187">1187</th><td>		*workp = <var>0</var>;</td></tr>
<tr><th id="1188">1188</th><td>		local_irq_enable();</td></tr>
<tr><th id="1189">1189</th><td>		<b>if</b> (work)</td></tr>
<tr><th id="1190">1190</th><td>			rcu_kthread_do_work();</td></tr>
<tr><th id="1191">1191</th><td>		local_bh_enable();</td></tr>
<tr><th id="1192">1192</th><td>		<b>if</b> (*workp == <var>0</var>) {</td></tr>
<tr><th id="1193">1193</th><td>			trace_rcu_utilization(TPS(<q>"End CPU kthread@rcu_wait"</q>));</td></tr>
<tr><th id="1194">1194</th><td>			*statusp = RCU_KTHREAD_WAITING;</td></tr>
<tr><th id="1195">1195</th><td>			<b>return</b>;</td></tr>
<tr><th id="1196">1196</th><td>		}</td></tr>
<tr><th id="1197">1197</th><td>	}</td></tr>
<tr><th id="1198">1198</th><td>	*statusp = RCU_KTHREAD_YIELDING;</td></tr>
<tr><th id="1199">1199</th><td>	trace_rcu_utilization(TPS(<q>"Start CPU kthread@rcu_yield"</q>));</td></tr>
<tr><th id="1200">1200</th><td>	schedule_timeout_interruptible(<var>2</var>);</td></tr>
<tr><th id="1201">1201</th><td>	trace_rcu_utilization(TPS(<q>"End CPU kthread@rcu_yield"</q>));</td></tr>
<tr><th id="1202">1202</th><td>	*statusp = RCU_KTHREAD_WAITING;</td></tr>
<tr><th id="1203">1203</th><td>}</td></tr>
<tr><th id="1204">1204</th><td></td></tr>
<tr><th id="1205">1205</th><td><i>/*</i></td></tr>
<tr><th id="1206">1206</th><td><i> * Set the per-rcu_node kthread's affinity to cover all CPUs that are</i></td></tr>
<tr><th id="1207">1207</th><td><i> * served by the rcu_node in question.  The CPU hotplug lock is still</i></td></tr>
<tr><th id="1208">1208</th><td><i> * held, so the value of rnp-&gt;qsmaskinit will be stable.</i></td></tr>
<tr><th id="1209">1209</th><td><i> *</i></td></tr>
<tr><th id="1210">1210</th><td><i> * We don't include outgoingcpu in the affinity set, use -1 if there is</i></td></tr>
<tr><th id="1211">1211</th><td><i> * no outgoing CPU.  If there are no CPUs left in the affinity set,</i></td></tr>
<tr><th id="1212">1212</th><td><i> * this function allows the kthread to execute on any CPU.</i></td></tr>
<tr><th id="1213">1213</th><td><i> */</i></td></tr>
<tr><th id="1214">1214</th><td><em>static</em> <em>void</em> rcu_boost_kthread_setaffinity(<b>struct</b> rcu_node *rnp, <em>int</em> outgoingcpu)</td></tr>
<tr><th id="1215">1215</th><td>{</td></tr>
<tr><th id="1216">1216</th><td>	<b>struct</b> task_struct *t = rnp-&gt;boost_kthread_task;</td></tr>
<tr><th id="1217">1217</th><td>	<em>unsigned</em> <em>long</em> mask = rcu_rnp_online_cpus(rnp);</td></tr>
<tr><th id="1218">1218</th><td>	cpumask_var_t cm;</td></tr>
<tr><th id="1219">1219</th><td>	<em>int</em> cpu;</td></tr>
<tr><th id="1220">1220</th><td></td></tr>
<tr><th id="1221">1221</th><td>	<b>if</b> (!t)</td></tr>
<tr><th id="1222">1222</th><td>		<b>return</b>;</td></tr>
<tr><th id="1223">1223</th><td>	<b>if</b> (!zalloc_cpumask_var(&amp;cm, GFP_KERNEL))</td></tr>
<tr><th id="1224">1224</th><td>		<b>return</b>;</td></tr>
<tr><th id="1225">1225</th><td>	for_each_leaf_node_possible_cpu(rnp, cpu)</td></tr>
<tr><th id="1226">1226</th><td>		<b>if</b> ((mask &amp; leaf_node_cpu_bit(rnp, cpu)) &amp;&amp;</td></tr>
<tr><th id="1227">1227</th><td>		    cpu != outgoingcpu)</td></tr>
<tr><th id="1228">1228</th><td>			cpumask_set_cpu(cpu, cm);</td></tr>
<tr><th id="1229">1229</th><td>	<b>if</b> (cpumask_weight(cm) == <var>0</var>)</td></tr>
<tr><th id="1230">1230</th><td>		cpumask_setall(cm);</td></tr>
<tr><th id="1231">1231</th><td>	set_cpus_allowed_ptr(t, cm);</td></tr>
<tr><th id="1232">1232</th><td>	free_cpumask_var(cm);</td></tr>
<tr><th id="1233">1233</th><td>}</td></tr>
<tr><th id="1234">1234</th><td></td></tr>
<tr><th id="1235">1235</th><td><em>static</em> <b>struct</b> smp_hotplug_thread rcu_cpu_thread_spec = {</td></tr>
<tr><th id="1236">1236</th><td>	.store			= &amp;rcu_cpu_kthread_task,</td></tr>
<tr><th id="1237">1237</th><td>	.thread_should_run	= rcu_cpu_kthread_should_run,</td></tr>
<tr><th id="1238">1238</th><td>	.thread_fn		= rcu_cpu_kthread,</td></tr>
<tr><th id="1239">1239</th><td>	.thread_comm		= <q>"rcuc/%u"</q>,</td></tr>
<tr><th id="1240">1240</th><td>	.setup			= rcu_cpu_kthread_setup,</td></tr>
<tr><th id="1241">1241</th><td>	.park			= rcu_cpu_kthread_park,</td></tr>
<tr><th id="1242">1242</th><td>};</td></tr>
<tr><th id="1243">1243</th><td></td></tr>
<tr><th id="1244">1244</th><td><i>/*</i></td></tr>
<tr><th id="1245">1245</th><td><i> * Spawn boost kthreads -- called as soon as the scheduler is running.</i></td></tr>
<tr><th id="1246">1246</th><td><i> */</i></td></tr>
<tr><th id="1247">1247</th><td><em>static</em> <em>void</em> __init rcu_spawn_boost_kthreads(<em>void</em>)</td></tr>
<tr><th id="1248">1248</th><td>{</td></tr>
<tr><th id="1249">1249</th><td>	<b>struct</b> rcu_node *rnp;</td></tr>
<tr><th id="1250">1250</th><td>	<em>int</em> cpu;</td></tr>
<tr><th id="1251">1251</th><td></td></tr>
<tr><th id="1252">1252</th><td>	for_each_possible_cpu(cpu)</td></tr>
<tr><th id="1253">1253</th><td>		per_cpu(rcu_cpu_has_work, cpu) = <var>0</var>;</td></tr>
<tr><th id="1254">1254</th><td>	BUG_ON(smpboot_register_percpu_thread(&amp;rcu_cpu_thread_spec));</td></tr>
<tr><th id="1255">1255</th><td>	rcu_for_each_leaf_node(rcu_state_p, rnp)</td></tr>
<tr><th id="1256">1256</th><td>		(<em>void</em>)rcu_spawn_one_boost_kthread(rcu_state_p, rnp);</td></tr>
<tr><th id="1257">1257</th><td>}</td></tr>
<tr><th id="1258">1258</th><td></td></tr>
<tr><th id="1259">1259</th><td><em>static</em> <em>void</em> rcu_prepare_kthreads(<em>int</em> cpu)</td></tr>
<tr><th id="1260">1260</th><td>{</td></tr>
<tr><th id="1261">1261</th><td>	<b>struct</b> rcu_data *rdp = per_cpu_ptr(rcu_state_p-&gt;rda, cpu);</td></tr>
<tr><th id="1262">1262</th><td>	<b>struct</b> rcu_node *rnp = rdp-&gt;mynode;</td></tr>
<tr><th id="1263">1263</th><td></td></tr>
<tr><th id="1264">1264</th><td>	<i>/* Fire up the incoming CPU's kthread and leaf rcu_node kthread. */</i></td></tr>
<tr><th id="1265">1265</th><td>	<b>if</b> (rcu_scheduler_fully_active)</td></tr>
<tr><th id="1266">1266</th><td>		(<em>void</em>)rcu_spawn_one_boost_kthread(rcu_state_p, rnp);</td></tr>
<tr><th id="1267">1267</th><td>}</td></tr>
<tr><th id="1268">1268</th><td></td></tr>
<tr><th id="1269">1269</th><td><u>#<span data-ppcond="918">else</span> /* #ifdef CONFIG_RCU_BOOST */</u></td></tr>
<tr><th id="1270">1270</th><td></td></tr>
<tr><th id="1271">1271</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_initiate_boost" title='rcu_initiate_boost' data-ref="rcu_initiate_boost">rcu_initiate_boost</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node">rcu_node</a> *<dfn class="local col7 decl" id="557rnp" title='rnp' data-type='struct rcu_node *' data-ref="557rnp">rnp</dfn>, <em>unsigned</em> <em>long</em> <dfn class="local col8 decl" id="558flags" title='flags' data-type='unsigned long' data-ref="558flags">flags</dfn>)</td></tr>
<tr><th id="1272">1272</th><td>	<a class="macro" href="../../include/linux/compiler_types.h.html#41" title="" data-ref="_M/__releases">__releases</a>(rnp-&gt;lock)</td></tr>
<tr><th id="1273">1273</th><td>{</td></tr>
<tr><th id="1274">1274</th><td>	<a class="macro" href="rcu.h.html#343" title="do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&amp;__dummy == &amp;__dummy2); 1; }); _raw_spin_unlock_irqrestore(&amp;((rnp)-&gt;lock), flags); } while (0)" data-ref="_M/raw_spin_unlock_irqrestore_rcu_node">raw_spin_unlock_irqrestore_rcu_node</a>(<a class="local col7 ref" href="#557rnp" title='rnp' data-ref="557rnp">rnp</a>, <a class="local col8 ref" href="#558flags" title='flags' data-ref="558flags">flags</a>);</td></tr>
<tr><th id="1275">1275</th><td>}</td></tr>
<tr><th id="1276">1276</th><td></td></tr>
<tr><th id="1277">1277</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="invoke_rcu_callbacks_kthread" title='invoke_rcu_callbacks_kthread' data-ref="invoke_rcu_callbacks_kthread">invoke_rcu_callbacks_kthread</dfn>(<em>void</em>)</td></tr>
<tr><th id="1278">1278</th><td>{</td></tr>
<tr><th id="1279">1279</th><td>	<a class="macro" href="../../include/asm-generic/bug.h.html#66" title="({ int __ret_warn_on = !!(1); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { asm volatile(&quot;1:\t&quot; &quot;.byte 0x0f, 0x0b&quot; &quot;\n&quot; &quot;.pushsection __bug_table,\&quot;aw\&quot;\n&quot; &quot;2:\t&quot; &quot;.long &quot; &quot;1b&quot; &quot; - 2b&quot; &quot;\t# bug_entry::bug_addr\n&quot; &quot;\t&quot; &quot;.long &quot; &quot;%c0&quot; &quot; - 2b&quot; &quot;\t# bug_entry::file\n&quot; &quot;\t.word %c1&quot; &quot;\t# bug_entry::line\n&quot; &quot;\t.word %c2&quot; &quot;\t# bug_entry::flags\n&quot; &quot;\t.org 2b+%c3\n&quot; &quot;.popsection&quot; : : &quot;i&quot; (&quot;/home/tempdban/kernel/stable/kernel/rcu/tree_plugin.h&quot;), &quot;i&quot; (1279), &quot;i&quot; ((1 &lt;&lt; 0)|((1 &lt;&lt; 1)|((9) &lt;&lt; 8))), &quot;i&quot; (sizeof(struct bug_entry))); } while (0); ({ asm(&quot;%c0:\n\t&quot; &quot;.pushsection .discard.reachable\n\t&quot; &quot;.long %c0b - .\n\t&quot; &quot;.popsection\n\t&quot; : : &quot;i&quot; (180)); }); } while (0); __builtin_expect(!!(__ret_warn_on), 0); })" data-ref="_M/WARN_ON_ONCE">WARN_ON_ONCE</a>(<var>1</var>);</td></tr>
<tr><th id="1280">1280</th><td>}</td></tr>
<tr><th id="1281">1281</th><td></td></tr>
<tr><th id="1282">1282</th><td><em>static</em> <a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool">bool</a> <dfn class="decl def fn" id="rcu_is_callbacks_kthread" title='rcu_is_callbacks_kthread' data-ref="rcu_is_callbacks_kthread">rcu_is_callbacks_kthread</dfn>(<em>void</em>)</td></tr>
<tr><th id="1283">1283</th><td>{</td></tr>
<tr><th id="1284">1284</th><td>	<b>return</b> <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false">false</a>;</td></tr>
<tr><th id="1285">1285</th><td>}</td></tr>
<tr><th id="1286">1286</th><td></td></tr>
<tr><th id="1287">1287</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_preempt_boost_start_gp" title='rcu_preempt_boost_start_gp' data-ref="rcu_preempt_boost_start_gp">rcu_preempt_boost_start_gp</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node">rcu_node</a> *<dfn class="local col9 decl" id="559rnp" title='rnp' data-type='struct rcu_node *' data-ref="559rnp">rnp</dfn>)</td></tr>
<tr><th id="1288">1288</th><td>{</td></tr>
<tr><th id="1289">1289</th><td>}</td></tr>
<tr><th id="1290">1290</th><td></td></tr>
<tr><th id="1291">1291</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_boost_kthread_setaffinity" title='rcu_boost_kthread_setaffinity' data-ref="rcu_boost_kthread_setaffinity">rcu_boost_kthread_setaffinity</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node">rcu_node</a> *<dfn class="local col0 decl" id="560rnp" title='rnp' data-type='struct rcu_node *' data-ref="560rnp">rnp</dfn>, <em>int</em> <dfn class="local col1 decl" id="561outgoingcpu" title='outgoingcpu' data-type='int' data-ref="561outgoingcpu">outgoingcpu</dfn>)</td></tr>
<tr><th id="1292">1292</th><td>{</td></tr>
<tr><th id="1293">1293</th><td>}</td></tr>
<tr><th id="1294">1294</th><td></td></tr>
<tr><th id="1295">1295</th><td><em>static</em> <em>void</em> <a class="macro" href="../../include/linux/init.h.html#50" title="__attribute__ ((__section__(&quot;.init.text&quot;)))" data-ref="_M/__init">__init</a> <dfn class="decl def fn" id="rcu_spawn_boost_kthreads" title='rcu_spawn_boost_kthreads' data-ref="rcu_spawn_boost_kthreads">rcu_spawn_boost_kthreads</dfn>(<em>void</em>)</td></tr>
<tr><th id="1296">1296</th><td>{</td></tr>
<tr><th id="1297">1297</th><td>}</td></tr>
<tr><th id="1298">1298</th><td></td></tr>
<tr><th id="1299">1299</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_prepare_kthreads" title='rcu_prepare_kthreads' data-ref="rcu_prepare_kthreads">rcu_prepare_kthreads</dfn>(<em>int</em> <dfn class="local col2 decl" id="562cpu" title='cpu' data-type='int' data-ref="562cpu">cpu</dfn>)</td></tr>
<tr><th id="1300">1300</th><td>{</td></tr>
<tr><th id="1301">1301</th><td>}</td></tr>
<tr><th id="1302">1302</th><td></td></tr>
<tr><th id="1303">1303</th><td><u>#<span data-ppcond="918">endif</span> /* #else #ifdef CONFIG_RCU_BOOST */</u></td></tr>
<tr><th id="1304">1304</th><td></td></tr>
<tr><th id="1305">1305</th><td><u>#<span data-ppcond="1305">if</span> !defined(<span class="macro" data-ref="_M/CONFIG_RCU_FAST_NO_HZ">CONFIG_RCU_FAST_NO_HZ</span>)</u></td></tr>
<tr><th id="1306">1306</th><td></td></tr>
<tr><th id="1307">1307</th><td><i>/*</i></td></tr>
<tr><th id="1308">1308</th><td><i> * Check to see if any future RCU-related work will need to be done</i></td></tr>
<tr><th id="1309">1309</th><td><i> * by the current CPU, even if none need be done immediately, returning</i></td></tr>
<tr><th id="1310">1310</th><td><i> * 1 if so.  This function is part of the RCU implementation; it is -not-</i></td></tr>
<tr><th id="1311">1311</th><td><i> * an exported member of the RCU API.</i></td></tr>
<tr><th id="1312">1312</th><td><i> *</i></td></tr>
<tr><th id="1313">1313</th><td><i> * Because we not have RCU_FAST_NO_HZ, just check whether this CPU needs</i></td></tr>
<tr><th id="1314">1314</th><td><i> * any flavor of RCU.</i></td></tr>
<tr><th id="1315">1315</th><td><i> */</i></td></tr>
<tr><th id="1316">1316</th><td><em>int</em> <dfn class="decl def fn" id="rcu_needs_cpu" title='rcu_needs_cpu' data-ref="rcu_needs_cpu">rcu_needs_cpu</dfn>(<a class="typedef" href="../../include/asm-generic/int-ll64.h.html#u64" title='u64' data-type='unsigned long long' data-ref="u64">u64</a> <dfn class="local col3 decl" id="563basemono" title='basemono' data-type='u64' data-ref="563basemono">basemono</dfn>, <a class="typedef" href="../../include/asm-generic/int-ll64.h.html#u64" title='u64' data-type='unsigned long long' data-ref="u64">u64</a> *<dfn class="local col4 decl" id="564nextevt" title='nextevt' data-type='u64 *' data-ref="564nextevt">nextevt</dfn>)</td></tr>
<tr><th id="1317">1317</th><td>{</td></tr>
<tr><th id="1318">1318</th><td>	*<a class="local col4 ref" href="#564nextevt" title='nextevt' data-ref="564nextevt">nextevt</a> = <a class="macro" href="../../include/linux/time64.h.html#42" title="((s64)~((u64)1 &lt;&lt; 63))" data-ref="_M/KTIME_MAX">KTIME_MAX</a>;</td></tr>
<tr><th id="1319">1319</th><td>	<b>return</b> <a class="tu ref fn" href="tree.c.html#rcu_cpu_has_callbacks" title='rcu_cpu_has_callbacks' data-use='c' data-ref="rcu_cpu_has_callbacks">rcu_cpu_has_callbacks</a>(<a class="macro" href="../../include/linux/stddef.h.html#8" title="((void *)0)" data-ref="_M/NULL">NULL</a>);</td></tr>
<tr><th id="1320">1320</th><td>}</td></tr>
<tr><th id="1321">1321</th><td></td></tr>
<tr><th id="1322">1322</th><td><i>/*</i></td></tr>
<tr><th id="1323">1323</th><td><i> * Because we do not have RCU_FAST_NO_HZ, don't bother cleaning up</i></td></tr>
<tr><th id="1324">1324</th><td><i> * after it.</i></td></tr>
<tr><th id="1325">1325</th><td><i> */</i></td></tr>
<tr><th id="1326">1326</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_cleanup_after_idle" title='rcu_cleanup_after_idle' data-ref="rcu_cleanup_after_idle">rcu_cleanup_after_idle</dfn>(<em>void</em>)</td></tr>
<tr><th id="1327">1327</th><td>{</td></tr>
<tr><th id="1328">1328</th><td>}</td></tr>
<tr><th id="1329">1329</th><td></td></tr>
<tr><th id="1330">1330</th><td><i>/*</i></td></tr>
<tr><th id="1331">1331</th><td><i> * Do the idle-entry grace-period work, which, because CONFIG_RCU_FAST_NO_HZ=n,</i></td></tr>
<tr><th id="1332">1332</th><td><i> * is nothing.</i></td></tr>
<tr><th id="1333">1333</th><td><i> */</i></td></tr>
<tr><th id="1334">1334</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_prepare_for_idle" title='rcu_prepare_for_idle' data-ref="rcu_prepare_for_idle">rcu_prepare_for_idle</dfn>(<em>void</em>)</td></tr>
<tr><th id="1335">1335</th><td>{</td></tr>
<tr><th id="1336">1336</th><td>}</td></tr>
<tr><th id="1337">1337</th><td></td></tr>
<tr><th id="1338">1338</th><td><i>/*</i></td></tr>
<tr><th id="1339">1339</th><td><i> * Don't bother keeping a running count of the number of RCU callbacks</i></td></tr>
<tr><th id="1340">1340</th><td><i> * posted because CONFIG_RCU_FAST_NO_HZ=n.</i></td></tr>
<tr><th id="1341">1341</th><td><i> */</i></td></tr>
<tr><th id="1342">1342</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_idle_count_callbacks_posted" title='rcu_idle_count_callbacks_posted' data-ref="rcu_idle_count_callbacks_posted">rcu_idle_count_callbacks_posted</dfn>(<em>void</em>)</td></tr>
<tr><th id="1343">1343</th><td>{</td></tr>
<tr><th id="1344">1344</th><td>}</td></tr>
<tr><th id="1345">1345</th><td></td></tr>
<tr><th id="1346">1346</th><td><u>#<span data-ppcond="1305">else</span> /* #if !defined(CONFIG_RCU_FAST_NO_HZ) */</u></td></tr>
<tr><th id="1347">1347</th><td></td></tr>
<tr><th id="1348">1348</th><td><i>/*</i></td></tr>
<tr><th id="1349">1349</th><td><i> * This code is invoked when a CPU goes idle, at which point we want</i></td></tr>
<tr><th id="1350">1350</th><td><i> * to have the CPU do everything required for RCU so that it can enter</i></td></tr>
<tr><th id="1351">1351</th><td><i> * the energy-efficient dyntick-idle mode.  This is handled by a</i></td></tr>
<tr><th id="1352">1352</th><td><i> * state machine implemented by rcu_prepare_for_idle() below.</i></td></tr>
<tr><th id="1353">1353</th><td><i> *</i></td></tr>
<tr><th id="1354">1354</th><td><i> * The following three proprocessor symbols control this state machine:</i></td></tr>
<tr><th id="1355">1355</th><td><i> *</i></td></tr>
<tr><th id="1356">1356</th><td><i> * RCU_IDLE_GP_DELAY gives the number of jiffies that a CPU is permitted</i></td></tr>
<tr><th id="1357">1357</th><td><i> *	to sleep in dyntick-idle mode with RCU callbacks pending.  This</i></td></tr>
<tr><th id="1358">1358</th><td><i> *	is sized to be roughly one RCU grace period.  Those energy-efficiency</i></td></tr>
<tr><th id="1359">1359</th><td><i> *	benchmarkers who might otherwise be tempted to set this to a large</i></td></tr>
<tr><th id="1360">1360</th><td><i> *	number, be warned: Setting RCU_IDLE_GP_DELAY too high can hang your</i></td></tr>
<tr><th id="1361">1361</th><td><i> *	system.  And if you are -that- concerned about energy efficiency,</i></td></tr>
<tr><th id="1362">1362</th><td><i> *	just power the system down and be done with it!</i></td></tr>
<tr><th id="1363">1363</th><td><i> * RCU_IDLE_LAZY_GP_DELAY gives the number of jiffies that a CPU is</i></td></tr>
<tr><th id="1364">1364</th><td><i> *	permitted to sleep in dyntick-idle mode with only lazy RCU</i></td></tr>
<tr><th id="1365">1365</th><td><i> *	callbacks pending.  Setting this too high can OOM your system.</i></td></tr>
<tr><th id="1366">1366</th><td><i> *</i></td></tr>
<tr><th id="1367">1367</th><td><i> * The values below work well in practice.  If future workloads require</i></td></tr>
<tr><th id="1368">1368</th><td><i> * adjustment, they can be converted into kernel config parameters, though</i></td></tr>
<tr><th id="1369">1369</th><td><i> * making the state machine smarter might be a better option.</i></td></tr>
<tr><th id="1370">1370</th><td><i> */</i></td></tr>
<tr><th id="1371">1371</th><td><u>#define RCU_IDLE_GP_DELAY 4		/* Roughly one grace period. */</u></td></tr>
<tr><th id="1372">1372</th><td><u>#define RCU_IDLE_LAZY_GP_DELAY (6 * HZ)	/* Roughly six seconds. */</u></td></tr>
<tr><th id="1373">1373</th><td></td></tr>
<tr><th id="1374">1374</th><td><em>static</em> <em>int</em> rcu_idle_gp_delay = RCU_IDLE_GP_DELAY;</td></tr>
<tr><th id="1375">1375</th><td>module_param(rcu_idle_gp_delay, <em>int</em>, <var>0644</var>);</td></tr>
<tr><th id="1376">1376</th><td><em>static</em> <em>int</em> rcu_idle_lazy_gp_delay = RCU_IDLE_LAZY_GP_DELAY;</td></tr>
<tr><th id="1377">1377</th><td>module_param(rcu_idle_lazy_gp_delay, <em>int</em>, <var>0644</var>);</td></tr>
<tr><th id="1378">1378</th><td></td></tr>
<tr><th id="1379">1379</th><td><i>/*</i></td></tr>
<tr><th id="1380">1380</th><td><i> * Try to advance callbacks for all flavors of RCU on the current CPU, but</i></td></tr>
<tr><th id="1381">1381</th><td><i> * only if it has been awhile since the last time we did so.  Afterwards,</i></td></tr>
<tr><th id="1382">1382</th><td><i> * if there are any callbacks ready for immediate invocation, return true.</i></td></tr>
<tr><th id="1383">1383</th><td><i> */</i></td></tr>
<tr><th id="1384">1384</th><td><em>static</em> bool __maybe_unused rcu_try_advance_all_cbs(<em>void</em>)</td></tr>
<tr><th id="1385">1385</th><td>{</td></tr>
<tr><th id="1386">1386</th><td>	bool cbs_ready = false;</td></tr>
<tr><th id="1387">1387</th><td>	<b>struct</b> rcu_data *rdp;</td></tr>
<tr><th id="1388">1388</th><td>	<b>struct</b> rcu_dynticks *rdtp = this_cpu_ptr(&amp;rcu_dynticks);</td></tr>
<tr><th id="1389">1389</th><td>	<b>struct</b> rcu_node *rnp;</td></tr>
<tr><th id="1390">1390</th><td>	<b>struct</b> rcu_state *rsp;</td></tr>
<tr><th id="1391">1391</th><td></td></tr>
<tr><th id="1392">1392</th><td>	<i>/* Exit early if we advanced recently. */</i></td></tr>
<tr><th id="1393">1393</th><td>	<b>if</b> (jiffies == rdtp-&gt;last_advance_all)</td></tr>
<tr><th id="1394">1394</th><td>		<b>return</b> false;</td></tr>
<tr><th id="1395">1395</th><td>	rdtp-&gt;last_advance_all = jiffies;</td></tr>
<tr><th id="1396">1396</th><td></td></tr>
<tr><th id="1397">1397</th><td>	for_each_rcu_flavor(rsp) {</td></tr>
<tr><th id="1398">1398</th><td>		rdp = this_cpu_ptr(rsp-&gt;rda);</td></tr>
<tr><th id="1399">1399</th><td>		rnp = rdp-&gt;mynode;</td></tr>
<tr><th id="1400">1400</th><td></td></tr>
<tr><th id="1401">1401</th><td>		<i>/*</i></td></tr>
<tr><th id="1402">1402</th><td><i>		 * Don't bother checking unless a grace period has</i></td></tr>
<tr><th id="1403">1403</th><td><i>		 * completed since we last checked and there are</i></td></tr>
<tr><th id="1404">1404</th><td><i>		 * callbacks not yet ready to invoke.</i></td></tr>
<tr><th id="1405">1405</th><td><i>		 */</i></td></tr>
<tr><th id="1406">1406</th><td>		<b>if</b> ((rdp-&gt;completed != rnp-&gt;completed ||</td></tr>
<tr><th id="1407">1407</th><td>		     unlikely(READ_ONCE(rdp-&gt;gpwrap))) &amp;&amp;</td></tr>
<tr><th id="1408">1408</th><td>		    rcu_segcblist_pend_cbs(&amp;rdp-&gt;cblist))</td></tr>
<tr><th id="1409">1409</th><td>			note_gp_changes(rsp, rdp);</td></tr>
<tr><th id="1410">1410</th><td></td></tr>
<tr><th id="1411">1411</th><td>		<b>if</b> (rcu_segcblist_ready_cbs(&amp;rdp-&gt;cblist))</td></tr>
<tr><th id="1412">1412</th><td>			cbs_ready = true;</td></tr>
<tr><th id="1413">1413</th><td>	}</td></tr>
<tr><th id="1414">1414</th><td>	<b>return</b> cbs_ready;</td></tr>
<tr><th id="1415">1415</th><td>}</td></tr>
<tr><th id="1416">1416</th><td></td></tr>
<tr><th id="1417">1417</th><td><i>/*</i></td></tr>
<tr><th id="1418">1418</th><td><i> * Allow the CPU to enter dyntick-idle mode unless it has callbacks ready</i></td></tr>
<tr><th id="1419">1419</th><td><i> * to invoke.  If the CPU has callbacks, try to advance them.  Tell the</i></td></tr>
<tr><th id="1420">1420</th><td><i> * caller to set the timeout based on whether or not there are non-lazy</i></td></tr>
<tr><th id="1421">1421</th><td><i> * callbacks.</i></td></tr>
<tr><th id="1422">1422</th><td><i> *</i></td></tr>
<tr><th id="1423">1423</th><td><i> * The caller must have disabled interrupts.</i></td></tr>
<tr><th id="1424">1424</th><td><i> */</i></td></tr>
<tr><th id="1425">1425</th><td><em>int</em> rcu_needs_cpu(u64 basemono, u64 *nextevt)</td></tr>
<tr><th id="1426">1426</th><td>{</td></tr>
<tr><th id="1427">1427</th><td>	<b>struct</b> rcu_dynticks *rdtp = this_cpu_ptr(&amp;rcu_dynticks);</td></tr>
<tr><th id="1428">1428</th><td>	<em>unsigned</em> <em>long</em> dj;</td></tr>
<tr><th id="1429">1429</th><td></td></tr>
<tr><th id="1430">1430</th><td>	RCU_LOCKDEP_WARN(!irqs_disabled(), <q>"rcu_needs_cpu() invoked with irqs enabled!!!"</q>);</td></tr>
<tr><th id="1431">1431</th><td></td></tr>
<tr><th id="1432">1432</th><td>	<i>/* Snapshot to detect later posting of non-lazy callback. */</i></td></tr>
<tr><th id="1433">1433</th><td>	rdtp-&gt;nonlazy_posted_snap = rdtp-&gt;nonlazy_posted;</td></tr>
<tr><th id="1434">1434</th><td></td></tr>
<tr><th id="1435">1435</th><td>	<i>/* If no callbacks, RCU doesn't need the CPU. */</i></td></tr>
<tr><th id="1436">1436</th><td>	<b>if</b> (!rcu_cpu_has_callbacks(&amp;rdtp-&gt;all_lazy)) {</td></tr>
<tr><th id="1437">1437</th><td>		*nextevt = KTIME_MAX;</td></tr>
<tr><th id="1438">1438</th><td>		<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1439">1439</th><td>	}</td></tr>
<tr><th id="1440">1440</th><td></td></tr>
<tr><th id="1441">1441</th><td>	<i>/* Attempt to advance callbacks. */</i></td></tr>
<tr><th id="1442">1442</th><td>	<b>if</b> (rcu_try_advance_all_cbs()) {</td></tr>
<tr><th id="1443">1443</th><td>		<i>/* Some ready to invoke, so initiate later invocation. */</i></td></tr>
<tr><th id="1444">1444</th><td>		invoke_rcu_core();</td></tr>
<tr><th id="1445">1445</th><td>		<b>return</b> <var>1</var>;</td></tr>
<tr><th id="1446">1446</th><td>	}</td></tr>
<tr><th id="1447">1447</th><td>	rdtp-&gt;last_accelerate = jiffies;</td></tr>
<tr><th id="1448">1448</th><td></td></tr>
<tr><th id="1449">1449</th><td>	<i>/* Request timer delay depending on laziness, and round. */</i></td></tr>
<tr><th id="1450">1450</th><td>	<b>if</b> (!rdtp-&gt;all_lazy) {</td></tr>
<tr><th id="1451">1451</th><td>		dj = round_up(rcu_idle_gp_delay + jiffies,</td></tr>
<tr><th id="1452">1452</th><td>			       rcu_idle_gp_delay) - jiffies;</td></tr>
<tr><th id="1453">1453</th><td>	} <b>else</b> {</td></tr>
<tr><th id="1454">1454</th><td>		dj = round_jiffies(rcu_idle_lazy_gp_delay + jiffies) - jiffies;</td></tr>
<tr><th id="1455">1455</th><td>	}</td></tr>
<tr><th id="1456">1456</th><td>	*nextevt = basemono + dj * TICK_NSEC;</td></tr>
<tr><th id="1457">1457</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1458">1458</th><td>}</td></tr>
<tr><th id="1459">1459</th><td></td></tr>
<tr><th id="1460">1460</th><td><i>/*</i></td></tr>
<tr><th id="1461">1461</th><td><i> * Prepare a CPU for idle from an RCU perspective.  The first major task</i></td></tr>
<tr><th id="1462">1462</th><td><i> * is to sense whether nohz mode has been enabled or disabled via sysfs.</i></td></tr>
<tr><th id="1463">1463</th><td><i> * The second major task is to check to see if a non-lazy callback has</i></td></tr>
<tr><th id="1464">1464</th><td><i> * arrived at a CPU that previously had only lazy callbacks.  The third</i></td></tr>
<tr><th id="1465">1465</th><td><i> * major task is to accelerate (that is, assign grace-period numbers to)</i></td></tr>
<tr><th id="1466">1466</th><td><i> * any recently arrived callbacks.</i></td></tr>
<tr><th id="1467">1467</th><td><i> *</i></td></tr>
<tr><th id="1468">1468</th><td><i> * The caller must have disabled interrupts.</i></td></tr>
<tr><th id="1469">1469</th><td><i> */</i></td></tr>
<tr><th id="1470">1470</th><td><em>static</em> <em>void</em> rcu_prepare_for_idle(<em>void</em>)</td></tr>
<tr><th id="1471">1471</th><td>{</td></tr>
<tr><th id="1472">1472</th><td>	bool needwake;</td></tr>
<tr><th id="1473">1473</th><td>	<b>struct</b> rcu_data *rdp;</td></tr>
<tr><th id="1474">1474</th><td>	<b>struct</b> rcu_dynticks *rdtp = this_cpu_ptr(&amp;rcu_dynticks);</td></tr>
<tr><th id="1475">1475</th><td>	<b>struct</b> rcu_node *rnp;</td></tr>
<tr><th id="1476">1476</th><td>	<b>struct</b> rcu_state *rsp;</td></tr>
<tr><th id="1477">1477</th><td>	<em>int</em> tne;</td></tr>
<tr><th id="1478">1478</th><td></td></tr>
<tr><th id="1479">1479</th><td>	RCU_LOCKDEP_WARN(!irqs_disabled(), <q>"rcu_prepare_for_idle() invoked with irqs enabled!!!"</q>);</td></tr>
<tr><th id="1480">1480</th><td>	<b>if</b> (rcu_is_nocb_cpu(smp_processor_id()))</td></tr>
<tr><th id="1481">1481</th><td>		<b>return</b>;</td></tr>
<tr><th id="1482">1482</th><td></td></tr>
<tr><th id="1483">1483</th><td>	<i>/* Handle nohz enablement switches conservatively. */</i></td></tr>
<tr><th id="1484">1484</th><td>	tne = READ_ONCE(tick_nohz_active);</td></tr>
<tr><th id="1485">1485</th><td>	<b>if</b> (tne != rdtp-&gt;tick_nohz_enabled_snap) {</td></tr>
<tr><th id="1486">1486</th><td>		<b>if</b> (rcu_cpu_has_callbacks(NULL))</td></tr>
<tr><th id="1487">1487</th><td>			invoke_rcu_core(); <i>/* force nohz to see update. */</i></td></tr>
<tr><th id="1488">1488</th><td>		rdtp-&gt;tick_nohz_enabled_snap = tne;</td></tr>
<tr><th id="1489">1489</th><td>		<b>return</b>;</td></tr>
<tr><th id="1490">1490</th><td>	}</td></tr>
<tr><th id="1491">1491</th><td>	<b>if</b> (!tne)</td></tr>
<tr><th id="1492">1492</th><td>		<b>return</b>;</td></tr>
<tr><th id="1493">1493</th><td></td></tr>
<tr><th id="1494">1494</th><td>	<i>/*</i></td></tr>
<tr><th id="1495">1495</th><td><i>	 * If a non-lazy callback arrived at a CPU having only lazy</i></td></tr>
<tr><th id="1496">1496</th><td><i>	 * callbacks, invoke RCU core for the side-effect of recalculating</i></td></tr>
<tr><th id="1497">1497</th><td><i>	 * idle duration on re-entry to idle.</i></td></tr>
<tr><th id="1498">1498</th><td><i>	 */</i></td></tr>
<tr><th id="1499">1499</th><td>	<b>if</b> (rdtp-&gt;all_lazy &amp;&amp;</td></tr>
<tr><th id="1500">1500</th><td>	    rdtp-&gt;nonlazy_posted != rdtp-&gt;nonlazy_posted_snap) {</td></tr>
<tr><th id="1501">1501</th><td>		rdtp-&gt;all_lazy = false;</td></tr>
<tr><th id="1502">1502</th><td>		rdtp-&gt;nonlazy_posted_snap = rdtp-&gt;nonlazy_posted;</td></tr>
<tr><th id="1503">1503</th><td>		invoke_rcu_core();</td></tr>
<tr><th id="1504">1504</th><td>		<b>return</b>;</td></tr>
<tr><th id="1505">1505</th><td>	}</td></tr>
<tr><th id="1506">1506</th><td></td></tr>
<tr><th id="1507">1507</th><td>	<i>/*</i></td></tr>
<tr><th id="1508">1508</th><td><i>	 * If we have not yet accelerated this jiffy, accelerate all</i></td></tr>
<tr><th id="1509">1509</th><td><i>	 * callbacks on this CPU.</i></td></tr>
<tr><th id="1510">1510</th><td><i>	 */</i></td></tr>
<tr><th id="1511">1511</th><td>	<b>if</b> (rdtp-&gt;last_accelerate == jiffies)</td></tr>
<tr><th id="1512">1512</th><td>		<b>return</b>;</td></tr>
<tr><th id="1513">1513</th><td>	rdtp-&gt;last_accelerate = jiffies;</td></tr>
<tr><th id="1514">1514</th><td>	for_each_rcu_flavor(rsp) {</td></tr>
<tr><th id="1515">1515</th><td>		rdp = this_cpu_ptr(rsp-&gt;rda);</td></tr>
<tr><th id="1516">1516</th><td>		<b>if</b> (!rcu_segcblist_pend_cbs(&amp;rdp-&gt;cblist))</td></tr>
<tr><th id="1517">1517</th><td>			<b>continue</b>;</td></tr>
<tr><th id="1518">1518</th><td>		rnp = rdp-&gt;mynode;</td></tr>
<tr><th id="1519">1519</th><td>		raw_spin_lock_rcu_node(rnp); <i>/* irqs already disabled. */</i></td></tr>
<tr><th id="1520">1520</th><td>		needwake = rcu_accelerate_cbs(rsp, rnp, rdp);</td></tr>
<tr><th id="1521">1521</th><td>		raw_spin_unlock_rcu_node(rnp); <i>/* irqs remain disabled. */</i></td></tr>
<tr><th id="1522">1522</th><td>		<b>if</b> (needwake)</td></tr>
<tr><th id="1523">1523</th><td>			rcu_gp_kthread_wake(rsp);</td></tr>
<tr><th id="1524">1524</th><td>	}</td></tr>
<tr><th id="1525">1525</th><td>}</td></tr>
<tr><th id="1526">1526</th><td></td></tr>
<tr><th id="1527">1527</th><td><i>/*</i></td></tr>
<tr><th id="1528">1528</th><td><i> * Clean up for exit from idle.  Attempt to advance callbacks based on</i></td></tr>
<tr><th id="1529">1529</th><td><i> * any grace periods that elapsed while the CPU was idle, and if any</i></td></tr>
<tr><th id="1530">1530</th><td><i> * callbacks are now ready to invoke, initiate invocation.</i></td></tr>
<tr><th id="1531">1531</th><td><i> */</i></td></tr>
<tr><th id="1532">1532</th><td><em>static</em> <em>void</em> rcu_cleanup_after_idle(<em>void</em>)</td></tr>
<tr><th id="1533">1533</th><td>{</td></tr>
<tr><th id="1534">1534</th><td>	RCU_LOCKDEP_WARN(!irqs_disabled(), <q>"rcu_cleanup_after_idle() invoked with irqs enabled!!!"</q>);</td></tr>
<tr><th id="1535">1535</th><td>	<b>if</b> (rcu_is_nocb_cpu(smp_processor_id()))</td></tr>
<tr><th id="1536">1536</th><td>		<b>return</b>;</td></tr>
<tr><th id="1537">1537</th><td>	<b>if</b> (rcu_try_advance_all_cbs())</td></tr>
<tr><th id="1538">1538</th><td>		invoke_rcu_core();</td></tr>
<tr><th id="1539">1539</th><td>}</td></tr>
<tr><th id="1540">1540</th><td></td></tr>
<tr><th id="1541">1541</th><td><i>/*</i></td></tr>
<tr><th id="1542">1542</th><td><i> * Keep a running count of the number of non-lazy callbacks posted</i></td></tr>
<tr><th id="1543">1543</th><td><i> * on this CPU.  This running counter (which is never decremented) allows</i></td></tr>
<tr><th id="1544">1544</th><td><i> * rcu_prepare_for_idle() to detect when something out of the idle loop</i></td></tr>
<tr><th id="1545">1545</th><td><i> * posts a callback, even if an equal number of callbacks are invoked.</i></td></tr>
<tr><th id="1546">1546</th><td><i> * Of course, callbacks should only be posted from within a trace event</i></td></tr>
<tr><th id="1547">1547</th><td><i> * designed to be called from idle or from within RCU_NONIDLE().</i></td></tr>
<tr><th id="1548">1548</th><td><i> */</i></td></tr>
<tr><th id="1549">1549</th><td><em>static</em> <em>void</em> rcu_idle_count_callbacks_posted(<em>void</em>)</td></tr>
<tr><th id="1550">1550</th><td>{</td></tr>
<tr><th id="1551">1551</th><td>	__this_cpu_add(rcu_dynticks.nonlazy_posted, <var>1</var>);</td></tr>
<tr><th id="1552">1552</th><td>}</td></tr>
<tr><th id="1553">1553</th><td></td></tr>
<tr><th id="1554">1554</th><td><i>/*</i></td></tr>
<tr><th id="1555">1555</th><td><i> * Data for flushing lazy RCU callbacks at OOM time.</i></td></tr>
<tr><th id="1556">1556</th><td><i> */</i></td></tr>
<tr><th id="1557">1557</th><td><em>static</em> atomic_t oom_callback_count;</td></tr>
<tr><th id="1558">1558</th><td><em>static</em> DECLARE_WAIT_QUEUE_HEAD(oom_callback_wq);</td></tr>
<tr><th id="1559">1559</th><td></td></tr>
<tr><th id="1560">1560</th><td><i>/*</i></td></tr>
<tr><th id="1561">1561</th><td><i> * RCU OOM callback -- decrement the outstanding count and deliver the</i></td></tr>
<tr><th id="1562">1562</th><td><i> * wake-up if we are the last one.</i></td></tr>
<tr><th id="1563">1563</th><td><i> */</i></td></tr>
<tr><th id="1564">1564</th><td><em>static</em> <em>void</em> rcu_oom_callback(<b>struct</b> rcu_head *rhp)</td></tr>
<tr><th id="1565">1565</th><td>{</td></tr>
<tr><th id="1566">1566</th><td>	<b>if</b> (atomic_dec_and_test(&amp;oom_callback_count))</td></tr>
<tr><th id="1567">1567</th><td>		wake_up(&amp;oom_callback_wq);</td></tr>
<tr><th id="1568">1568</th><td>}</td></tr>
<tr><th id="1569">1569</th><td></td></tr>
<tr><th id="1570">1570</th><td><i>/*</i></td></tr>
<tr><th id="1571">1571</th><td><i> * Post an rcu_oom_notify callback on the current CPU if it has at</i></td></tr>
<tr><th id="1572">1572</th><td><i> * least one lazy callback.  This will unnecessarily post callbacks</i></td></tr>
<tr><th id="1573">1573</th><td><i> * to CPUs that already have a non-lazy callback at the end of their</i></td></tr>
<tr><th id="1574">1574</th><td><i> * callback list, but this is an infrequent operation, so accept some</i></td></tr>
<tr><th id="1575">1575</th><td><i> * extra overhead to keep things simple.</i></td></tr>
<tr><th id="1576">1576</th><td><i> */</i></td></tr>
<tr><th id="1577">1577</th><td><em>static</em> <em>void</em> rcu_oom_notify_cpu(<em>void</em> *unused)</td></tr>
<tr><th id="1578">1578</th><td>{</td></tr>
<tr><th id="1579">1579</th><td>	<b>struct</b> rcu_state *rsp;</td></tr>
<tr><th id="1580">1580</th><td>	<b>struct</b> rcu_data *rdp;</td></tr>
<tr><th id="1581">1581</th><td></td></tr>
<tr><th id="1582">1582</th><td>	for_each_rcu_flavor(rsp) {</td></tr>
<tr><th id="1583">1583</th><td>		rdp = raw_cpu_ptr(rsp-&gt;rda);</td></tr>
<tr><th id="1584">1584</th><td>		<b>if</b> (rcu_segcblist_n_lazy_cbs(&amp;rdp-&gt;cblist)) {</td></tr>
<tr><th id="1585">1585</th><td>			atomic_inc(&amp;oom_callback_count);</td></tr>
<tr><th id="1586">1586</th><td>			rsp-&gt;call(&amp;rdp-&gt;oom_head, rcu_oom_callback);</td></tr>
<tr><th id="1587">1587</th><td>		}</td></tr>
<tr><th id="1588">1588</th><td>	}</td></tr>
<tr><th id="1589">1589</th><td>}</td></tr>
<tr><th id="1590">1590</th><td></td></tr>
<tr><th id="1591">1591</th><td><i>/*</i></td></tr>
<tr><th id="1592">1592</th><td><i> * If low on memory, ensure that each CPU has a non-lazy callback.</i></td></tr>
<tr><th id="1593">1593</th><td><i> * This will wake up CPUs that have only lazy callbacks, in turn</i></td></tr>
<tr><th id="1594">1594</th><td><i> * ensuring that they free up the corresponding memory in a timely manner.</i></td></tr>
<tr><th id="1595">1595</th><td><i> * Because an uncertain amount of memory will be freed in some uncertain</i></td></tr>
<tr><th id="1596">1596</th><td><i> * timeframe, we do not claim to have freed anything.</i></td></tr>
<tr><th id="1597">1597</th><td><i> */</i></td></tr>
<tr><th id="1598">1598</th><td><em>static</em> <em>int</em> rcu_oom_notify(<b>struct</b> notifier_block *self,</td></tr>
<tr><th id="1599">1599</th><td>			  <em>unsigned</em> <em>long</em> notused, <em>void</em> *nfreed)</td></tr>
<tr><th id="1600">1600</th><td>{</td></tr>
<tr><th id="1601">1601</th><td>	<em>int</em> cpu;</td></tr>
<tr><th id="1602">1602</th><td></td></tr>
<tr><th id="1603">1603</th><td>	<i>/* Wait for callbacks from earlier instance to complete. */</i></td></tr>
<tr><th id="1604">1604</th><td>	wait_event(oom_callback_wq, atomic_read(&amp;oom_callback_count) == <var>0</var>);</td></tr>
<tr><th id="1605">1605</th><td>	smp_mb(); <i>/* Ensure callback reuse happens after callback invocation. */</i></td></tr>
<tr><th id="1606">1606</th><td></td></tr>
<tr><th id="1607">1607</th><td>	<i>/*</i></td></tr>
<tr><th id="1608">1608</th><td><i>	 * Prevent premature wakeup: ensure that all increments happen</i></td></tr>
<tr><th id="1609">1609</th><td><i>	 * before there is a chance of the counter reaching zero.</i></td></tr>
<tr><th id="1610">1610</th><td><i>	 */</i></td></tr>
<tr><th id="1611">1611</th><td>	atomic_set(&amp;oom_callback_count, <var>1</var>);</td></tr>
<tr><th id="1612">1612</th><td></td></tr>
<tr><th id="1613">1613</th><td>	for_each_online_cpu(cpu) {</td></tr>
<tr><th id="1614">1614</th><td>		smp_call_function_single(cpu, rcu_oom_notify_cpu, NULL, <var>1</var>);</td></tr>
<tr><th id="1615">1615</th><td>		cond_resched_rcu_qs();</td></tr>
<tr><th id="1616">1616</th><td>	}</td></tr>
<tr><th id="1617">1617</th><td></td></tr>
<tr><th id="1618">1618</th><td>	<i>/* Unconditionally decrement: no need to wake ourselves up. */</i></td></tr>
<tr><th id="1619">1619</th><td>	atomic_dec(&amp;oom_callback_count);</td></tr>
<tr><th id="1620">1620</th><td></td></tr>
<tr><th id="1621">1621</th><td>	<b>return</b> NOTIFY_OK;</td></tr>
<tr><th id="1622">1622</th><td>}</td></tr>
<tr><th id="1623">1623</th><td></td></tr>
<tr><th id="1624">1624</th><td><em>static</em> <b>struct</b> notifier_block rcu_oom_nb = {</td></tr>
<tr><th id="1625">1625</th><td>	.notifier_call = rcu_oom_notify</td></tr>
<tr><th id="1626">1626</th><td>};</td></tr>
<tr><th id="1627">1627</th><td></td></tr>
<tr><th id="1628">1628</th><td><em>static</em> <em>int</em> __init rcu_register_oom_notifier(<em>void</em>)</td></tr>
<tr><th id="1629">1629</th><td>{</td></tr>
<tr><th id="1630">1630</th><td>	register_oom_notifier(&amp;rcu_oom_nb);</td></tr>
<tr><th id="1631">1631</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1632">1632</th><td>}</td></tr>
<tr><th id="1633">1633</th><td>early_initcall(rcu_register_oom_notifier);</td></tr>
<tr><th id="1634">1634</th><td></td></tr>
<tr><th id="1635">1635</th><td><u>#<span data-ppcond="1305">endif</span> /* #else #if !defined(CONFIG_RCU_FAST_NO_HZ) */</u></td></tr>
<tr><th id="1636">1636</th><td></td></tr>
<tr><th id="1637">1637</th><td><u>#<span data-ppcond="1637">ifdef</span> <span class="macro" data-ref="_M/CONFIG_RCU_FAST_NO_HZ">CONFIG_RCU_FAST_NO_HZ</span></u></td></tr>
<tr><th id="1638">1638</th><td></td></tr>
<tr><th id="1639">1639</th><td><em>static</em> <em>void</em> print_cpu_stall_fast_no_hz(<em>char</em> *cp, <em>int</em> cpu)</td></tr>
<tr><th id="1640">1640</th><td>{</td></tr>
<tr><th id="1641">1641</th><td>	<b>struct</b> rcu_dynticks *rdtp = &amp;per_cpu(rcu_dynticks, cpu);</td></tr>
<tr><th id="1642">1642</th><td>	<em>unsigned</em> <em>long</em> nlpd = rdtp-&gt;nonlazy_posted - rdtp-&gt;nonlazy_posted_snap;</td></tr>
<tr><th id="1643">1643</th><td></td></tr>
<tr><th id="1644">1644</th><td>	sprintf(cp, <q>"last_accelerate: %04lx/%04lx, nonlazy_posted: %ld, %c%c"</q>,</td></tr>
<tr><th id="1645">1645</th><td>		rdtp-&gt;last_accelerate &amp; <var>0xffff</var>, jiffies &amp; <var>0xffff</var>,</td></tr>
<tr><th id="1646">1646</th><td>		ulong2long(nlpd),</td></tr>
<tr><th id="1647">1647</th><td>		rdtp-&gt;all_lazy ? <kbd>'L'</kbd> : <kbd>'.'</kbd>,</td></tr>
<tr><th id="1648">1648</th><td>		rdtp-&gt;tick_nohz_enabled_snap ? <kbd>'.'</kbd> : <kbd>'D'</kbd>);</td></tr>
<tr><th id="1649">1649</th><td>}</td></tr>
<tr><th id="1650">1650</th><td></td></tr>
<tr><th id="1651">1651</th><td><u>#<span data-ppcond="1637">else</span> /* #ifdef CONFIG_RCU_FAST_NO_HZ */</u></td></tr>
<tr><th id="1652">1652</th><td></td></tr>
<tr><th id="1653">1653</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="print_cpu_stall_fast_no_hz" title='print_cpu_stall_fast_no_hz' data-ref="print_cpu_stall_fast_no_hz">print_cpu_stall_fast_no_hz</dfn>(<em>char</em> *<dfn class="local col5 decl" id="565cp" title='cp' data-type='char *' data-ref="565cp">cp</dfn>, <em>int</em> <dfn class="local col6 decl" id="566cpu" title='cpu' data-type='int' data-ref="566cpu">cpu</dfn>)</td></tr>
<tr><th id="1654">1654</th><td>{</td></tr>
<tr><th id="1655">1655</th><td>	*<a class="local col5 ref" href="#565cp" title='cp' data-ref="565cp">cp</a> = <kbd>'\0'</kbd>;</td></tr>
<tr><th id="1656">1656</th><td>}</td></tr>
<tr><th id="1657">1657</th><td></td></tr>
<tr><th id="1658">1658</th><td><u>#<span data-ppcond="1637">endif</span> /* #else #ifdef CONFIG_RCU_FAST_NO_HZ */</u></td></tr>
<tr><th id="1659">1659</th><td></td></tr>
<tr><th id="1660">1660</th><td><i>/* Initiate the stall-info list. */</i></td></tr>
<tr><th id="1661">1661</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="print_cpu_stall_info_begin" title='print_cpu_stall_info_begin' data-ref="print_cpu_stall_info_begin">print_cpu_stall_info_begin</dfn>(<em>void</em>)</td></tr>
<tr><th id="1662">1662</th><td>{</td></tr>
<tr><th id="1663">1663</th><td>	<a class="macro" href="../../include/linux/printk.h.html#315" title="printk(&quot;\001&quot; &quot;c&quot; &quot;\n&quot;)" data-ref="_M/pr_cont">pr_cont</a>(<q>"\n"</q>);</td></tr>
<tr><th id="1664">1664</th><td>}</td></tr>
<tr><th id="1665">1665</th><td></td></tr>
<tr><th id="1666">1666</th><td><i>/*</i></td></tr>
<tr><th id="1667">1667</th><td><i> * Print out diagnostic information for the specified stalled CPU.</i></td></tr>
<tr><th id="1668">1668</th><td><i> *</i></td></tr>
<tr><th id="1669">1669</th><td><i> * If the specified CPU is aware of the current RCU grace period</i></td></tr>
<tr><th id="1670">1670</th><td><i> * (flavor specified by rsp), then print the number of scheduling</i></td></tr>
<tr><th id="1671">1671</th><td><i> * clock interrupts the CPU has taken during the time that it has</i></td></tr>
<tr><th id="1672">1672</th><td><i> * been aware.  Otherwise, print the number of RCU grace periods</i></td></tr>
<tr><th id="1673">1673</th><td><i> * that this CPU is ignorant of, for example, "1" if the CPU was</i></td></tr>
<tr><th id="1674">1674</th><td><i> * aware of the previous grace period.</i></td></tr>
<tr><th id="1675">1675</th><td><i> *</i></td></tr>
<tr><th id="1676">1676</th><td><i> * Also print out idle and (if CONFIG_RCU_FAST_NO_HZ) idle-entry info.</i></td></tr>
<tr><th id="1677">1677</th><td><i> */</i></td></tr>
<tr><th id="1678">1678</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="print_cpu_stall_info" title='print_cpu_stall_info' data-ref="print_cpu_stall_info">print_cpu_stall_info</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_state" title='rcu_state' data-ref="rcu_state">rcu_state</a> *<dfn class="local col7 decl" id="567rsp" title='rsp' data-type='struct rcu_state *' data-ref="567rsp">rsp</dfn>, <em>int</em> <dfn class="local col8 decl" id="568cpu" title='cpu' data-type='int' data-ref="568cpu">cpu</dfn>)</td></tr>
<tr><th id="1679">1679</th><td>{</td></tr>
<tr><th id="1680">1680</th><td>	<em>char</em> <dfn class="local col9 decl" id="569fast_no_hz" title='fast_no_hz' data-type='char [72]' data-ref="569fast_no_hz">fast_no_hz</dfn>[<var>72</var>];</td></tr>
<tr><th id="1681">1681</th><td>	<b>struct</b> <a class="type" href="tree.h.html#rcu_data" title='rcu_data' data-ref="rcu_data">rcu_data</a> *<dfn class="local col0 decl" id="570rdp" title='rdp' data-type='struct rcu_data *' data-ref="570rdp">rdp</dfn> = <a class="macro" href="../../include/linux/percpu-defs.h.html#220" title="({ do { const void *__vpp_verify = (typeof((rsp-&gt;rda) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __asm__ (&quot;&quot; : &quot;=r&quot;(__ptr) : &quot;0&quot;((typeof(*((rsp-&gt;rda))) *)((rsp-&gt;rda)))); (typeof((typeof(*((rsp-&gt;rda))) *)((rsp-&gt;rda)))) (__ptr + (((__per_cpu_offset[(cpu)])))); }); })" data-ref="_M/per_cpu_ptr">per_cpu_ptr</a>(<a class="local col7 ref" href="#567rsp" title='rsp' data-ref="567rsp">rsp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_state::rda" title='rcu_state::rda' data-ref="rcu_state::rda">rda</a>, <a class="local col8 ref" href="#568cpu" title='cpu' data-ref="568cpu">cpu</a>);</td></tr>
<tr><th id="1682">1682</th><td>	<b>struct</b> <a class="type" href="tree.h.html#rcu_dynticks" title='rcu_dynticks' data-ref="rcu_dynticks">rcu_dynticks</a> *<dfn class="local col1 decl" id="571rdtp" title='rdtp' data-type='struct rcu_dynticks *' data-ref="571rdtp">rdtp</dfn> = <a class="local col0 ref" href="#570rdp" title='rdp' data-ref="570rdp">rdp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_data::dynticks" title='rcu_data::dynticks' data-ref="rcu_data::dynticks">dynticks</a>;</td></tr>
<tr><th id="1683">1683</th><td>	<em>char</em> *<dfn class="local col2 decl" id="572ticks_title" title='ticks_title' data-type='char *' data-ref="572ticks_title">ticks_title</dfn>;</td></tr>
<tr><th id="1684">1684</th><td>	<em>unsigned</em> <em>long</em> <dfn class="local col3 decl" id="573ticks_value" title='ticks_value' data-type='unsigned long' data-ref="573ticks_value">ticks_value</dfn>;</td></tr>
<tr><th id="1685">1685</th><td></td></tr>
<tr><th id="1686">1686</th><td>	<i>/*</i></td></tr>
<tr><th id="1687">1687</th><td><i>	 * We could be printing a lot while holding a spinlock.  Avoid</i></td></tr>
<tr><th id="1688">1688</th><td><i>	 * triggering hard lockup.</i></td></tr>
<tr><th id="1689">1689</th><td><i>	 */</i></td></tr>
<tr><th id="1690">1690</th><td>	<a class="ref fn" href="../../include/linux/nmi.h.html#touch_nmi_watchdog" title='touch_nmi_watchdog' data-ref="touch_nmi_watchdog">touch_nmi_watchdog</a>();</td></tr>
<tr><th id="1691">1691</th><td></td></tr>
<tr><th id="1692">1692</th><td>	<b>if</b> (<a class="local col7 ref" href="#567rsp" title='rsp' data-ref="567rsp">rsp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_state::gpnum" title='rcu_state::gpnum' data-ref="rcu_state::gpnum">gpnum</a> == <a class="local col0 ref" href="#570rdp" title='rdp' data-ref="570rdp">rdp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_data::gpnum" title='rcu_data::gpnum' data-ref="rcu_data::gpnum">gpnum</a>) {</td></tr>
<tr><th id="1693">1693</th><td>		<a class="local col2 ref" href="#572ticks_title" title='ticks_title' data-ref="572ticks_title">ticks_title</a> = <q>"ticks this GP"</q>;</td></tr>
<tr><th id="1694">1694</th><td>		<a class="local col3 ref" href="#573ticks_value" title='ticks_value' data-ref="573ticks_value">ticks_value</a> = <a class="local col0 ref" href="#570rdp" title='rdp' data-ref="570rdp">rdp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_data::ticks_this_gp" title='rcu_data::ticks_this_gp' data-ref="rcu_data::ticks_this_gp">ticks_this_gp</a>;</td></tr>
<tr><th id="1695">1695</th><td>	} <b>else</b> {</td></tr>
<tr><th id="1696">1696</th><td>		<a class="local col2 ref" href="#572ticks_title" title='ticks_title' data-ref="572ticks_title">ticks_title</a> = <q>"GPs behind"</q>;</td></tr>
<tr><th id="1697">1697</th><td>		<a class="local col3 ref" href="#573ticks_value" title='ticks_value' data-ref="573ticks_value">ticks_value</a> = <a class="local col7 ref" href="#567rsp" title='rsp' data-ref="567rsp">rsp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_state::gpnum" title='rcu_state::gpnum' data-ref="rcu_state::gpnum">gpnum</a> - <a class="local col0 ref" href="#570rdp" title='rdp' data-ref="570rdp">rdp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_data::gpnum" title='rcu_data::gpnum' data-ref="rcu_data::gpnum">gpnum</a>;</td></tr>
<tr><th id="1698">1698</th><td>	}</td></tr>
<tr><th id="1699">1699</th><td>	<a class="ref fn" href="#print_cpu_stall_fast_no_hz" title='print_cpu_stall_fast_no_hz' data-ref="print_cpu_stall_fast_no_hz">print_cpu_stall_fast_no_hz</a>(<a class="local col9 ref" href="#569fast_no_hz" title='fast_no_hz' data-ref="569fast_no_hz">fast_no_hz</a>, <a class="local col8 ref" href="#568cpu" title='cpu' data-ref="568cpu">cpu</a>);</td></tr>
<tr><th id="1700">1700</th><td>	<a class="macro" href="../../include/linux/printk.h.html#301" title="printk(&quot;\001&quot; &quot;3&quot; &quot;\t%d-%c%c%c: (%lu %s) idle=%03x/%llx/%d softirq=%u/%u fqs=%ld %s\n&quot;, cpu, &quot;O.&quot;[!!cpumask_test_cpu((cpu), ((const struct cpumask *)&amp;__cpu_online_mask))], &quot;o.&quot;[!!(rdp-&gt;grpmask &amp; rdp-&gt;mynode-&gt;qsmaskinit)], &quot;N.&quot;[!!(rdp-&gt;grpmask &amp; rdp-&gt;mynode-&gt;qsmaskinitnext)], ticks_value, ticks_title, rcu_dynticks_snap(rdtp) &amp; 0xfff, rdtp-&gt;dynticks_nesting, rdtp-&gt;dynticks_nmi_nesting, rdp-&gt;softirq_snap, kstat_softirqs_cpu(RCU_SOFTIRQ, cpu), ({ union { typeof(rsp-&gt;n_force_qs) __val; char __c[1]; } __u; if (1) __read_once_size(&amp;(rsp-&gt;n_force_qs), __u.__c, sizeof(rsp-&gt;n_force_qs)); else __read_once_size_nocheck(&amp;(rsp-&gt;n_force_qs), __u.__c, sizeof(rsp-&gt;n_force_qs)); do { } while (0); __u.__val; }) - rsp-&gt;n_force_qs_gpstart, fast_no_hz)" data-ref="_M/pr_err">pr_err</a>(<q>"\t%d-%c%c%c: (%lu %s) idle=%03x/%llx/%d softirq=%u/%u fqs=%ld %s\n"</q>,</td></tr>
<tr><th id="1701">1701</th><td>	       <a class="local col8 ref" href="#568cpu" title='cpu' data-ref="568cpu">cpu</a>,</td></tr>
<tr><th id="1702">1702</th><td>	       <q>"O."</q>[!!cpu_online(<a class="local col8 ref" href="#568cpu" title='cpu' data-ref="568cpu">cpu</a>)],</td></tr>
<tr><th id="1703">1703</th><td>	       <q>"o."</q>[!!(<a class="local col0 ref" href="#570rdp" title='rdp' data-ref="570rdp">rdp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_data::grpmask" title='rcu_data::grpmask' data-ref="rcu_data::grpmask">grpmask</a> &amp; <a class="local col0 ref" href="#570rdp" title='rdp' data-ref="570rdp">rdp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_data::mynode" title='rcu_data::mynode' data-ref="rcu_data::mynode">mynode</a>-&gt;<a class="ref field" href="tree.h.html#rcu_node::qsmaskinit" title='rcu_node::qsmaskinit' data-ref="rcu_node::qsmaskinit">qsmaskinit</a>)],</td></tr>
<tr><th id="1704">1704</th><td>	       <q>"N."</q>[!!(<a class="local col0 ref" href="#570rdp" title='rdp' data-ref="570rdp">rdp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_data::grpmask" title='rcu_data::grpmask' data-ref="rcu_data::grpmask">grpmask</a> &amp; <a class="local col0 ref" href="#570rdp" title='rdp' data-ref="570rdp">rdp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_data::mynode" title='rcu_data::mynode' data-ref="rcu_data::mynode">mynode</a>-&gt;<a class="ref field" href="tree.h.html#rcu_node::qsmaskinitnext" title='rcu_node::qsmaskinitnext' data-ref="rcu_node::qsmaskinitnext">qsmaskinitnext</a>)],</td></tr>
<tr><th id="1705">1705</th><td>	       <a class="local col3 ref" href="#573ticks_value" title='ticks_value' data-ref="573ticks_value">ticks_value</a>, <a class="local col2 ref" href="#572ticks_title" title='ticks_title' data-ref="572ticks_title">ticks_title</a>,</td></tr>
<tr><th id="1706">1706</th><td>	       <a class="ref fn" href="tree.c.html#rcu_dynticks_snap" title='rcu_dynticks_snap' data-ref="rcu_dynticks_snap">rcu_dynticks_snap</a>(<a class="local col1 ref" href="#571rdtp" title='rdtp' data-ref="571rdtp">rdtp</a>) &amp; <var>0xfff</var>,</td></tr>
<tr><th id="1707">1707</th><td>	       <a class="local col1 ref" href="#571rdtp" title='rdtp' data-ref="571rdtp">rdtp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_dynticks::dynticks_nesting" title='rcu_dynticks::dynticks_nesting' data-ref="rcu_dynticks::dynticks_nesting">dynticks_nesting</a>, <a class="local col1 ref" href="#571rdtp" title='rdtp' data-ref="571rdtp">rdtp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_dynticks::dynticks_nmi_nesting" title='rcu_dynticks::dynticks_nmi_nesting' data-ref="rcu_dynticks::dynticks_nmi_nesting">dynticks_nmi_nesting</a>,</td></tr>
<tr><th id="1708">1708</th><td>	       <a class="local col0 ref" href="#570rdp" title='rdp' data-ref="570rdp">rdp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_data::softirq_snap" title='rcu_data::softirq_snap' data-ref="rcu_data::softirq_snap">softirq_snap</a>, <a class="ref fn" href="../../include/linux/kernel_stat.h.html#kstat_softirqs_cpu" title='kstat_softirqs_cpu' data-ref="kstat_softirqs_cpu">kstat_softirqs_cpu</a>(<a class="enum" href="../../include/linux/interrupt.h.html#RCU_SOFTIRQ" title='RCU_SOFTIRQ' data-ref="RCU_SOFTIRQ">RCU_SOFTIRQ</a>, <a class="local col8 ref" href="#568cpu" title='cpu' data-ref="568cpu">cpu</a>),</td></tr>
<tr><th id="1709">1709</th><td>	       READ_ONCE(<a class="local col7 ref" href="#567rsp" title='rsp' data-ref="567rsp">rsp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_state::n_force_qs" title='rcu_state::n_force_qs' data-ref="rcu_state::n_force_qs">n_force_qs</a>) - <a class="local col7 ref" href="#567rsp" title='rsp' data-ref="567rsp">rsp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_state::n_force_qs_gpstart" title='rcu_state::n_force_qs_gpstart' data-ref="rcu_state::n_force_qs_gpstart">n_force_qs_gpstart</a>,</td></tr>
<tr><th id="1710">1710</th><td>	       <a class="local col9 ref" href="#569fast_no_hz" title='fast_no_hz' data-ref="569fast_no_hz">fast_no_hz</a>);</td></tr>
<tr><th id="1711">1711</th><td>}</td></tr>
<tr><th id="1712">1712</th><td></td></tr>
<tr><th id="1713">1713</th><td><i>/* Terminate the stall-info list. */</i></td></tr>
<tr><th id="1714">1714</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="print_cpu_stall_info_end" title='print_cpu_stall_info_end' data-ref="print_cpu_stall_info_end">print_cpu_stall_info_end</dfn>(<em>void</em>)</td></tr>
<tr><th id="1715">1715</th><td>{</td></tr>
<tr><th id="1716">1716</th><td>	<a class="macro" href="../../include/linux/printk.h.html#301" title="printk(&quot;\001&quot; &quot;3&quot; &quot;\t&quot;)" data-ref="_M/pr_err">pr_err</a>(<q>"\t"</q>);</td></tr>
<tr><th id="1717">1717</th><td>}</td></tr>
<tr><th id="1718">1718</th><td></td></tr>
<tr><th id="1719">1719</th><td><i>/* Zero -&gt;ticks_this_gp for all flavors of RCU. */</i></td></tr>
<tr><th id="1720">1720</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="zero_cpu_stall_ticks" title='zero_cpu_stall_ticks' data-ref="zero_cpu_stall_ticks">zero_cpu_stall_ticks</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_data" title='rcu_data' data-ref="rcu_data">rcu_data</a> *<dfn class="local col4 decl" id="574rdp" title='rdp' data-type='struct rcu_data *' data-ref="574rdp">rdp</dfn>)</td></tr>
<tr><th id="1721">1721</th><td>{</td></tr>
<tr><th id="1722">1722</th><td>	<a class="local col4 ref" href="#574rdp" title='rdp' data-ref="574rdp">rdp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_data::ticks_this_gp" title='rcu_data::ticks_this_gp' data-ref="rcu_data::ticks_this_gp">ticks_this_gp</a> = <var>0</var>;</td></tr>
<tr><th id="1723">1723</th><td>	<a class="local col4 ref" href="#574rdp" title='rdp' data-ref="574rdp">rdp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_data::softirq_snap" title='rcu_data::softirq_snap' data-ref="rcu_data::softirq_snap">softirq_snap</a> = <a class="ref fn" href="../../include/linux/kernel_stat.h.html#kstat_softirqs_cpu" title='kstat_softirqs_cpu' data-ref="kstat_softirqs_cpu">kstat_softirqs_cpu</a>(<a class="enum" href="../../include/linux/interrupt.h.html#RCU_SOFTIRQ" title='RCU_SOFTIRQ' data-ref="RCU_SOFTIRQ">RCU_SOFTIRQ</a>, <a class="macro" href="../../include/linux/smp.h.html#199" title="(({ typeof(cpu_number) pscr_ret__; do { const void *__vpp_verify = (typeof((&amp;(cpu_number)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(cpu_number)) { case 1: pscr_ret__ = ({ typeof(cpu_number) pfo_ret__; switch (sizeof(cpu_number)) { case 1: asm(&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (cpu_number)); break; case 2: asm(&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (cpu_number)); break; case 4: asm(&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (cpu_number)); break; case 8: asm(&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (cpu_number)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 2: pscr_ret__ = ({ typeof(cpu_number) pfo_ret__; switch (sizeof(cpu_number)) { case 1: asm(&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (cpu_number)); break; case 2: asm(&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (cpu_number)); break; case 4: asm(&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (cpu_number)); break; case 8: asm(&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (cpu_number)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 4: pscr_ret__ = ({ typeof(cpu_number) pfo_ret__; switch (sizeof(cpu_number)) { case 1: asm(&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (cpu_number)); break; case 2: asm(&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (cpu_number)); break; case 4: asm(&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (cpu_number)); break; case 8: asm(&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (cpu_number)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 8: pscr_ret__ = ({ typeof(cpu_number) pfo_ret__; switch (sizeof(cpu_number)) { case 1: asm(&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (cpu_number)); break; case 2: asm(&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (cpu_number)); break; case 4: asm(&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (cpu_number)); break; case 8: asm(&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (cpu_number)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; }))" data-ref="_M/smp_processor_id">smp_processor_id</a>());</td></tr>
<tr><th id="1724">1724</th><td>}</td></tr>
<tr><th id="1725">1725</th><td></td></tr>
<tr><th id="1726">1726</th><td><i>/* Increment -&gt;ticks_this_gp for all flavors of RCU. */</i></td></tr>
<tr><th id="1727">1727</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="increment_cpu_stall_ticks" title='increment_cpu_stall_ticks' data-ref="increment_cpu_stall_ticks">increment_cpu_stall_ticks</dfn>(<em>void</em>)</td></tr>
<tr><th id="1728">1728</th><td>{</td></tr>
<tr><th id="1729">1729</th><td>	<b>struct</b> <a class="type" href="tree.h.html#rcu_state" title='rcu_state' data-ref="rcu_state">rcu_state</a> *<dfn class="local col5 decl" id="575rsp" title='rsp' data-type='struct rcu_state *' data-ref="575rsp">rsp</dfn>;</td></tr>
<tr><th id="1730">1730</th><td></td></tr>
<tr><th id="1731">1731</th><td>	<a class="macro" href="tree.h.html#422" title="for ((rsp) = ({ void *__mptr = (void *)((&amp;rcu_struct_flavors)-&gt;next); do { bool __cond = !(!(!__builtin_types_compatible_p(typeof(*((&amp;rcu_struct_flavors)-&gt;next)), typeof(((typeof(*(rsp)) *)0)-&gt;flavors)) &amp;&amp; !__builtin_types_compatible_p(typeof(*((&amp;rcu_struct_flavors)-&gt;next)), typeof(void)))); extern void __compiletime_assert_1731(void) ; if (__cond) __compiletime_assert_1731(); do { ((void)sizeof(char[1 - 2 * __cond])); } while (0); } while (0); ((typeof(*(rsp)) *)(__mptr - __builtin_offsetof(typeof(*(rsp)), flavors))); }); &amp;(rsp)-&gt;flavors != (&amp;rcu_struct_flavors); (rsp) = ({ void *__mptr = (void *)(((rsp))-&gt;flavors.next); do { bool __cond = !(!(!__builtin_types_compatible_p(typeof(*(((rsp))-&gt;flavors.next)), typeof(((typeof(*((rsp))) *)0)-&gt;flavors)) &amp;&amp; !__builtin_types_compatible_p(typeof(*(((rsp))-&gt;flavors.next)), typeof(void)))); extern void __compiletime_assert_1731(void) ; if (__cond) __compiletime_assert_1731(); do { ((void)sizeof(char[1 - 2 * __cond])); } while (0); } while (0); ((typeof(*((rsp))) *)(__mptr - __builtin_offsetof(typeof(*((rsp))), flavors))); }))" data-ref="_M/for_each_rcu_flavor">for_each_rcu_flavor</a>(<a class="local col5 ref" href="#575rsp" title='rsp' data-ref="575rsp">rsp</a>)</td></tr>
<tr><th id="1732">1732</th><td>		<a class="macro" href="../../include/linux/percpu-defs.h.html#420" title="do { do { const void *__vpp_verify = (typeof((&amp;(rsp-&gt;rda-&gt;ticks_this_gp)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(rsp-&gt;rda-&gt;ticks_this_gp)) { case 1: do { typedef typeof((rsp-&gt;rda-&gt;ticks_this_gp)) pao_T__; const int pao_ID__ = (__builtin_constant_p(1) &amp;&amp; ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { pao_T__ pao_tmp__; pao_tmp__ = (1); (void)pao_tmp__; } switch (sizeof((rsp-&gt;rda-&gt;ticks_this_gp))) { case 1: if (pao_ID__ == 1) asm(&quot;incb &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else if (pao_ID__ == -1) asm(&quot;decb &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else asm(&quot;addb %1, &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp)) : &quot;qi&quot; ((pao_T__)(1))); break; case 2: if (pao_ID__ == 1) asm(&quot;incw &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else if (pao_ID__ == -1) asm(&quot;decw &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else asm(&quot;addw %1, &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp)) : &quot;ri&quot; ((pao_T__)(1))); break; case 4: if (pao_ID__ == 1) asm(&quot;incl &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else if (pao_ID__ == -1) asm(&quot;decl &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else asm(&quot;addl %1, &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp)) : &quot;ri&quot; ((pao_T__)(1))); break; case 8: if (pao_ID__ == 1) asm(&quot;incq &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else if (pao_ID__ == -1) asm(&quot;decq &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else asm(&quot;addq %1, &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp)) : &quot;re&quot; ((pao_T__)(1))); break; default: __bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((rsp-&gt;rda-&gt;ticks_this_gp)) pao_T__; const int pao_ID__ = (__builtin_constant_p(1) &amp;&amp; ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { pao_T__ pao_tmp__; pao_tmp__ = (1); (void)pao_tmp__; } switch (sizeof((rsp-&gt;rda-&gt;ticks_this_gp))) { case 1: if (pao_ID__ == 1) asm(&quot;incb &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else if (pao_ID__ == -1) asm(&quot;decb &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else asm(&quot;addb %1, &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp)) : &quot;qi&quot; ((pao_T__)(1))); break; case 2: if (pao_ID__ == 1) asm(&quot;incw &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else if (pao_ID__ == -1) asm(&quot;decw &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else asm(&quot;addw %1, &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp)) : &quot;ri&quot; ((pao_T__)(1))); break; case 4: if (pao_ID__ == 1) asm(&quot;incl &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else if (pao_ID__ == -1) asm(&quot;decl &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else asm(&quot;addl %1, &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp)) : &quot;ri&quot; ((pao_T__)(1))); break; case 8: if (pao_ID__ == 1) asm(&quot;incq &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else if (pao_ID__ == -1) asm(&quot;decq &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else asm(&quot;addq %1, &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp)) : &quot;re&quot; ((pao_T__)(1))); break; default: __bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((rsp-&gt;rda-&gt;ticks_this_gp)) pao_T__; const int pao_ID__ = (__builtin_constant_p(1) &amp;&amp; ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { pao_T__ pao_tmp__; pao_tmp__ = (1); (void)pao_tmp__; } switch (sizeof((rsp-&gt;rda-&gt;ticks_this_gp))) { case 1: if (pao_ID__ == 1) asm(&quot;incb &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else if (pao_ID__ == -1) asm(&quot;decb &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else asm(&quot;addb %1, &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp)) : &quot;qi&quot; ((pao_T__)(1))); break; case 2: if (pao_ID__ == 1) asm(&quot;incw &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else if (pao_ID__ == -1) asm(&quot;decw &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else asm(&quot;addw %1, &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp)) : &quot;ri&quot; ((pao_T__)(1))); break; case 4: if (pao_ID__ == 1) asm(&quot;incl &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else if (pao_ID__ == -1) asm(&quot;decl &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else asm(&quot;addl %1, &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp)) : &quot;ri&quot; ((pao_T__)(1))); break; case 8: if (pao_ID__ == 1) asm(&quot;incq &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else if (pao_ID__ == -1) asm(&quot;decq &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else asm(&quot;addq %1, &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp)) : &quot;re&quot; ((pao_T__)(1))); break; default: __bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((rsp-&gt;rda-&gt;ticks_this_gp)) pao_T__; const int pao_ID__ = (__builtin_constant_p(1) &amp;&amp; ((1) == 1 || (1) == -1)) ? (int)(1) : 0; if (0) { pao_T__ pao_tmp__; pao_tmp__ = (1); (void)pao_tmp__; } switch (sizeof((rsp-&gt;rda-&gt;ticks_this_gp))) { case 1: if (pao_ID__ == 1) asm(&quot;incb &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else if (pao_ID__ == -1) asm(&quot;decb &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else asm(&quot;addb %1, &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp)) : &quot;qi&quot; ((pao_T__)(1))); break; case 2: if (pao_ID__ == 1) asm(&quot;incw &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else if (pao_ID__ == -1) asm(&quot;decw &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else asm(&quot;addw %1, &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp)) : &quot;ri&quot; ((pao_T__)(1))); break; case 4: if (pao_ID__ == 1) asm(&quot;incl &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else if (pao_ID__ == -1) asm(&quot;decl &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else asm(&quot;addl %1, &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp)) : &quot;ri&quot; ((pao_T__)(1))); break; case 8: if (pao_ID__ == 1) asm(&quot;incq &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else if (pao_ID__ == -1) asm(&quot;decq &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp))); else asm(&quot;addq %1, &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rsp-&gt;rda-&gt;ticks_this_gp)) : &quot;re&quot; ((pao_T__)(1))); break; default: __bad_percpu_size(); } } while (0);break; default: __bad_size_call_parameter();break; } } while (0)" data-ref="_M/raw_cpu_inc">raw_cpu_inc</a>(<a class="local col5 ref" href="#575rsp" title='rsp' data-ref="575rsp">rsp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_state::rda" title='rcu_state::rda' data-ref="rcu_state::rda">rda</a>-&gt;<a class="ref field" href="tree.h.html#rcu_data::ticks_this_gp" title='rcu_data::ticks_this_gp' data-ref="rcu_data::ticks_this_gp">ticks_this_gp</a>);</td></tr>
<tr><th id="1733">1733</th><td>}</td></tr>
<tr><th id="1734">1734</th><td></td></tr>
<tr><th id="1735">1735</th><td><u>#<span data-ppcond="1735">ifdef</span> <span class="macro" data-ref="_M/CONFIG_RCU_NOCB_CPU">CONFIG_RCU_NOCB_CPU</span></u></td></tr>
<tr><th id="1736">1736</th><td></td></tr>
<tr><th id="1737">1737</th><td><i>/*</i></td></tr>
<tr><th id="1738">1738</th><td><i> * Offload callback processing from the boot-time-specified set of CPUs</i></td></tr>
<tr><th id="1739">1739</th><td><i> * specified by rcu_nocb_mask.  For each CPU in the set, there is a</i></td></tr>
<tr><th id="1740">1740</th><td><i> * kthread created that pulls the callbacks from the corresponding CPU,</i></td></tr>
<tr><th id="1741">1741</th><td><i> * waits for a grace period to elapse, and invokes the callbacks.</i></td></tr>
<tr><th id="1742">1742</th><td><i> * The no-CBs CPUs do a wake_up() on their kthread when they insert</i></td></tr>
<tr><th id="1743">1743</th><td><i> * a callback into any empty list, unless the rcu_nocb_poll boot parameter</i></td></tr>
<tr><th id="1744">1744</th><td><i> * has been specified, in which case each kthread actively polls its</i></td></tr>
<tr><th id="1745">1745</th><td><i> * CPU.  (Which isn't so great for energy efficiency, but which does</i></td></tr>
<tr><th id="1746">1746</th><td><i> * reduce RCU's overhead on that CPU.)</i></td></tr>
<tr><th id="1747">1747</th><td><i> *</i></td></tr>
<tr><th id="1748">1748</th><td><i> * This is intended to be used in conjunction with Frederic Weisbecker's</i></td></tr>
<tr><th id="1749">1749</th><td><i> * adaptive-idle work, which would seriously reduce OS jitter on CPUs</i></td></tr>
<tr><th id="1750">1750</th><td><i> * running CPU-bound user-mode computations.</i></td></tr>
<tr><th id="1751">1751</th><td><i> *</i></td></tr>
<tr><th id="1752">1752</th><td><i> * Offloading of callback processing could also in theory be used as</i></td></tr>
<tr><th id="1753">1753</th><td><i> * an energy-efficiency measure because CPUs with no RCU callbacks</i></td></tr>
<tr><th id="1754">1754</th><td><i> * queued are more aggressive about entering dyntick-idle mode.</i></td></tr>
<tr><th id="1755">1755</th><td><i> */</i></td></tr>
<tr><th id="1756">1756</th><td></td></tr>
<tr><th id="1757">1757</th><td></td></tr>
<tr><th id="1758">1758</th><td><i>/* Parse the boot-time rcu_nocb_mask CPU list from the kernel parameters. */</i></td></tr>
<tr><th id="1759">1759</th><td><em>static</em> <em>int</em> __init rcu_nocb_setup(<em>char</em> *str)</td></tr>
<tr><th id="1760">1760</th><td>{</td></tr>
<tr><th id="1761">1761</th><td>	alloc_bootmem_cpumask_var(&amp;rcu_nocb_mask);</td></tr>
<tr><th id="1762">1762</th><td>	have_rcu_nocb_mask = true;</td></tr>
<tr><th id="1763">1763</th><td>	cpulist_parse(str, rcu_nocb_mask);</td></tr>
<tr><th id="1764">1764</th><td>	<b>return</b> <var>1</var>;</td></tr>
<tr><th id="1765">1765</th><td>}</td></tr>
<tr><th id="1766">1766</th><td>__setup(<q>"rcu_nocbs="</q>, rcu_nocb_setup);</td></tr>
<tr><th id="1767">1767</th><td></td></tr>
<tr><th id="1768">1768</th><td><em>static</em> <em>int</em> __init parse_rcu_nocb_poll(<em>char</em> *arg)</td></tr>
<tr><th id="1769">1769</th><td>{</td></tr>
<tr><th id="1770">1770</th><td>	rcu_nocb_poll = true;</td></tr>
<tr><th id="1771">1771</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1772">1772</th><td>}</td></tr>
<tr><th id="1773">1773</th><td>early_param(<q>"rcu_nocb_poll"</q>, parse_rcu_nocb_poll);</td></tr>
<tr><th id="1774">1774</th><td></td></tr>
<tr><th id="1775">1775</th><td><i>/*</i></td></tr>
<tr><th id="1776">1776</th><td><i> * Wake up any no-CBs CPUs' kthreads that were waiting on the just-ended</i></td></tr>
<tr><th id="1777">1777</th><td><i> * grace period.</i></td></tr>
<tr><th id="1778">1778</th><td><i> */</i></td></tr>
<tr><th id="1779">1779</th><td><em>static</em> <em>void</em> rcu_nocb_gp_cleanup(<b>struct</b> swait_queue_head *sq)</td></tr>
<tr><th id="1780">1780</th><td>{</td></tr>
<tr><th id="1781">1781</th><td>	swake_up_all(sq);</td></tr>
<tr><th id="1782">1782</th><td>}</td></tr>
<tr><th id="1783">1783</th><td></td></tr>
<tr><th id="1784">1784</th><td><i>/*</i></td></tr>
<tr><th id="1785">1785</th><td><i> * Set the root rcu_node structure's -&gt;need_future_gp field</i></td></tr>
<tr><th id="1786">1786</th><td><i> * based on the sum of those of all rcu_node structures.  This does</i></td></tr>
<tr><th id="1787">1787</th><td><i> * double-count the root rcu_node structure's requests, but this</i></td></tr>
<tr><th id="1788">1788</th><td><i> * is necessary to handle the possibility of a rcu_nocb_kthread()</i></td></tr>
<tr><th id="1789">1789</th><td><i> * having awakened during the time that the rcu_node structures</i></td></tr>
<tr><th id="1790">1790</th><td><i> * were being updated for the end of the previous grace period.</i></td></tr>
<tr><th id="1791">1791</th><td><i> */</i></td></tr>
<tr><th id="1792">1792</th><td><em>static</em> <em>void</em> rcu_nocb_gp_set(<b>struct</b> rcu_node *rnp, <em>int</em> nrq)</td></tr>
<tr><th id="1793">1793</th><td>{</td></tr>
<tr><th id="1794">1794</th><td>	rnp-&gt;need_future_gp[(rnp-&gt;completed + <var>1</var>) &amp; <var>0x1</var>] += nrq;</td></tr>
<tr><th id="1795">1795</th><td>}</td></tr>
<tr><th id="1796">1796</th><td></td></tr>
<tr><th id="1797">1797</th><td><em>static</em> <b>struct</b> swait_queue_head *rcu_nocb_gp_get(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="1798">1798</th><td>{</td></tr>
<tr><th id="1799">1799</th><td>	<b>return</b> &amp;rnp-&gt;nocb_gp_wq[rnp-&gt;completed &amp; <var>0x1</var>];</td></tr>
<tr><th id="1800">1800</th><td>}</td></tr>
<tr><th id="1801">1801</th><td></td></tr>
<tr><th id="1802">1802</th><td><em>static</em> <em>void</em> rcu_init_one_nocb(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="1803">1803</th><td>{</td></tr>
<tr><th id="1804">1804</th><td>	init_swait_queue_head(&amp;rnp-&gt;nocb_gp_wq[<var>0</var>]);</td></tr>
<tr><th id="1805">1805</th><td>	init_swait_queue_head(&amp;rnp-&gt;nocb_gp_wq[<var>1</var>]);</td></tr>
<tr><th id="1806">1806</th><td>}</td></tr>
<tr><th id="1807">1807</th><td></td></tr>
<tr><th id="1808">1808</th><td><i>/* Is the specified CPU a no-CBs CPU? */</i></td></tr>
<tr><th id="1809">1809</th><td>bool rcu_is_nocb_cpu(<em>int</em> cpu)</td></tr>
<tr><th id="1810">1810</th><td>{</td></tr>
<tr><th id="1811">1811</th><td>	<b>if</b> (have_rcu_nocb_mask)</td></tr>
<tr><th id="1812">1812</th><td>		<b>return</b> cpumask_test_cpu(cpu, rcu_nocb_mask);</td></tr>
<tr><th id="1813">1813</th><td>	<b>return</b> false;</td></tr>
<tr><th id="1814">1814</th><td>}</td></tr>
<tr><th id="1815">1815</th><td></td></tr>
<tr><th id="1816">1816</th><td><i>/*</i></td></tr>
<tr><th id="1817">1817</th><td><i> * Kick the leader kthread for this NOCB group.  Caller holds -&gt;nocb_lock</i></td></tr>
<tr><th id="1818">1818</th><td><i> * and this function releases it.</i></td></tr>
<tr><th id="1819">1819</th><td><i> */</i></td></tr>
<tr><th id="1820">1820</th><td><em>static</em> <em>void</em> __wake_nocb_leader(<b>struct</b> rcu_data *rdp, bool force,</td></tr>
<tr><th id="1821">1821</th><td>			       <em>unsigned</em> <em>long</em> flags)</td></tr>
<tr><th id="1822">1822</th><td>	__releases(rdp-&gt;nocb_lock)</td></tr>
<tr><th id="1823">1823</th><td>{</td></tr>
<tr><th id="1824">1824</th><td>	<b>struct</b> rcu_data *rdp_leader = rdp-&gt;nocb_leader;</td></tr>
<tr><th id="1825">1825</th><td></td></tr>
<tr><th id="1826">1826</th><td>	lockdep_assert_held(&amp;rdp-&gt;nocb_lock);</td></tr>
<tr><th id="1827">1827</th><td>	<b>if</b> (!READ_ONCE(rdp_leader-&gt;nocb_kthread)) {</td></tr>
<tr><th id="1828">1828</th><td>		raw_spin_unlock_irqrestore(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1829">1829</th><td>		<b>return</b>;</td></tr>
<tr><th id="1830">1830</th><td>	}</td></tr>
<tr><th id="1831">1831</th><td>	<b>if</b> (rdp_leader-&gt;nocb_leader_sleep || force) {</td></tr>
<tr><th id="1832">1832</th><td>		<i>/* Prior smp_mb__after_atomic() orders against prior enqueue. */</i></td></tr>
<tr><th id="1833">1833</th><td>		WRITE_ONCE(rdp_leader-&gt;nocb_leader_sleep, false);</td></tr>
<tr><th id="1834">1834</th><td>		del_timer(&amp;rdp-&gt;nocb_timer);</td></tr>
<tr><th id="1835">1835</th><td>		raw_spin_unlock_irqrestore(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1836">1836</th><td>		smp_mb(); <i>/* -&gt;nocb_leader_sleep before swake_up(). */</i></td></tr>
<tr><th id="1837">1837</th><td>		swake_up(&amp;rdp_leader-&gt;nocb_wq);</td></tr>
<tr><th id="1838">1838</th><td>	} <b>else</b> {</td></tr>
<tr><th id="1839">1839</th><td>		raw_spin_unlock_irqrestore(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1840">1840</th><td>	}</td></tr>
<tr><th id="1841">1841</th><td>}</td></tr>
<tr><th id="1842">1842</th><td></td></tr>
<tr><th id="1843">1843</th><td><i>/*</i></td></tr>
<tr><th id="1844">1844</th><td><i> * Kick the leader kthread for this NOCB group, but caller has not</i></td></tr>
<tr><th id="1845">1845</th><td><i> * acquired locks.</i></td></tr>
<tr><th id="1846">1846</th><td><i> */</i></td></tr>
<tr><th id="1847">1847</th><td><em>static</em> <em>void</em> wake_nocb_leader(<b>struct</b> rcu_data *rdp, bool force)</td></tr>
<tr><th id="1848">1848</th><td>{</td></tr>
<tr><th id="1849">1849</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="1850">1850</th><td></td></tr>
<tr><th id="1851">1851</th><td>	raw_spin_lock_irqsave(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1852">1852</th><td>	__wake_nocb_leader(rdp, force, flags);</td></tr>
<tr><th id="1853">1853</th><td>}</td></tr>
<tr><th id="1854">1854</th><td></td></tr>
<tr><th id="1855">1855</th><td><i>/*</i></td></tr>
<tr><th id="1856">1856</th><td><i> * Arrange to wake the leader kthread for this NOCB group at some</i></td></tr>
<tr><th id="1857">1857</th><td><i> * future time when it is safe to do so.</i></td></tr>
<tr><th id="1858">1858</th><td><i> */</i></td></tr>
<tr><th id="1859">1859</th><td><em>static</em> <em>void</em> wake_nocb_leader_defer(<b>struct</b> rcu_data *rdp, <em>int</em> waketype,</td></tr>
<tr><th id="1860">1860</th><td>				   <em>const</em> <em>char</em> *reason)</td></tr>
<tr><th id="1861">1861</th><td>{</td></tr>
<tr><th id="1862">1862</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="1863">1863</th><td></td></tr>
<tr><th id="1864">1864</th><td>	raw_spin_lock_irqsave(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1865">1865</th><td>	<b>if</b> (rdp-&gt;nocb_defer_wakeup == RCU_NOCB_WAKE_NOT)</td></tr>
<tr><th id="1866">1866</th><td>		mod_timer(&amp;rdp-&gt;nocb_timer, jiffies + <var>1</var>);</td></tr>
<tr><th id="1867">1867</th><td>	WRITE_ONCE(rdp-&gt;nocb_defer_wakeup, waketype);</td></tr>
<tr><th id="1868">1868</th><td>	trace_rcu_nocb_wake(rdp-&gt;rsp-&gt;name, rdp-&gt;cpu, reason);</td></tr>
<tr><th id="1869">1869</th><td>	raw_spin_unlock_irqrestore(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1870">1870</th><td>}</td></tr>
<tr><th id="1871">1871</th><td></td></tr>
<tr><th id="1872">1872</th><td><i>/*</i></td></tr>
<tr><th id="1873">1873</th><td><i> * Does the specified CPU need an RCU callback for the specified flavor</i></td></tr>
<tr><th id="1874">1874</th><td><i> * of rcu_barrier()?</i></td></tr>
<tr><th id="1875">1875</th><td><i> */</i></td></tr>
<tr><th id="1876">1876</th><td><em>static</em> bool rcu_nocb_cpu_needs_barrier(<b>struct</b> rcu_state *rsp, <em>int</em> cpu)</td></tr>
<tr><th id="1877">1877</th><td>{</td></tr>
<tr><th id="1878">1878</th><td>	<b>struct</b> rcu_data *rdp = per_cpu_ptr(rsp-&gt;rda, cpu);</td></tr>
<tr><th id="1879">1879</th><td>	<em>unsigned</em> <em>long</em> ret;</td></tr>
<tr><th id="1880">1880</th><td><u>#ifdef CONFIG_PROVE_RCU</u></td></tr>
<tr><th id="1881">1881</th><td>	<b>struct</b> rcu_head *rhp;</td></tr>
<tr><th id="1882">1882</th><td><u>#endif /* #ifdef CONFIG_PROVE_RCU */</u></td></tr>
<tr><th id="1883">1883</th><td></td></tr>
<tr><th id="1884">1884</th><td>	<i>/*</i></td></tr>
<tr><th id="1885">1885</th><td><i>	 * Check count of all no-CBs callbacks awaiting invocation.</i></td></tr>
<tr><th id="1886">1886</th><td><i>	 * There needs to be a barrier before this function is called,</i></td></tr>
<tr><th id="1887">1887</th><td><i>	 * but associated with a prior determination that no more</i></td></tr>
<tr><th id="1888">1888</th><td><i>	 * callbacks would be posted.  In the worst case, the first</i></td></tr>
<tr><th id="1889">1889</th><td><i>	 * barrier in _rcu_barrier() suffices (but the caller cannot</i></td></tr>
<tr><th id="1890">1890</th><td><i>	 * necessarily rely on this, not a substitute for the caller</i></td></tr>
<tr><th id="1891">1891</th><td><i>	 * getting the concurrency design right!).  There must also be</i></td></tr>
<tr><th id="1892">1892</th><td><i>	 * a barrier between the following load an posting of a callback</i></td></tr>
<tr><th id="1893">1893</th><td><i>	 * (if a callback is in fact needed).  This is associated with an</i></td></tr>
<tr><th id="1894">1894</th><td><i>	 * atomic_inc() in the caller.</i></td></tr>
<tr><th id="1895">1895</th><td><i>	 */</i></td></tr>
<tr><th id="1896">1896</th><td>	ret = atomic_long_read(&amp;rdp-&gt;nocb_q_count);</td></tr>
<tr><th id="1897">1897</th><td></td></tr>
<tr><th id="1898">1898</th><td><u>#ifdef CONFIG_PROVE_RCU</u></td></tr>
<tr><th id="1899">1899</th><td>	rhp = READ_ONCE(rdp-&gt;nocb_head);</td></tr>
<tr><th id="1900">1900</th><td>	<b>if</b> (!rhp)</td></tr>
<tr><th id="1901">1901</th><td>		rhp = READ_ONCE(rdp-&gt;nocb_gp_head);</td></tr>
<tr><th id="1902">1902</th><td>	<b>if</b> (!rhp)</td></tr>
<tr><th id="1903">1903</th><td>		rhp = READ_ONCE(rdp-&gt;nocb_follower_head);</td></tr>
<tr><th id="1904">1904</th><td></td></tr>
<tr><th id="1905">1905</th><td>	<i>/* Having no rcuo kthread but CBs after scheduler starts is bad! */</i></td></tr>
<tr><th id="1906">1906</th><td>	<b>if</b> (!READ_ONCE(rdp-&gt;nocb_kthread) &amp;&amp; rhp &amp;&amp;</td></tr>
<tr><th id="1907">1907</th><td>	    rcu_scheduler_fully_active) {</td></tr>
<tr><th id="1908">1908</th><td>		<i>/* RCU callback enqueued before CPU first came online??? */</i></td></tr>
<tr><th id="1909">1909</th><td>		pr_err(<q>"RCU: Never-onlined no-CBs CPU %d has CB %p\n"</q>,</td></tr>
<tr><th id="1910">1910</th><td>		       cpu, rhp-&gt;func);</td></tr>
<tr><th id="1911">1911</th><td>		WARN_ON_ONCE(<var>1</var>);</td></tr>
<tr><th id="1912">1912</th><td>	}</td></tr>
<tr><th id="1913">1913</th><td><u>#endif /* #ifdef CONFIG_PROVE_RCU */</u></td></tr>
<tr><th id="1914">1914</th><td></td></tr>
<tr><th id="1915">1915</th><td>	<b>return</b> !!ret;</td></tr>
<tr><th id="1916">1916</th><td>}</td></tr>
<tr><th id="1917">1917</th><td></td></tr>
<tr><th id="1918">1918</th><td><i>/*</i></td></tr>
<tr><th id="1919">1919</th><td><i> * Enqueue the specified string of rcu_head structures onto the specified</i></td></tr>
<tr><th id="1920">1920</th><td><i> * CPU's no-CBs lists.  The CPU is specified by rdp, the head of the</i></td></tr>
<tr><th id="1921">1921</th><td><i> * string by rhp, and the tail of the string by rhtp.  The non-lazy/lazy</i></td></tr>
<tr><th id="1922">1922</th><td><i> * counts are supplied by rhcount and rhcount_lazy.</i></td></tr>
<tr><th id="1923">1923</th><td><i> *</i></td></tr>
<tr><th id="1924">1924</th><td><i> * If warranted, also wake up the kthread servicing this CPUs queues.</i></td></tr>
<tr><th id="1925">1925</th><td><i> */</i></td></tr>
<tr><th id="1926">1926</th><td><em>static</em> <em>void</em> __call_rcu_nocb_enqueue(<b>struct</b> rcu_data *rdp,</td></tr>
<tr><th id="1927">1927</th><td>				    <b>struct</b> rcu_head *rhp,</td></tr>
<tr><th id="1928">1928</th><td>				    <b>struct</b> rcu_head **rhtp,</td></tr>
<tr><th id="1929">1929</th><td>				    <em>int</em> rhcount, <em>int</em> rhcount_lazy,</td></tr>
<tr><th id="1930">1930</th><td>				    <em>unsigned</em> <em>long</em> flags)</td></tr>
<tr><th id="1931">1931</th><td>{</td></tr>
<tr><th id="1932">1932</th><td>	<em>int</em> len;</td></tr>
<tr><th id="1933">1933</th><td>	<b>struct</b> rcu_head **old_rhpp;</td></tr>
<tr><th id="1934">1934</th><td>	<b>struct</b> task_struct *t;</td></tr>
<tr><th id="1935">1935</th><td></td></tr>
<tr><th id="1936">1936</th><td>	<i>/* Enqueue the callback on the nocb list and update counts. */</i></td></tr>
<tr><th id="1937">1937</th><td>	atomic_long_add(rhcount, &amp;rdp-&gt;nocb_q_count);</td></tr>
<tr><th id="1938">1938</th><td>	<i>/* rcu_barrier() relies on -&gt;nocb_q_count add before xchg. */</i></td></tr>
<tr><th id="1939">1939</th><td>	old_rhpp = xchg(&amp;rdp-&gt;nocb_tail, rhtp);</td></tr>
<tr><th id="1940">1940</th><td>	WRITE_ONCE(*old_rhpp, rhp);</td></tr>
<tr><th id="1941">1941</th><td>	atomic_long_add(rhcount_lazy, &amp;rdp-&gt;nocb_q_count_lazy);</td></tr>
<tr><th id="1942">1942</th><td>	smp_mb__after_atomic(); <i>/* Store *old_rhpp before _wake test. */</i></td></tr>
<tr><th id="1943">1943</th><td></td></tr>
<tr><th id="1944">1944</th><td>	<i>/* If we are not being polled and there is a kthread, awaken it ... */</i></td></tr>
<tr><th id="1945">1945</th><td>	t = READ_ONCE(rdp-&gt;nocb_kthread);</td></tr>
<tr><th id="1946">1946</th><td>	<b>if</b> (rcu_nocb_poll || !t) {</td></tr>
<tr><th id="1947">1947</th><td>		trace_rcu_nocb_wake(rdp-&gt;rsp-&gt;name, rdp-&gt;cpu,</td></tr>
<tr><th id="1948">1948</th><td>				    TPS(<q>"WakeNotPoll"</q>));</td></tr>
<tr><th id="1949">1949</th><td>		<b>return</b>;</td></tr>
<tr><th id="1950">1950</th><td>	}</td></tr>
<tr><th id="1951">1951</th><td>	len = atomic_long_read(&amp;rdp-&gt;nocb_q_count);</td></tr>
<tr><th id="1952">1952</th><td>	<b>if</b> (old_rhpp == &amp;rdp-&gt;nocb_head) {</td></tr>
<tr><th id="1953">1953</th><td>		<b>if</b> (!irqs_disabled_flags(flags)) {</td></tr>
<tr><th id="1954">1954</th><td>			<i>/* ... if queue was empty ... */</i></td></tr>
<tr><th id="1955">1955</th><td>			wake_nocb_leader(rdp, false);</td></tr>
<tr><th id="1956">1956</th><td>			trace_rcu_nocb_wake(rdp-&gt;rsp-&gt;name, rdp-&gt;cpu,</td></tr>
<tr><th id="1957">1957</th><td>					    TPS(<q>"WakeEmpty"</q>));</td></tr>
<tr><th id="1958">1958</th><td>		} <b>else</b> {</td></tr>
<tr><th id="1959">1959</th><td>			wake_nocb_leader_defer(rdp, RCU_NOCB_WAKE,</td></tr>
<tr><th id="1960">1960</th><td>					       TPS(<q>"WakeEmptyIsDeferred"</q>));</td></tr>
<tr><th id="1961">1961</th><td>		}</td></tr>
<tr><th id="1962">1962</th><td>		rdp-&gt;qlen_last_fqs_check = <var>0</var>;</td></tr>
<tr><th id="1963">1963</th><td>	} <b>else</b> <b>if</b> (len &gt; rdp-&gt;qlen_last_fqs_check + qhimark) {</td></tr>
<tr><th id="1964">1964</th><td>		<i>/* ... or if many callbacks queued. */</i></td></tr>
<tr><th id="1965">1965</th><td>		<b>if</b> (!irqs_disabled_flags(flags)) {</td></tr>
<tr><th id="1966">1966</th><td>			wake_nocb_leader(rdp, true);</td></tr>
<tr><th id="1967">1967</th><td>			trace_rcu_nocb_wake(rdp-&gt;rsp-&gt;name, rdp-&gt;cpu,</td></tr>
<tr><th id="1968">1968</th><td>					    TPS(<q>"WakeOvf"</q>));</td></tr>
<tr><th id="1969">1969</th><td>		} <b>else</b> {</td></tr>
<tr><th id="1970">1970</th><td>			wake_nocb_leader_defer(rdp, RCU_NOCB_WAKE,</td></tr>
<tr><th id="1971">1971</th><td>					       TPS(<q>"WakeOvfIsDeferred"</q>));</td></tr>
<tr><th id="1972">1972</th><td>		}</td></tr>
<tr><th id="1973">1973</th><td>		rdp-&gt;qlen_last_fqs_check = LONG_MAX / <var>2</var>;</td></tr>
<tr><th id="1974">1974</th><td>	} <b>else</b> {</td></tr>
<tr><th id="1975">1975</th><td>		trace_rcu_nocb_wake(rdp-&gt;rsp-&gt;name, rdp-&gt;cpu, TPS(<q>"WakeNot"</q>));</td></tr>
<tr><th id="1976">1976</th><td>	}</td></tr>
<tr><th id="1977">1977</th><td>	<b>return</b>;</td></tr>
<tr><th id="1978">1978</th><td>}</td></tr>
<tr><th id="1979">1979</th><td></td></tr>
<tr><th id="1980">1980</th><td><i>/*</i></td></tr>
<tr><th id="1981">1981</th><td><i> * This is a helper for __call_rcu(), which invokes this when the normal</i></td></tr>
<tr><th id="1982">1982</th><td><i> * callback queue is inoperable.  If this is not a no-CBs CPU, this</i></td></tr>
<tr><th id="1983">1983</th><td><i> * function returns failure back to __call_rcu(), which can complain</i></td></tr>
<tr><th id="1984">1984</th><td><i> * appropriately.</i></td></tr>
<tr><th id="1985">1985</th><td><i> *</i></td></tr>
<tr><th id="1986">1986</th><td><i> * Otherwise, this function queues the callback where the corresponding</i></td></tr>
<tr><th id="1987">1987</th><td><i> * "rcuo" kthread can find it.</i></td></tr>
<tr><th id="1988">1988</th><td><i> */</i></td></tr>
<tr><th id="1989">1989</th><td><em>static</em> bool __call_rcu_nocb(<b>struct</b> rcu_data *rdp, <b>struct</b> rcu_head *rhp,</td></tr>
<tr><th id="1990">1990</th><td>			    bool lazy, <em>unsigned</em> <em>long</em> flags)</td></tr>
<tr><th id="1991">1991</th><td>{</td></tr>
<tr><th id="1992">1992</th><td></td></tr>
<tr><th id="1993">1993</th><td>	<b>if</b> (!rcu_is_nocb_cpu(rdp-&gt;cpu))</td></tr>
<tr><th id="1994">1994</th><td>		<b>return</b> false;</td></tr>
<tr><th id="1995">1995</th><td>	__call_rcu_nocb_enqueue(rdp, rhp, &amp;rhp-&gt;next, <var>1</var>, lazy, flags);</td></tr>
<tr><th id="1996">1996</th><td>	<b>if</b> (__is_kfree_rcu_offset((<em>unsigned</em> <em>long</em>)rhp-&gt;func))</td></tr>
<tr><th id="1997">1997</th><td>		trace_rcu_kfree_callback(rdp-&gt;rsp-&gt;name, rhp,</td></tr>
<tr><th id="1998">1998</th><td>					 (<em>unsigned</em> <em>long</em>)rhp-&gt;func,</td></tr>
<tr><th id="1999">1999</th><td>					 -atomic_long_read(&amp;rdp-&gt;nocb_q_count_lazy),</td></tr>
<tr><th id="2000">2000</th><td>					 -atomic_long_read(&amp;rdp-&gt;nocb_q_count));</td></tr>
<tr><th id="2001">2001</th><td>	<b>else</b></td></tr>
<tr><th id="2002">2002</th><td>		trace_rcu_callback(rdp-&gt;rsp-&gt;name, rhp,</td></tr>
<tr><th id="2003">2003</th><td>				   -atomic_long_read(&amp;rdp-&gt;nocb_q_count_lazy),</td></tr>
<tr><th id="2004">2004</th><td>				   -atomic_long_read(&amp;rdp-&gt;nocb_q_count));</td></tr>
<tr><th id="2005">2005</th><td></td></tr>
<tr><th id="2006">2006</th><td>	<i>/*</i></td></tr>
<tr><th id="2007">2007</th><td><i>	 * If called from an extended quiescent state with interrupts</i></td></tr>
<tr><th id="2008">2008</th><td><i>	 * disabled, invoke the RCU core in order to allow the idle-entry</i></td></tr>
<tr><th id="2009">2009</th><td><i>	 * deferred-wakeup check to function.</i></td></tr>
<tr><th id="2010">2010</th><td><i>	 */</i></td></tr>
<tr><th id="2011">2011</th><td>	<b>if</b> (irqs_disabled_flags(flags) &amp;&amp;</td></tr>
<tr><th id="2012">2012</th><td>	    !rcu_is_watching() &amp;&amp;</td></tr>
<tr><th id="2013">2013</th><td>	    cpu_online(smp_processor_id()))</td></tr>
<tr><th id="2014">2014</th><td>		invoke_rcu_core();</td></tr>
<tr><th id="2015">2015</th><td></td></tr>
<tr><th id="2016">2016</th><td>	<b>return</b> true;</td></tr>
<tr><th id="2017">2017</th><td>}</td></tr>
<tr><th id="2018">2018</th><td></td></tr>
<tr><th id="2019">2019</th><td><i>/*</i></td></tr>
<tr><th id="2020">2020</th><td><i> * Adopt orphaned callbacks on a no-CBs CPU, or return 0 if this is</i></td></tr>
<tr><th id="2021">2021</th><td><i> * not a no-CBs CPU.</i></td></tr>
<tr><th id="2022">2022</th><td><i> */</i></td></tr>
<tr><th id="2023">2023</th><td><em>static</em> bool __maybe_unused rcu_nocb_adopt_orphan_cbs(<b>struct</b> rcu_data *my_rdp,</td></tr>
<tr><th id="2024">2024</th><td>						     <b>struct</b> rcu_data *rdp,</td></tr>
<tr><th id="2025">2025</th><td>						     <em>unsigned</em> <em>long</em> flags)</td></tr>
<tr><th id="2026">2026</th><td>{</td></tr>
<tr><th id="2027">2027</th><td>	RCU_LOCKDEP_WARN(!irqs_disabled(), <q>"rcu_nocb_adopt_orphan_cbs() invoked with irqs enabled!!!"</q>);</td></tr>
<tr><th id="2028">2028</th><td>	<b>if</b> (!rcu_is_nocb_cpu(smp_processor_id()))</td></tr>
<tr><th id="2029">2029</th><td>		<b>return</b> false; <i>/* Not NOCBs CPU, caller must migrate CBs. */</i></td></tr>
<tr><th id="2030">2030</th><td>	__call_rcu_nocb_enqueue(my_rdp, rcu_segcblist_head(&amp;rdp-&gt;cblist),</td></tr>
<tr><th id="2031">2031</th><td>				rcu_segcblist_tail(&amp;rdp-&gt;cblist),</td></tr>
<tr><th id="2032">2032</th><td>				rcu_segcblist_n_cbs(&amp;rdp-&gt;cblist),</td></tr>
<tr><th id="2033">2033</th><td>				rcu_segcblist_n_lazy_cbs(&amp;rdp-&gt;cblist), flags);</td></tr>
<tr><th id="2034">2034</th><td>	rcu_segcblist_init(&amp;rdp-&gt;cblist);</td></tr>
<tr><th id="2035">2035</th><td>	rcu_segcblist_disable(&amp;rdp-&gt;cblist);</td></tr>
<tr><th id="2036">2036</th><td>	<b>return</b> true;</td></tr>
<tr><th id="2037">2037</th><td>}</td></tr>
<tr><th id="2038">2038</th><td></td></tr>
<tr><th id="2039">2039</th><td><i>/*</i></td></tr>
<tr><th id="2040">2040</th><td><i> * If necessary, kick off a new grace period, and either way wait</i></td></tr>
<tr><th id="2041">2041</th><td><i> * for a subsequent grace period to complete.</i></td></tr>
<tr><th id="2042">2042</th><td><i> */</i></td></tr>
<tr><th id="2043">2043</th><td><em>static</em> <em>void</em> rcu_nocb_wait_gp(<b>struct</b> rcu_data *rdp)</td></tr>
<tr><th id="2044">2044</th><td>{</td></tr>
<tr><th id="2045">2045</th><td>	<em>unsigned</em> <em>long</em> c;</td></tr>
<tr><th id="2046">2046</th><td>	bool d;</td></tr>
<tr><th id="2047">2047</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="2048">2048</th><td>	bool needwake;</td></tr>
<tr><th id="2049">2049</th><td>	<b>struct</b> rcu_node *rnp = rdp-&gt;mynode;</td></tr>
<tr><th id="2050">2050</th><td></td></tr>
<tr><th id="2051">2051</th><td>	raw_spin_lock_irqsave_rcu_node(rnp, flags);</td></tr>
<tr><th id="2052">2052</th><td>	needwake = rcu_start_future_gp(rnp, rdp, &amp;c);</td></tr>
<tr><th id="2053">2053</th><td>	raw_spin_unlock_irqrestore_rcu_node(rnp, flags);</td></tr>
<tr><th id="2054">2054</th><td>	<b>if</b> (needwake)</td></tr>
<tr><th id="2055">2055</th><td>		rcu_gp_kthread_wake(rdp-&gt;rsp);</td></tr>
<tr><th id="2056">2056</th><td></td></tr>
<tr><th id="2057">2057</th><td>	<i>/*</i></td></tr>
<tr><th id="2058">2058</th><td><i>	 * Wait for the grace period.  Do so interruptibly to avoid messing</i></td></tr>
<tr><th id="2059">2059</th><td><i>	 * up the load average.</i></td></tr>
<tr><th id="2060">2060</th><td><i>	 */</i></td></tr>
<tr><th id="2061">2061</th><td>	trace_rcu_future_gp(rnp, rdp, c, TPS(<q>"StartWait"</q>));</td></tr>
<tr><th id="2062">2062</th><td>	<b>for</b> (;;) {</td></tr>
<tr><th id="2063">2063</th><td>		swait_event_interruptible(</td></tr>
<tr><th id="2064">2064</th><td>			rnp-&gt;nocb_gp_wq[c &amp; <var>0x1</var>],</td></tr>
<tr><th id="2065">2065</th><td>			(d = ULONG_CMP_GE(READ_ONCE(rnp-&gt;completed), c)));</td></tr>
<tr><th id="2066">2066</th><td>		<b>if</b> (likely(d))</td></tr>
<tr><th id="2067">2067</th><td>			<b>break</b>;</td></tr>
<tr><th id="2068">2068</th><td>		WARN_ON(signal_pending(current));</td></tr>
<tr><th id="2069">2069</th><td>		trace_rcu_future_gp(rnp, rdp, c, TPS(<q>"ResumeWait"</q>));</td></tr>
<tr><th id="2070">2070</th><td>	}</td></tr>
<tr><th id="2071">2071</th><td>	trace_rcu_future_gp(rnp, rdp, c, TPS(<q>"EndWait"</q>));</td></tr>
<tr><th id="2072">2072</th><td>	smp_mb(); <i>/* Ensure that CB invocation happens after GP end. */</i></td></tr>
<tr><th id="2073">2073</th><td>}</td></tr>
<tr><th id="2074">2074</th><td></td></tr>
<tr><th id="2075">2075</th><td><i>/*</i></td></tr>
<tr><th id="2076">2076</th><td><i> * Leaders come here to wait for additional callbacks to show up.</i></td></tr>
<tr><th id="2077">2077</th><td><i> * This function does not return until callbacks appear.</i></td></tr>
<tr><th id="2078">2078</th><td><i> */</i></td></tr>
<tr><th id="2079">2079</th><td><em>static</em> <em>void</em> nocb_leader_wait(<b>struct</b> rcu_data *my_rdp)</td></tr>
<tr><th id="2080">2080</th><td>{</td></tr>
<tr><th id="2081">2081</th><td>	bool firsttime = true;</td></tr>
<tr><th id="2082">2082</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="2083">2083</th><td>	bool gotcbs;</td></tr>
<tr><th id="2084">2084</th><td>	<b>struct</b> rcu_data *rdp;</td></tr>
<tr><th id="2085">2085</th><td>	<b>struct</b> rcu_head **tail;</td></tr>
<tr><th id="2086">2086</th><td></td></tr>
<tr><th id="2087">2087</th><td>wait_again:</td></tr>
<tr><th id="2088">2088</th><td></td></tr>
<tr><th id="2089">2089</th><td>	<i>/* Wait for callbacks to appear. */</i></td></tr>
<tr><th id="2090">2090</th><td>	<b>if</b> (!rcu_nocb_poll) {</td></tr>
<tr><th id="2091">2091</th><td>		trace_rcu_nocb_wake(my_rdp-&gt;rsp-&gt;name, my_rdp-&gt;cpu, TPS(<q>"Sleep"</q>));</td></tr>
<tr><th id="2092">2092</th><td>		swait_event_interruptible(my_rdp-&gt;nocb_wq,</td></tr>
<tr><th id="2093">2093</th><td>				!READ_ONCE(my_rdp-&gt;nocb_leader_sleep));</td></tr>
<tr><th id="2094">2094</th><td>		raw_spin_lock_irqsave(&amp;my_rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="2095">2095</th><td>		my_rdp-&gt;nocb_leader_sleep = true;</td></tr>
<tr><th id="2096">2096</th><td>		WRITE_ONCE(my_rdp-&gt;nocb_defer_wakeup, RCU_NOCB_WAKE_NOT);</td></tr>
<tr><th id="2097">2097</th><td>		del_timer(&amp;my_rdp-&gt;nocb_timer);</td></tr>
<tr><th id="2098">2098</th><td>		raw_spin_unlock_irqrestore(&amp;my_rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="2099">2099</th><td>	} <b>else</b> <b>if</b> (firsttime) {</td></tr>
<tr><th id="2100">2100</th><td>		firsttime = false; <i>/* Don't drown trace log with "Poll"! */</i></td></tr>
<tr><th id="2101">2101</th><td>		trace_rcu_nocb_wake(my_rdp-&gt;rsp-&gt;name, my_rdp-&gt;cpu, TPS(<q>"Poll"</q>));</td></tr>
<tr><th id="2102">2102</th><td>	}</td></tr>
<tr><th id="2103">2103</th><td></td></tr>
<tr><th id="2104">2104</th><td>	<i>/*</i></td></tr>
<tr><th id="2105">2105</th><td><i>	 * Each pass through the following loop checks a follower for CBs.</i></td></tr>
<tr><th id="2106">2106</th><td><i>	 * We are our own first follower.  Any CBs found are moved to</i></td></tr>
<tr><th id="2107">2107</th><td><i>	 * nocb_gp_head, where they await a grace period.</i></td></tr>
<tr><th id="2108">2108</th><td><i>	 */</i></td></tr>
<tr><th id="2109">2109</th><td>	gotcbs = false;</td></tr>
<tr><th id="2110">2110</th><td>	smp_mb(); <i>/* wakeup and _sleep before -&gt;nocb_head reads. */</i></td></tr>
<tr><th id="2111">2111</th><td>	<b>for</b> (rdp = my_rdp; rdp; rdp = rdp-&gt;nocb_next_follower) {</td></tr>
<tr><th id="2112">2112</th><td>		rdp-&gt;nocb_gp_head = READ_ONCE(rdp-&gt;nocb_head);</td></tr>
<tr><th id="2113">2113</th><td>		<b>if</b> (!rdp-&gt;nocb_gp_head)</td></tr>
<tr><th id="2114">2114</th><td>			<b>continue</b>;  <i>/* No CBs here, try next follower. */</i></td></tr>
<tr><th id="2115">2115</th><td></td></tr>
<tr><th id="2116">2116</th><td>		<i>/* Move callbacks to wait-for-GP list, which is empty. */</i></td></tr>
<tr><th id="2117">2117</th><td>		WRITE_ONCE(rdp-&gt;nocb_head, NULL);</td></tr>
<tr><th id="2118">2118</th><td>		rdp-&gt;nocb_gp_tail = xchg(&amp;rdp-&gt;nocb_tail, &amp;rdp-&gt;nocb_head);</td></tr>
<tr><th id="2119">2119</th><td>		gotcbs = true;</td></tr>
<tr><th id="2120">2120</th><td>	}</td></tr>
<tr><th id="2121">2121</th><td></td></tr>
<tr><th id="2122">2122</th><td>	<i>/* No callbacks?  Sleep a bit if polling, and go retry.  */</i></td></tr>
<tr><th id="2123">2123</th><td>	<b>if</b> (unlikely(!gotcbs)) {</td></tr>
<tr><th id="2124">2124</th><td>		WARN_ON(signal_pending(current));</td></tr>
<tr><th id="2125">2125</th><td>		<b>if</b> (rcu_nocb_poll) {</td></tr>
<tr><th id="2126">2126</th><td>			schedule_timeout_interruptible(<var>1</var>);</td></tr>
<tr><th id="2127">2127</th><td>		} <b>else</b> {</td></tr>
<tr><th id="2128">2128</th><td>			trace_rcu_nocb_wake(my_rdp-&gt;rsp-&gt;name, my_rdp-&gt;cpu,</td></tr>
<tr><th id="2129">2129</th><td>					    TPS(<q>"WokeEmpty"</q>));</td></tr>
<tr><th id="2130">2130</th><td>		}</td></tr>
<tr><th id="2131">2131</th><td>		<b>goto</b> wait_again;</td></tr>
<tr><th id="2132">2132</th><td>	}</td></tr>
<tr><th id="2133">2133</th><td></td></tr>
<tr><th id="2134">2134</th><td>	<i>/* Wait for one grace period. */</i></td></tr>
<tr><th id="2135">2135</th><td>	rcu_nocb_wait_gp(my_rdp);</td></tr>
<tr><th id="2136">2136</th><td></td></tr>
<tr><th id="2137">2137</th><td>	<i>/* Each pass through the following loop wakes a follower, if needed. */</i></td></tr>
<tr><th id="2138">2138</th><td>	<b>for</b> (rdp = my_rdp; rdp; rdp = rdp-&gt;nocb_next_follower) {</td></tr>
<tr><th id="2139">2139</th><td>		<b>if</b> (!rcu_nocb_poll &amp;&amp;</td></tr>
<tr><th id="2140">2140</th><td>		    READ_ONCE(rdp-&gt;nocb_head) &amp;&amp;</td></tr>
<tr><th id="2141">2141</th><td>		    READ_ONCE(my_rdp-&gt;nocb_leader_sleep)) {</td></tr>
<tr><th id="2142">2142</th><td>			raw_spin_lock_irqsave(&amp;my_rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="2143">2143</th><td>			my_rdp-&gt;nocb_leader_sleep = false;<i>/* No need to sleep.*/</i></td></tr>
<tr><th id="2144">2144</th><td>			raw_spin_unlock_irqrestore(&amp;my_rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="2145">2145</th><td>		}</td></tr>
<tr><th id="2146">2146</th><td>		<b>if</b> (!rdp-&gt;nocb_gp_head)</td></tr>
<tr><th id="2147">2147</th><td>			<b>continue</b>; <i>/* No CBs, so no need to wake follower. */</i></td></tr>
<tr><th id="2148">2148</th><td></td></tr>
<tr><th id="2149">2149</th><td>		<i>/* Append callbacks to follower's "done" list. */</i></td></tr>
<tr><th id="2150">2150</th><td>		raw_spin_lock_irqsave(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="2151">2151</th><td>		tail = rdp-&gt;nocb_follower_tail;</td></tr>
<tr><th id="2152">2152</th><td>		rdp-&gt;nocb_follower_tail = rdp-&gt;nocb_gp_tail;</td></tr>
<tr><th id="2153">2153</th><td>		*tail = rdp-&gt;nocb_gp_head;</td></tr>
<tr><th id="2154">2154</th><td>		raw_spin_unlock_irqrestore(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="2155">2155</th><td>		<b>if</b> (rdp != my_rdp &amp;&amp; tail == &amp;rdp-&gt;nocb_follower_head) {</td></tr>
<tr><th id="2156">2156</th><td>			<i>/* List was empty, so wake up the follower.  */</i></td></tr>
<tr><th id="2157">2157</th><td>			swake_up(&amp;rdp-&gt;nocb_wq);</td></tr>
<tr><th id="2158">2158</th><td>		}</td></tr>
<tr><th id="2159">2159</th><td>	}</td></tr>
<tr><th id="2160">2160</th><td></td></tr>
<tr><th id="2161">2161</th><td>	<i>/* If we (the leader) don't have CBs, go wait some more. */</i></td></tr>
<tr><th id="2162">2162</th><td>	<b>if</b> (!my_rdp-&gt;nocb_follower_head)</td></tr>
<tr><th id="2163">2163</th><td>		<b>goto</b> wait_again;</td></tr>
<tr><th id="2164">2164</th><td>}</td></tr>
<tr><th id="2165">2165</th><td></td></tr>
<tr><th id="2166">2166</th><td><i>/*</i></td></tr>
<tr><th id="2167">2167</th><td><i> * Followers come here to wait for additional callbacks to show up.</i></td></tr>
<tr><th id="2168">2168</th><td><i> * This function does not return until callbacks appear.</i></td></tr>
<tr><th id="2169">2169</th><td><i> */</i></td></tr>
<tr><th id="2170">2170</th><td><em>static</em> <em>void</em> nocb_follower_wait(<b>struct</b> rcu_data *rdp)</td></tr>
<tr><th id="2171">2171</th><td>{</td></tr>
<tr><th id="2172">2172</th><td>	<b>for</b> (;;) {</td></tr>
<tr><th id="2173">2173</th><td>		trace_rcu_nocb_wake(rdp-&gt;rsp-&gt;name, rdp-&gt;cpu, TPS(<q>"FollowerSleep"</q>));</td></tr>
<tr><th id="2174">2174</th><td>		swait_event_interruptible(rdp-&gt;nocb_wq,</td></tr>
<tr><th id="2175">2175</th><td>					 READ_ONCE(rdp-&gt;nocb_follower_head));</td></tr>
<tr><th id="2176">2176</th><td>		<b>if</b> (smp_load_acquire(&amp;rdp-&gt;nocb_follower_head)) {</td></tr>
<tr><th id="2177">2177</th><td>			<i>/* ^^^ Ensure CB invocation follows _head test. */</i></td></tr>
<tr><th id="2178">2178</th><td>			<b>return</b>;</td></tr>
<tr><th id="2179">2179</th><td>		}</td></tr>
<tr><th id="2180">2180</th><td>		WARN_ON(signal_pending(current));</td></tr>
<tr><th id="2181">2181</th><td>		trace_rcu_nocb_wake(rdp-&gt;rsp-&gt;name, rdp-&gt;cpu, TPS(<q>"WokeEmpty"</q>));</td></tr>
<tr><th id="2182">2182</th><td>	}</td></tr>
<tr><th id="2183">2183</th><td>}</td></tr>
<tr><th id="2184">2184</th><td></td></tr>
<tr><th id="2185">2185</th><td><i>/*</i></td></tr>
<tr><th id="2186">2186</th><td><i> * Per-rcu_data kthread, but only for no-CBs CPUs.  Each kthread invokes</i></td></tr>
<tr><th id="2187">2187</th><td><i> * callbacks queued by the corresponding no-CBs CPU, however, there is</i></td></tr>
<tr><th id="2188">2188</th><td><i> * an optional leader-follower relationship so that the grace-period</i></td></tr>
<tr><th id="2189">2189</th><td><i> * kthreads don't have to do quite so many wakeups.</i></td></tr>
<tr><th id="2190">2190</th><td><i> */</i></td></tr>
<tr><th id="2191">2191</th><td><em>static</em> <em>int</em> rcu_nocb_kthread(<em>void</em> *arg)</td></tr>
<tr><th id="2192">2192</th><td>{</td></tr>
<tr><th id="2193">2193</th><td>	<em>int</em> c, cl;</td></tr>
<tr><th id="2194">2194</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="2195">2195</th><td>	<b>struct</b> rcu_head *list;</td></tr>
<tr><th id="2196">2196</th><td>	<b>struct</b> rcu_head *next;</td></tr>
<tr><th id="2197">2197</th><td>	<b>struct</b> rcu_head **tail;</td></tr>
<tr><th id="2198">2198</th><td>	<b>struct</b> rcu_data *rdp = arg;</td></tr>
<tr><th id="2199">2199</th><td></td></tr>
<tr><th id="2200">2200</th><td>	<i>/* Each pass through this loop invokes one batch of callbacks */</i></td></tr>
<tr><th id="2201">2201</th><td>	<b>for</b> (;;) {</td></tr>
<tr><th id="2202">2202</th><td>		<i>/* Wait for callbacks. */</i></td></tr>
<tr><th id="2203">2203</th><td>		<b>if</b> (rdp-&gt;nocb_leader == rdp)</td></tr>
<tr><th id="2204">2204</th><td>			nocb_leader_wait(rdp);</td></tr>
<tr><th id="2205">2205</th><td>		<b>else</b></td></tr>
<tr><th id="2206">2206</th><td>			nocb_follower_wait(rdp);</td></tr>
<tr><th id="2207">2207</th><td></td></tr>
<tr><th id="2208">2208</th><td>		<i>/* Pull the ready-to-invoke callbacks onto local list. */</i></td></tr>
<tr><th id="2209">2209</th><td>		raw_spin_lock_irqsave(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="2210">2210</th><td>		list = rdp-&gt;nocb_follower_head;</td></tr>
<tr><th id="2211">2211</th><td>		rdp-&gt;nocb_follower_head = NULL;</td></tr>
<tr><th id="2212">2212</th><td>		tail = rdp-&gt;nocb_follower_tail;</td></tr>
<tr><th id="2213">2213</th><td>		rdp-&gt;nocb_follower_tail = &amp;rdp-&gt;nocb_follower_head;</td></tr>
<tr><th id="2214">2214</th><td>		raw_spin_unlock_irqrestore(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="2215">2215</th><td>		BUG_ON(!list);</td></tr>
<tr><th id="2216">2216</th><td>		trace_rcu_nocb_wake(rdp-&gt;rsp-&gt;name, rdp-&gt;cpu, TPS(<q>"WokeNonEmpty"</q>));</td></tr>
<tr><th id="2217">2217</th><td></td></tr>
<tr><th id="2218">2218</th><td>		<i>/* Each pass through the following loop invokes a callback. */</i></td></tr>
<tr><th id="2219">2219</th><td>		trace_rcu_batch_start(rdp-&gt;rsp-&gt;name,</td></tr>
<tr><th id="2220">2220</th><td>				      atomic_long_read(&amp;rdp-&gt;nocb_q_count_lazy),</td></tr>
<tr><th id="2221">2221</th><td>				      atomic_long_read(&amp;rdp-&gt;nocb_q_count), -<var>1</var>);</td></tr>
<tr><th id="2222">2222</th><td>		c = cl = <var>0</var>;</td></tr>
<tr><th id="2223">2223</th><td>		<b>while</b> (list) {</td></tr>
<tr><th id="2224">2224</th><td>			next = list-&gt;next;</td></tr>
<tr><th id="2225">2225</th><td>			<i>/* Wait for enqueuing to complete, if needed. */</i></td></tr>
<tr><th id="2226">2226</th><td>			<b>while</b> (next == NULL &amp;&amp; &amp;list-&gt;next != tail) {</td></tr>
<tr><th id="2227">2227</th><td>				trace_rcu_nocb_wake(rdp-&gt;rsp-&gt;name, rdp-&gt;cpu,</td></tr>
<tr><th id="2228">2228</th><td>						    TPS(<q>"WaitQueue"</q>));</td></tr>
<tr><th id="2229">2229</th><td>				schedule_timeout_interruptible(<var>1</var>);</td></tr>
<tr><th id="2230">2230</th><td>				trace_rcu_nocb_wake(rdp-&gt;rsp-&gt;name, rdp-&gt;cpu,</td></tr>
<tr><th id="2231">2231</th><td>						    TPS(<q>"WokeQueue"</q>));</td></tr>
<tr><th id="2232">2232</th><td>				next = list-&gt;next;</td></tr>
<tr><th id="2233">2233</th><td>			}</td></tr>
<tr><th id="2234">2234</th><td>			debug_rcu_head_unqueue(list);</td></tr>
<tr><th id="2235">2235</th><td>			local_bh_disable();</td></tr>
<tr><th id="2236">2236</th><td>			<b>if</b> (__rcu_reclaim(rdp-&gt;rsp-&gt;name, list))</td></tr>
<tr><th id="2237">2237</th><td>				cl++;</td></tr>
<tr><th id="2238">2238</th><td>			c++;</td></tr>
<tr><th id="2239">2239</th><td>			local_bh_enable();</td></tr>
<tr><th id="2240">2240</th><td>			cond_resched_rcu_qs();</td></tr>
<tr><th id="2241">2241</th><td>			list = next;</td></tr>
<tr><th id="2242">2242</th><td>		}</td></tr>
<tr><th id="2243">2243</th><td>		trace_rcu_batch_end(rdp-&gt;rsp-&gt;name, c, !!list, <var>0</var>, <var>0</var>, <var>1</var>);</td></tr>
<tr><th id="2244">2244</th><td>		smp_mb__before_atomic();  <i>/* _add after CB invocation. */</i></td></tr>
<tr><th id="2245">2245</th><td>		atomic_long_add(-c, &amp;rdp-&gt;nocb_q_count);</td></tr>
<tr><th id="2246">2246</th><td>		atomic_long_add(-cl, &amp;rdp-&gt;nocb_q_count_lazy);</td></tr>
<tr><th id="2247">2247</th><td>		rdp-&gt;n_nocbs_invoked += c;</td></tr>
<tr><th id="2248">2248</th><td>	}</td></tr>
<tr><th id="2249">2249</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="2250">2250</th><td>}</td></tr>
<tr><th id="2251">2251</th><td></td></tr>
<tr><th id="2252">2252</th><td><i>/* Is a deferred wakeup of rcu_nocb_kthread() required? */</i></td></tr>
<tr><th id="2253">2253</th><td><em>static</em> <em>int</em> rcu_nocb_need_deferred_wakeup(<b>struct</b> rcu_data *rdp)</td></tr>
<tr><th id="2254">2254</th><td>{</td></tr>
<tr><th id="2255">2255</th><td>	<b>return</b> READ_ONCE(rdp-&gt;nocb_defer_wakeup);</td></tr>
<tr><th id="2256">2256</th><td>}</td></tr>
<tr><th id="2257">2257</th><td></td></tr>
<tr><th id="2258">2258</th><td><i>/* Do a deferred wakeup of rcu_nocb_kthread(). */</i></td></tr>
<tr><th id="2259">2259</th><td><em>static</em> <em>void</em> do_nocb_deferred_wakeup_common(<b>struct</b> rcu_data *rdp)</td></tr>
<tr><th id="2260">2260</th><td>{</td></tr>
<tr><th id="2261">2261</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="2262">2262</th><td>	<em>int</em> ndw;</td></tr>
<tr><th id="2263">2263</th><td></td></tr>
<tr><th id="2264">2264</th><td>	raw_spin_lock_irqsave(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="2265">2265</th><td>	<b>if</b> (!rcu_nocb_need_deferred_wakeup(rdp)) {</td></tr>
<tr><th id="2266">2266</th><td>		raw_spin_unlock_irqrestore(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="2267">2267</th><td>		<b>return</b>;</td></tr>
<tr><th id="2268">2268</th><td>	}</td></tr>
<tr><th id="2269">2269</th><td>	ndw = READ_ONCE(rdp-&gt;nocb_defer_wakeup);</td></tr>
<tr><th id="2270">2270</th><td>	WRITE_ONCE(rdp-&gt;nocb_defer_wakeup, RCU_NOCB_WAKE_NOT);</td></tr>
<tr><th id="2271">2271</th><td>	__wake_nocb_leader(rdp, ndw == RCU_NOCB_WAKE_FORCE, flags);</td></tr>
<tr><th id="2272">2272</th><td>	trace_rcu_nocb_wake(rdp-&gt;rsp-&gt;name, rdp-&gt;cpu, TPS(<q>"DeferredWake"</q>));</td></tr>
<tr><th id="2273">2273</th><td>}</td></tr>
<tr><th id="2274">2274</th><td></td></tr>
<tr><th id="2275">2275</th><td><i>/* Do a deferred wakeup of rcu_nocb_kthread() from a timer handler. */</i></td></tr>
<tr><th id="2276">2276</th><td><em>static</em> <em>void</em> do_nocb_deferred_wakeup_timer(<em>unsigned</em> <em>long</em> x)</td></tr>
<tr><th id="2277">2277</th><td>{</td></tr>
<tr><th id="2278">2278</th><td>	do_nocb_deferred_wakeup_common((<b>struct</b> rcu_data *)x);</td></tr>
<tr><th id="2279">2279</th><td>}</td></tr>
<tr><th id="2280">2280</th><td></td></tr>
<tr><th id="2281">2281</th><td><i>/*</i></td></tr>
<tr><th id="2282">2282</th><td><i> * Do a deferred wakeup of rcu_nocb_kthread() from fastpath.</i></td></tr>
<tr><th id="2283">2283</th><td><i> * This means we do an inexact common-case check.  Note that if</i></td></tr>
<tr><th id="2284">2284</th><td><i> * we miss, -&gt;nocb_timer will eventually clean things up.</i></td></tr>
<tr><th id="2285">2285</th><td><i> */</i></td></tr>
<tr><th id="2286">2286</th><td><em>static</em> <em>void</em> do_nocb_deferred_wakeup(<b>struct</b> rcu_data *rdp)</td></tr>
<tr><th id="2287">2287</th><td>{</td></tr>
<tr><th id="2288">2288</th><td>	<b>if</b> (rcu_nocb_need_deferred_wakeup(rdp))</td></tr>
<tr><th id="2289">2289</th><td>		do_nocb_deferred_wakeup_common(rdp);</td></tr>
<tr><th id="2290">2290</th><td>}</td></tr>
<tr><th id="2291">2291</th><td></td></tr>
<tr><th id="2292">2292</th><td><em>void</em> __init rcu_init_nohz(<em>void</em>)</td></tr>
<tr><th id="2293">2293</th><td>{</td></tr>
<tr><th id="2294">2294</th><td>	<em>int</em> cpu;</td></tr>
<tr><th id="2295">2295</th><td>	bool need_rcu_nocb_mask = true;</td></tr>
<tr><th id="2296">2296</th><td>	<b>struct</b> rcu_state *rsp;</td></tr>
<tr><th id="2297">2297</th><td></td></tr>
<tr><th id="2298">2298</th><td><u>#if defined(CONFIG_NO_HZ_FULL)</u></td></tr>
<tr><th id="2299">2299</th><td>	<b>if</b> (tick_nohz_full_running &amp;&amp; cpumask_weight(tick_nohz_full_mask))</td></tr>
<tr><th id="2300">2300</th><td>		need_rcu_nocb_mask = true;</td></tr>
<tr><th id="2301">2301</th><td><u>#endif /* #if defined(CONFIG_NO_HZ_FULL) */</u></td></tr>
<tr><th id="2302">2302</th><td></td></tr>
<tr><th id="2303">2303</th><td>	<b>if</b> (!have_rcu_nocb_mask &amp;&amp; need_rcu_nocb_mask) {</td></tr>
<tr><th id="2304">2304</th><td>		<b>if</b> (!zalloc_cpumask_var(&amp;rcu_nocb_mask, GFP_KERNEL)) {</td></tr>
<tr><th id="2305">2305</th><td>			pr_info(<q>"rcu_nocb_mask allocation failed, callback offloading disabled.\n"</q>);</td></tr>
<tr><th id="2306">2306</th><td>			<b>return</b>;</td></tr>
<tr><th id="2307">2307</th><td>		}</td></tr>
<tr><th id="2308">2308</th><td>		have_rcu_nocb_mask = true;</td></tr>
<tr><th id="2309">2309</th><td>	}</td></tr>
<tr><th id="2310">2310</th><td>	<b>if</b> (!have_rcu_nocb_mask)</td></tr>
<tr><th id="2311">2311</th><td>		<b>return</b>;</td></tr>
<tr><th id="2312">2312</th><td></td></tr>
<tr><th id="2313">2313</th><td><u>#if defined(CONFIG_NO_HZ_FULL)</u></td></tr>
<tr><th id="2314">2314</th><td>	<b>if</b> (tick_nohz_full_running)</td></tr>
<tr><th id="2315">2315</th><td>		cpumask_or(rcu_nocb_mask, rcu_nocb_mask, tick_nohz_full_mask);</td></tr>
<tr><th id="2316">2316</th><td><u>#endif /* #if defined(CONFIG_NO_HZ_FULL) */</u></td></tr>
<tr><th id="2317">2317</th><td></td></tr>
<tr><th id="2318">2318</th><td>	<b>if</b> (!cpumask_subset(rcu_nocb_mask, cpu_possible_mask)) {</td></tr>
<tr><th id="2319">2319</th><td>		pr_info(<q>"\tNote: kernel parameter 'rcu_nocbs=' contains nonexistent CPUs.\n"</q>);</td></tr>
<tr><th id="2320">2320</th><td>		cpumask_and(rcu_nocb_mask, cpu_possible_mask,</td></tr>
<tr><th id="2321">2321</th><td>			    rcu_nocb_mask);</td></tr>
<tr><th id="2322">2322</th><td>	}</td></tr>
<tr><th id="2323">2323</th><td>	pr_info(<q>"\tOffload RCU callbacks from CPUs: %*pbl.\n"</q>,</td></tr>
<tr><th id="2324">2324</th><td>		cpumask_pr_args(rcu_nocb_mask));</td></tr>
<tr><th id="2325">2325</th><td>	<b>if</b> (rcu_nocb_poll)</td></tr>
<tr><th id="2326">2326</th><td>		pr_info(<q>"\tPoll for callbacks from no-CBs CPUs.\n"</q>);</td></tr>
<tr><th id="2327">2327</th><td></td></tr>
<tr><th id="2328">2328</th><td>	for_each_rcu_flavor(rsp) {</td></tr>
<tr><th id="2329">2329</th><td>		for_each_cpu(cpu, rcu_nocb_mask)</td></tr>
<tr><th id="2330">2330</th><td>			init_nocb_callback_list(per_cpu_ptr(rsp-&gt;rda, cpu));</td></tr>
<tr><th id="2331">2331</th><td>		rcu_organize_nocb_kthreads(rsp);</td></tr>
<tr><th id="2332">2332</th><td>	}</td></tr>
<tr><th id="2333">2333</th><td>}</td></tr>
<tr><th id="2334">2334</th><td></td></tr>
<tr><th id="2335">2335</th><td><i>/* Initialize per-rcu_data variables for no-CBs CPUs. */</i></td></tr>
<tr><th id="2336">2336</th><td><em>static</em> <em>void</em> __init rcu_boot_init_nocb_percpu_data(<b>struct</b> rcu_data *rdp)</td></tr>
<tr><th id="2337">2337</th><td>{</td></tr>
<tr><th id="2338">2338</th><td>	rdp-&gt;nocb_tail = &amp;rdp-&gt;nocb_head;</td></tr>
<tr><th id="2339">2339</th><td>	init_swait_queue_head(&amp;rdp-&gt;nocb_wq);</td></tr>
<tr><th id="2340">2340</th><td>	rdp-&gt;nocb_follower_tail = &amp;rdp-&gt;nocb_follower_head;</td></tr>
<tr><th id="2341">2341</th><td>	raw_spin_lock_init(&amp;rdp-&gt;nocb_lock);</td></tr>
<tr><th id="2342">2342</th><td>	setup_timer(&amp;rdp-&gt;nocb_timer, do_nocb_deferred_wakeup_timer,</td></tr>
<tr><th id="2343">2343</th><td>		    (<em>unsigned</em> <em>long</em>)rdp);</td></tr>
<tr><th id="2344">2344</th><td>}</td></tr>
<tr><th id="2345">2345</th><td></td></tr>
<tr><th id="2346">2346</th><td><i>/*</i></td></tr>
<tr><th id="2347">2347</th><td><i> * If the specified CPU is a no-CBs CPU that does not already have its</i></td></tr>
<tr><th id="2348">2348</th><td><i> * rcuo kthread for the specified RCU flavor, spawn it.  If the CPUs are</i></td></tr>
<tr><th id="2349">2349</th><td><i> * brought online out of order, this can require re-organizing the</i></td></tr>
<tr><th id="2350">2350</th><td><i> * leader-follower relationships.</i></td></tr>
<tr><th id="2351">2351</th><td><i> */</i></td></tr>
<tr><th id="2352">2352</th><td><em>static</em> <em>void</em> rcu_spawn_one_nocb_kthread(<b>struct</b> rcu_state *rsp, <em>int</em> cpu)</td></tr>
<tr><th id="2353">2353</th><td>{</td></tr>
<tr><th id="2354">2354</th><td>	<b>struct</b> rcu_data *rdp;</td></tr>
<tr><th id="2355">2355</th><td>	<b>struct</b> rcu_data *rdp_last;</td></tr>
<tr><th id="2356">2356</th><td>	<b>struct</b> rcu_data *rdp_old_leader;</td></tr>
<tr><th id="2357">2357</th><td>	<b>struct</b> rcu_data *rdp_spawn = per_cpu_ptr(rsp-&gt;rda, cpu);</td></tr>
<tr><th id="2358">2358</th><td>	<b>struct</b> task_struct *t;</td></tr>
<tr><th id="2359">2359</th><td></td></tr>
<tr><th id="2360">2360</th><td>	<i>/*</i></td></tr>
<tr><th id="2361">2361</th><td><i>	 * If this isn't a no-CBs CPU or if it already has an rcuo kthread,</i></td></tr>
<tr><th id="2362">2362</th><td><i>	 * then nothing to do.</i></td></tr>
<tr><th id="2363">2363</th><td><i>	 */</i></td></tr>
<tr><th id="2364">2364</th><td>	<b>if</b> (!rcu_is_nocb_cpu(cpu) || rdp_spawn-&gt;nocb_kthread)</td></tr>
<tr><th id="2365">2365</th><td>		<b>return</b>;</td></tr>
<tr><th id="2366">2366</th><td></td></tr>
<tr><th id="2367">2367</th><td>	<i>/* If we didn't spawn the leader first, reorganize! */</i></td></tr>
<tr><th id="2368">2368</th><td>	rdp_old_leader = rdp_spawn-&gt;nocb_leader;</td></tr>
<tr><th id="2369">2369</th><td>	<b>if</b> (rdp_old_leader != rdp_spawn &amp;&amp; !rdp_old_leader-&gt;nocb_kthread) {</td></tr>
<tr><th id="2370">2370</th><td>		rdp_last = NULL;</td></tr>
<tr><th id="2371">2371</th><td>		rdp = rdp_old_leader;</td></tr>
<tr><th id="2372">2372</th><td>		<b>do</b> {</td></tr>
<tr><th id="2373">2373</th><td>			rdp-&gt;nocb_leader = rdp_spawn;</td></tr>
<tr><th id="2374">2374</th><td>			<b>if</b> (rdp_last &amp;&amp; rdp != rdp_spawn)</td></tr>
<tr><th id="2375">2375</th><td>				rdp_last-&gt;nocb_next_follower = rdp;</td></tr>
<tr><th id="2376">2376</th><td>			<b>if</b> (rdp == rdp_spawn) {</td></tr>
<tr><th id="2377">2377</th><td>				rdp = rdp-&gt;nocb_next_follower;</td></tr>
<tr><th id="2378">2378</th><td>			} <b>else</b> {</td></tr>
<tr><th id="2379">2379</th><td>				rdp_last = rdp;</td></tr>
<tr><th id="2380">2380</th><td>				rdp = rdp-&gt;nocb_next_follower;</td></tr>
<tr><th id="2381">2381</th><td>				rdp_last-&gt;nocb_next_follower = NULL;</td></tr>
<tr><th id="2382">2382</th><td>			}</td></tr>
<tr><th id="2383">2383</th><td>		} <b>while</b> (rdp);</td></tr>
<tr><th id="2384">2384</th><td>		rdp_spawn-&gt;nocb_next_follower = rdp_old_leader;</td></tr>
<tr><th id="2385">2385</th><td>	}</td></tr>
<tr><th id="2386">2386</th><td></td></tr>
<tr><th id="2387">2387</th><td>	<i>/* Spawn the kthread for this CPU and RCU flavor. */</i></td></tr>
<tr><th id="2388">2388</th><td>	t = kthread_run(rcu_nocb_kthread, rdp_spawn,</td></tr>
<tr><th id="2389">2389</th><td>			<q>"rcuo%c/%d"</q>, rsp-&gt;abbr, cpu);</td></tr>
<tr><th id="2390">2390</th><td>	BUG_ON(IS_ERR(t));</td></tr>
<tr><th id="2391">2391</th><td>	WRITE_ONCE(rdp_spawn-&gt;nocb_kthread, t);</td></tr>
<tr><th id="2392">2392</th><td>}</td></tr>
<tr><th id="2393">2393</th><td></td></tr>
<tr><th id="2394">2394</th><td><i>/*</i></td></tr>
<tr><th id="2395">2395</th><td><i> * If the specified CPU is a no-CBs CPU that does not already have its</i></td></tr>
<tr><th id="2396">2396</th><td><i> * rcuo kthreads, spawn them.</i></td></tr>
<tr><th id="2397">2397</th><td><i> */</i></td></tr>
<tr><th id="2398">2398</th><td><em>static</em> <em>void</em> rcu_spawn_all_nocb_kthreads(<em>int</em> cpu)</td></tr>
<tr><th id="2399">2399</th><td>{</td></tr>
<tr><th id="2400">2400</th><td>	<b>struct</b> rcu_state *rsp;</td></tr>
<tr><th id="2401">2401</th><td></td></tr>
<tr><th id="2402">2402</th><td>	<b>if</b> (rcu_scheduler_fully_active)</td></tr>
<tr><th id="2403">2403</th><td>		for_each_rcu_flavor(rsp)</td></tr>
<tr><th id="2404">2404</th><td>			rcu_spawn_one_nocb_kthread(rsp, cpu);</td></tr>
<tr><th id="2405">2405</th><td>}</td></tr>
<tr><th id="2406">2406</th><td></td></tr>
<tr><th id="2407">2407</th><td><i>/*</i></td></tr>
<tr><th id="2408">2408</th><td><i> * Once the scheduler is running, spawn rcuo kthreads for all online</i></td></tr>
<tr><th id="2409">2409</th><td><i> * no-CBs CPUs.  This assumes that the early_initcall()s happen before</i></td></tr>
<tr><th id="2410">2410</th><td><i> * non-boot CPUs come online -- if this changes, we will need to add</i></td></tr>
<tr><th id="2411">2411</th><td><i> * some mutual exclusion.</i></td></tr>
<tr><th id="2412">2412</th><td><i> */</i></td></tr>
<tr><th id="2413">2413</th><td><em>static</em> <em>void</em> __init rcu_spawn_nocb_kthreads(<em>void</em>)</td></tr>
<tr><th id="2414">2414</th><td>{</td></tr>
<tr><th id="2415">2415</th><td>	<em>int</em> cpu;</td></tr>
<tr><th id="2416">2416</th><td></td></tr>
<tr><th id="2417">2417</th><td>	for_each_online_cpu(cpu)</td></tr>
<tr><th id="2418">2418</th><td>		rcu_spawn_all_nocb_kthreads(cpu);</td></tr>
<tr><th id="2419">2419</th><td>}</td></tr>
<tr><th id="2420">2420</th><td></td></tr>
<tr><th id="2421">2421</th><td><i>/* How many follower CPU IDs per leader?  Default of -1 for sqrt(nr_cpu_ids). */</i></td></tr>
<tr><th id="2422">2422</th><td><em>static</em> <em>int</em> rcu_nocb_leader_stride = -<var>1</var>;</td></tr>
<tr><th id="2423">2423</th><td>module_param(rcu_nocb_leader_stride, <em>int</em>, <var>0444</var>);</td></tr>
<tr><th id="2424">2424</th><td></td></tr>
<tr><th id="2425">2425</th><td><i>/*</i></td></tr>
<tr><th id="2426">2426</th><td><i> * Initialize leader-follower relationships for all no-CBs CPU.</i></td></tr>
<tr><th id="2427">2427</th><td><i> */</i></td></tr>
<tr><th id="2428">2428</th><td><em>static</em> <em>void</em> __init rcu_organize_nocb_kthreads(<b>struct</b> rcu_state *rsp)</td></tr>
<tr><th id="2429">2429</th><td>{</td></tr>
<tr><th id="2430">2430</th><td>	<em>int</em> cpu;</td></tr>
<tr><th id="2431">2431</th><td>	<em>int</em> ls = rcu_nocb_leader_stride;</td></tr>
<tr><th id="2432">2432</th><td>	<em>int</em> nl = <var>0</var>;  <i>/* Next leader. */</i></td></tr>
<tr><th id="2433">2433</th><td>	<b>struct</b> rcu_data *rdp;</td></tr>
<tr><th id="2434">2434</th><td>	<b>struct</b> rcu_data *rdp_leader = NULL;  <i>/* Suppress misguided gcc warn. */</i></td></tr>
<tr><th id="2435">2435</th><td>	<b>struct</b> rcu_data *rdp_prev = NULL;</td></tr>
<tr><th id="2436">2436</th><td></td></tr>
<tr><th id="2437">2437</th><td>	<b>if</b> (!have_rcu_nocb_mask)</td></tr>
<tr><th id="2438">2438</th><td>		<b>return</b>;</td></tr>
<tr><th id="2439">2439</th><td>	<b>if</b> (ls == -<var>1</var>) {</td></tr>
<tr><th id="2440">2440</th><td>		ls = int_sqrt(nr_cpu_ids);</td></tr>
<tr><th id="2441">2441</th><td>		rcu_nocb_leader_stride = ls;</td></tr>
<tr><th id="2442">2442</th><td>	}</td></tr>
<tr><th id="2443">2443</th><td></td></tr>
<tr><th id="2444">2444</th><td>	<i>/*</i></td></tr>
<tr><th id="2445">2445</th><td><i>	 * Each pass through this loop sets up one rcu_data structure.</i></td></tr>
<tr><th id="2446">2446</th><td><i>	 * Should the corresponding CPU come online in the future, then</i></td></tr>
<tr><th id="2447">2447</th><td><i>	 * we will spawn the needed set of rcu_nocb_kthread() kthreads.</i></td></tr>
<tr><th id="2448">2448</th><td><i>	 */</i></td></tr>
<tr><th id="2449">2449</th><td>	for_each_cpu(cpu, rcu_nocb_mask) {</td></tr>
<tr><th id="2450">2450</th><td>		rdp = per_cpu_ptr(rsp-&gt;rda, cpu);</td></tr>
<tr><th id="2451">2451</th><td>		<b>if</b> (rdp-&gt;cpu &gt;= nl) {</td></tr>
<tr><th id="2452">2452</th><td>			<i>/* New leader, set up for followers &amp; next leader. */</i></td></tr>
<tr><th id="2453">2453</th><td>			nl = DIV_ROUND_UP(rdp-&gt;cpu + <var>1</var>, ls) * ls;</td></tr>
<tr><th id="2454">2454</th><td>			rdp-&gt;nocb_leader = rdp;</td></tr>
<tr><th id="2455">2455</th><td>			rdp_leader = rdp;</td></tr>
<tr><th id="2456">2456</th><td>		} <b>else</b> {</td></tr>
<tr><th id="2457">2457</th><td>			<i>/* Another follower, link to previous leader. */</i></td></tr>
<tr><th id="2458">2458</th><td>			rdp-&gt;nocb_leader = rdp_leader;</td></tr>
<tr><th id="2459">2459</th><td>			rdp_prev-&gt;nocb_next_follower = rdp;</td></tr>
<tr><th id="2460">2460</th><td>		}</td></tr>
<tr><th id="2461">2461</th><td>		rdp_prev = rdp;</td></tr>
<tr><th id="2462">2462</th><td>	}</td></tr>
<tr><th id="2463">2463</th><td>}</td></tr>
<tr><th id="2464">2464</th><td></td></tr>
<tr><th id="2465">2465</th><td><i>/* Prevent __call_rcu() from enqueuing callbacks on no-CBs CPUs */</i></td></tr>
<tr><th id="2466">2466</th><td><em>static</em> bool init_nocb_callback_list(<b>struct</b> rcu_data *rdp)</td></tr>
<tr><th id="2467">2467</th><td>{</td></tr>
<tr><th id="2468">2468</th><td>	<b>if</b> (!rcu_is_nocb_cpu(rdp-&gt;cpu))</td></tr>
<tr><th id="2469">2469</th><td>		<b>return</b> false;</td></tr>
<tr><th id="2470">2470</th><td></td></tr>
<tr><th id="2471">2471</th><td>	<i>/* If there are early-boot callbacks, move them to nocb lists. */</i></td></tr>
<tr><th id="2472">2472</th><td>	<b>if</b> (!rcu_segcblist_empty(&amp;rdp-&gt;cblist)) {</td></tr>
<tr><th id="2473">2473</th><td>		rdp-&gt;nocb_head = rcu_segcblist_head(&amp;rdp-&gt;cblist);</td></tr>
<tr><th id="2474">2474</th><td>		rdp-&gt;nocb_tail = rcu_segcblist_tail(&amp;rdp-&gt;cblist);</td></tr>
<tr><th id="2475">2475</th><td>		atomic_long_set(&amp;rdp-&gt;nocb_q_count,</td></tr>
<tr><th id="2476">2476</th><td>				rcu_segcblist_n_cbs(&amp;rdp-&gt;cblist));</td></tr>
<tr><th id="2477">2477</th><td>		atomic_long_set(&amp;rdp-&gt;nocb_q_count_lazy,</td></tr>
<tr><th id="2478">2478</th><td>				rcu_segcblist_n_lazy_cbs(&amp;rdp-&gt;cblist));</td></tr>
<tr><th id="2479">2479</th><td>		rcu_segcblist_init(&amp;rdp-&gt;cblist);</td></tr>
<tr><th id="2480">2480</th><td>	}</td></tr>
<tr><th id="2481">2481</th><td>	rcu_segcblist_disable(&amp;rdp-&gt;cblist);</td></tr>
<tr><th id="2482">2482</th><td>	<b>return</b> true;</td></tr>
<tr><th id="2483">2483</th><td>}</td></tr>
<tr><th id="2484">2484</th><td></td></tr>
<tr><th id="2485">2485</th><td><u>#<span data-ppcond="1735">else</span> /* #ifdef CONFIG_RCU_NOCB_CPU */</u></td></tr>
<tr><th id="2486">2486</th><td></td></tr>
<tr><th id="2487">2487</th><td><em>static</em> <a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool">bool</a> <dfn class="decl def fn" id="rcu_nocb_cpu_needs_barrier" title='rcu_nocb_cpu_needs_barrier' data-ref="rcu_nocb_cpu_needs_barrier">rcu_nocb_cpu_needs_barrier</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_state" title='rcu_state' data-ref="rcu_state">rcu_state</a> *<dfn class="local col6 decl" id="576rsp" title='rsp' data-type='struct rcu_state *' data-ref="576rsp">rsp</dfn>, <em>int</em> <dfn class="local col7 decl" id="577cpu" title='cpu' data-type='int' data-ref="577cpu">cpu</dfn>)</td></tr>
<tr><th id="2488">2488</th><td>{</td></tr>
<tr><th id="2489">2489</th><td>	<a class="macro" href="../../include/asm-generic/bug.h.html#66" title="({ int __ret_warn_on = !!(1); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { asm volatile(&quot;1:\t&quot; &quot;.byte 0x0f, 0x0b&quot; &quot;\n&quot; &quot;.pushsection __bug_table,\&quot;aw\&quot;\n&quot; &quot;2:\t&quot; &quot;.long &quot; &quot;1b&quot; &quot; - 2b&quot; &quot;\t# bug_entry::bug_addr\n&quot; &quot;\t&quot; &quot;.long &quot; &quot;%c0&quot; &quot; - 2b&quot; &quot;\t# bug_entry::file\n&quot; &quot;\t.word %c1&quot; &quot;\t# bug_entry::line\n&quot; &quot;\t.word %c2&quot; &quot;\t# bug_entry::flags\n&quot; &quot;\t.org 2b+%c3\n&quot; &quot;.popsection&quot; : : &quot;i&quot; (&quot;/home/tempdban/kernel/stable/kernel/rcu/tree_plugin.h&quot;), &quot;i&quot; (2489), &quot;i&quot; ((1 &lt;&lt; 0)|((1 &lt;&lt; 1)|((9) &lt;&lt; 8))), &quot;i&quot; (sizeof(struct bug_entry))); } while (0); ({ asm(&quot;%c0:\n\t&quot; &quot;.pushsection .discard.reachable\n\t&quot; &quot;.long %c0b - .\n\t&quot; &quot;.popsection\n\t&quot; : : &quot;i&quot; (182)); }); } while (0); __builtin_expect(!!(__ret_warn_on), 0); })" data-ref="_M/WARN_ON_ONCE">WARN_ON_ONCE</a>(<var>1</var>); <i>/* Should be dead code. */</i></td></tr>
<tr><th id="2490">2490</th><td>	<b>return</b> <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false">false</a>;</td></tr>
<tr><th id="2491">2491</th><td>}</td></tr>
<tr><th id="2492">2492</th><td></td></tr>
<tr><th id="2493">2493</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_nocb_gp_cleanup" title='rcu_nocb_gp_cleanup' data-ref="rcu_nocb_gp_cleanup">rcu_nocb_gp_cleanup</dfn>(<b>struct</b> <a class="type" href="../../include/linux/swait.h.html#swait_queue_head" title='swait_queue_head' data-ref="swait_queue_head">swait_queue_head</a> *<dfn class="local col8 decl" id="578sq" title='sq' data-type='struct swait_queue_head *' data-ref="578sq">sq</dfn>)</td></tr>
<tr><th id="2494">2494</th><td>{</td></tr>
<tr><th id="2495">2495</th><td>}</td></tr>
<tr><th id="2496">2496</th><td></td></tr>
<tr><th id="2497">2497</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_nocb_gp_set" title='rcu_nocb_gp_set' data-ref="rcu_nocb_gp_set">rcu_nocb_gp_set</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node">rcu_node</a> *<dfn class="local col9 decl" id="579rnp" title='rnp' data-type='struct rcu_node *' data-ref="579rnp">rnp</dfn>, <em>int</em> <dfn class="local col0 decl" id="580nrq" title='nrq' data-type='int' data-ref="580nrq">nrq</dfn>)</td></tr>
<tr><th id="2498">2498</th><td>{</td></tr>
<tr><th id="2499">2499</th><td>}</td></tr>
<tr><th id="2500">2500</th><td></td></tr>
<tr><th id="2501">2501</th><td><em>static</em> <b>struct</b> <a class="type" href="../../include/linux/swait.h.html#swait_queue_head" title='swait_queue_head' data-ref="swait_queue_head">swait_queue_head</a> *<dfn class="decl def fn" id="rcu_nocb_gp_get" title='rcu_nocb_gp_get' data-ref="rcu_nocb_gp_get">rcu_nocb_gp_get</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node">rcu_node</a> *<dfn class="local col1 decl" id="581rnp" title='rnp' data-type='struct rcu_node *' data-ref="581rnp">rnp</dfn>)</td></tr>
<tr><th id="2502">2502</th><td>{</td></tr>
<tr><th id="2503">2503</th><td>	<b>return</b> <a class="macro" href="../../include/linux/stddef.h.html#8" title="((void *)0)" data-ref="_M/NULL">NULL</a>;</td></tr>
<tr><th id="2504">2504</th><td>}</td></tr>
<tr><th id="2505">2505</th><td></td></tr>
<tr><th id="2506">2506</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_init_one_nocb" title='rcu_init_one_nocb' data-ref="rcu_init_one_nocb">rcu_init_one_nocb</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node">rcu_node</a> *<dfn class="local col2 decl" id="582rnp" title='rnp' data-type='struct rcu_node *' data-ref="582rnp">rnp</dfn>)</td></tr>
<tr><th id="2507">2507</th><td>{</td></tr>
<tr><th id="2508">2508</th><td>}</td></tr>
<tr><th id="2509">2509</th><td></td></tr>
<tr><th id="2510">2510</th><td><em>static</em> <a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool">bool</a> <dfn class="decl def fn" id="__call_rcu_nocb" title='__call_rcu_nocb' data-ref="__call_rcu_nocb">__call_rcu_nocb</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_data" title='rcu_data' data-ref="rcu_data">rcu_data</a> *<dfn class="local col3 decl" id="583rdp" title='rdp' data-type='struct rcu_data *' data-ref="583rdp">rdp</dfn>, <b>struct</b> <a class="macro" href="../../include/linux/types.h.html#227" title="callback_head" data-ref="_M/rcu_head">rcu_head</a> *<dfn class="local col4 decl" id="584rhp" title='rhp' data-type='struct callback_head *' data-ref="584rhp">rhp</dfn>,</td></tr>
<tr><th id="2511">2511</th><td>			    <a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool">bool</a> <dfn class="local col5 decl" id="585lazy" title='lazy' data-type='bool' data-ref="585lazy">lazy</dfn>, <em>unsigned</em> <em>long</em> <dfn class="local col6 decl" id="586flags" title='flags' data-type='unsigned long' data-ref="586flags">flags</dfn>)</td></tr>
<tr><th id="2512">2512</th><td>{</td></tr>
<tr><th id="2513">2513</th><td>	<b>return</b> <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false">false</a>;</td></tr>
<tr><th id="2514">2514</th><td>}</td></tr>
<tr><th id="2515">2515</th><td></td></tr>
<tr><th id="2516">2516</th><td><em>static</em> <a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool">bool</a> <a class="macro" href="../../include/linux/compiler-gcc.h.html#147" title="__attribute__((unused))" data-ref="_M/__maybe_unused">__maybe_unused</a> <dfn class="decl def fn" id="rcu_nocb_adopt_orphan_cbs" title='rcu_nocb_adopt_orphan_cbs' data-ref="rcu_nocb_adopt_orphan_cbs">rcu_nocb_adopt_orphan_cbs</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_data" title='rcu_data' data-ref="rcu_data">rcu_data</a> *<dfn class="local col7 decl" id="587my_rdp" title='my_rdp' data-type='struct rcu_data *' data-ref="587my_rdp">my_rdp</dfn>,</td></tr>
<tr><th id="2517">2517</th><td>						     <b>struct</b> <a class="type" href="tree.h.html#rcu_data" title='rcu_data' data-ref="rcu_data">rcu_data</a> *<dfn class="local col8 decl" id="588rdp" title='rdp' data-type='struct rcu_data *' data-ref="588rdp">rdp</dfn>,</td></tr>
<tr><th id="2518">2518</th><td>						     <em>unsigned</em> <em>long</em> <dfn class="local col9 decl" id="589flags" title='flags' data-type='unsigned long' data-ref="589flags">flags</dfn>)</td></tr>
<tr><th id="2519">2519</th><td>{</td></tr>
<tr><th id="2520">2520</th><td>	<b>return</b> <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false">false</a>;</td></tr>
<tr><th id="2521">2521</th><td>}</td></tr>
<tr><th id="2522">2522</th><td></td></tr>
<tr><th id="2523">2523</th><td><em>static</em> <em>void</em> <a class="macro" href="../../include/linux/init.h.html#50" title="__attribute__ ((__section__(&quot;.init.text&quot;)))" data-ref="_M/__init">__init</a> <dfn class="decl def fn" id="rcu_boot_init_nocb_percpu_data" title='rcu_boot_init_nocb_percpu_data' data-ref="rcu_boot_init_nocb_percpu_data">rcu_boot_init_nocb_percpu_data</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_data" title='rcu_data' data-ref="rcu_data">rcu_data</a> *<dfn class="local col0 decl" id="590rdp" title='rdp' data-type='struct rcu_data *' data-ref="590rdp">rdp</dfn>)</td></tr>
<tr><th id="2524">2524</th><td>{</td></tr>
<tr><th id="2525">2525</th><td>}</td></tr>
<tr><th id="2526">2526</th><td></td></tr>
<tr><th id="2527">2527</th><td><em>static</em> <em>int</em> <dfn class="decl def fn" id="rcu_nocb_need_deferred_wakeup" title='rcu_nocb_need_deferred_wakeup' data-ref="rcu_nocb_need_deferred_wakeup">rcu_nocb_need_deferred_wakeup</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_data" title='rcu_data' data-ref="rcu_data">rcu_data</a> *<dfn class="local col1 decl" id="591rdp" title='rdp' data-type='struct rcu_data *' data-ref="591rdp">rdp</dfn>)</td></tr>
<tr><th id="2528">2528</th><td>{</td></tr>
<tr><th id="2529">2529</th><td>	<b>return</b> <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false">false</a>;</td></tr>
<tr><th id="2530">2530</th><td>}</td></tr>
<tr><th id="2531">2531</th><td></td></tr>
<tr><th id="2532">2532</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="do_nocb_deferred_wakeup" title='do_nocb_deferred_wakeup' data-ref="do_nocb_deferred_wakeup">do_nocb_deferred_wakeup</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_data" title='rcu_data' data-ref="rcu_data">rcu_data</a> *<dfn class="local col2 decl" id="592rdp" title='rdp' data-type='struct rcu_data *' data-ref="592rdp">rdp</dfn>)</td></tr>
<tr><th id="2533">2533</th><td>{</td></tr>
<tr><th id="2534">2534</th><td>}</td></tr>
<tr><th id="2535">2535</th><td></td></tr>
<tr><th id="2536">2536</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_spawn_all_nocb_kthreads" title='rcu_spawn_all_nocb_kthreads' data-ref="rcu_spawn_all_nocb_kthreads">rcu_spawn_all_nocb_kthreads</dfn>(<em>int</em> <dfn class="local col3 decl" id="593cpu" title='cpu' data-type='int' data-ref="593cpu">cpu</dfn>)</td></tr>
<tr><th id="2537">2537</th><td>{</td></tr>
<tr><th id="2538">2538</th><td>}</td></tr>
<tr><th id="2539">2539</th><td></td></tr>
<tr><th id="2540">2540</th><td><em>static</em> <em>void</em> <a class="macro" href="../../include/linux/init.h.html#50" title="__attribute__ ((__section__(&quot;.init.text&quot;)))" data-ref="_M/__init">__init</a> <dfn class="decl def fn" id="rcu_spawn_nocb_kthreads" title='rcu_spawn_nocb_kthreads' data-ref="rcu_spawn_nocb_kthreads">rcu_spawn_nocb_kthreads</dfn>(<em>void</em>)</td></tr>
<tr><th id="2541">2541</th><td>{</td></tr>
<tr><th id="2542">2542</th><td>}</td></tr>
<tr><th id="2543">2543</th><td></td></tr>
<tr><th id="2544">2544</th><td><em>static</em> <a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool">bool</a> <dfn class="decl def fn" id="init_nocb_callback_list" title='init_nocb_callback_list' data-ref="init_nocb_callback_list">init_nocb_callback_list</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_data" title='rcu_data' data-ref="rcu_data">rcu_data</a> *<dfn class="local col4 decl" id="594rdp" title='rdp' data-type='struct rcu_data *' data-ref="594rdp">rdp</dfn>)</td></tr>
<tr><th id="2545">2545</th><td>{</td></tr>
<tr><th id="2546">2546</th><td>	<b>return</b> <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false">false</a>;</td></tr>
<tr><th id="2547">2547</th><td>}</td></tr>
<tr><th id="2548">2548</th><td></td></tr>
<tr><th id="2549">2549</th><td><u>#<span data-ppcond="1735">endif</span> /* #else #ifdef CONFIG_RCU_NOCB_CPU */</u></td></tr>
<tr><th id="2550">2550</th><td></td></tr>
<tr><th id="2551">2551</th><td><i>/*</i></td></tr>
<tr><th id="2552">2552</th><td><i> * An adaptive-ticks CPU can potentially execute in kernel mode for an</i></td></tr>
<tr><th id="2553">2553</th><td><i> * arbitrarily long period of time with the scheduling-clock tick turned</i></td></tr>
<tr><th id="2554">2554</th><td><i> * off.  RCU will be paying attention to this CPU because it is in the</i></td></tr>
<tr><th id="2555">2555</th><td><i> * kernel, but the CPU cannot be guaranteed to be executing the RCU state</i></td></tr>
<tr><th id="2556">2556</th><td><i> * machine because the scheduling-clock tick has been disabled.  Therefore,</i></td></tr>
<tr><th id="2557">2557</th><td><i> * if an adaptive-ticks CPU is failing to respond to the current grace</i></td></tr>
<tr><th id="2558">2558</th><td><i> * period and has not be idle from an RCU perspective, kick it.</i></td></tr>
<tr><th id="2559">2559</th><td><i> */</i></td></tr>
<tr><th id="2560">2560</th><td><em>static</em> <em>void</em> <a class="macro" href="../../include/linux/compiler-gcc.h.html#147" title="__attribute__((unused))" data-ref="_M/__maybe_unused">__maybe_unused</a> <dfn class="decl def fn" id="rcu_kick_nohz_cpu" title='rcu_kick_nohz_cpu' data-ref="rcu_kick_nohz_cpu">rcu_kick_nohz_cpu</dfn>(<em>int</em> <dfn class="local col5 decl" id="595cpu" title='cpu' data-type='int' data-ref="595cpu">cpu</dfn>)</td></tr>
<tr><th id="2561">2561</th><td>{</td></tr>
<tr><th id="2562">2562</th><td><u>#<span data-ppcond="2562">ifdef</span> <span class="macro" data-ref="_M/CONFIG_NO_HZ_FULL">CONFIG_NO_HZ_FULL</span></u></td></tr>
<tr><th id="2563">2563</th><td>	<b>if</b> (tick_nohz_full_cpu(cpu))</td></tr>
<tr><th id="2564">2564</th><td>		smp_send_reschedule(cpu);</td></tr>
<tr><th id="2565">2565</th><td><u>#<span data-ppcond="2562">endif</span> /* #ifdef CONFIG_NO_HZ_FULL */</u></td></tr>
<tr><th id="2566">2566</th><td>}</td></tr>
<tr><th id="2567">2567</th><td></td></tr>
<tr><th id="2568">2568</th><td><i>/*</i></td></tr>
<tr><th id="2569">2569</th><td><i> * Is this CPU a NO_HZ_FULL CPU that should ignore RCU so that the</i></td></tr>
<tr><th id="2570">2570</th><td><i> * grace-period kthread will do force_quiescent_state() processing?</i></td></tr>
<tr><th id="2571">2571</th><td><i> * The idea is to avoid waking up RCU core processing on such a</i></td></tr>
<tr><th id="2572">2572</th><td><i> * CPU unless the grace period has extended for too long.</i></td></tr>
<tr><th id="2573">2573</th><td><i> *</i></td></tr>
<tr><th id="2574">2574</th><td><i> * This code relies on the fact that all NO_HZ_FULL CPUs are also</i></td></tr>
<tr><th id="2575">2575</th><td><i> * CONFIG_RCU_NOCB_CPU CPUs.</i></td></tr>
<tr><th id="2576">2576</th><td><i> */</i></td></tr>
<tr><th id="2577">2577</th><td><em>static</em> <a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool">bool</a> <dfn class="decl def fn" id="rcu_nohz_full_cpu" title='rcu_nohz_full_cpu' data-ref="rcu_nohz_full_cpu">rcu_nohz_full_cpu</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_state" title='rcu_state' data-ref="rcu_state">rcu_state</a> *<dfn class="local col6 decl" id="596rsp" title='rsp' data-type='struct rcu_state *' data-ref="596rsp">rsp</dfn>)</td></tr>
<tr><th id="2578">2578</th><td>{</td></tr>
<tr><th id="2579">2579</th><td><u>#<span data-ppcond="2579">ifdef</span> <span class="macro" data-ref="_M/CONFIG_NO_HZ_FULL">CONFIG_NO_HZ_FULL</span></u></td></tr>
<tr><th id="2580">2580</th><td>	<b>if</b> (tick_nohz_full_cpu(smp_processor_id()) &amp;&amp;</td></tr>
<tr><th id="2581">2581</th><td>	    (!rcu_gp_in_progress(rsp) ||</td></tr>
<tr><th id="2582">2582</th><td>	     ULONG_CMP_LT(jiffies, READ_ONCE(rsp-&gt;gp_start) + HZ)))</td></tr>
<tr><th id="2583">2583</th><td>		<b>return</b> true;</td></tr>
<tr><th id="2584">2584</th><td><u>#<span data-ppcond="2579">endif</span> /* #ifdef CONFIG_NO_HZ_FULL */</u></td></tr>
<tr><th id="2585">2585</th><td>	<b>return</b> <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false">false</a>;</td></tr>
<tr><th id="2586">2586</th><td>}</td></tr>
<tr><th id="2587">2587</th><td></td></tr>
<tr><th id="2588">2588</th><td><i>/*</i></td></tr>
<tr><th id="2589">2589</th><td><i> * Bind the grace-period kthread for the sysidle flavor of RCU to the</i></td></tr>
<tr><th id="2590">2590</th><td><i> * timekeeping CPU.</i></td></tr>
<tr><th id="2591">2591</th><td><i> */</i></td></tr>
<tr><th id="2592">2592</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_bind_gp_kthread" title='rcu_bind_gp_kthread' data-ref="rcu_bind_gp_kthread">rcu_bind_gp_kthread</dfn>(<em>void</em>)</td></tr>
<tr><th id="2593">2593</th><td>{</td></tr>
<tr><th id="2594">2594</th><td>	<em>int</em> <a class="macro" href="../../include/linux/compiler-gcc.h.html#147" title="__attribute__((unused))" data-ref="_M/__maybe_unused">__maybe_unused</a> <dfn class="local col7 decl" id="597cpu" title='cpu' data-type='int' data-ref="597cpu">cpu</dfn>;</td></tr>
<tr><th id="2595">2595</th><td></td></tr>
<tr><th id="2596">2596</th><td>	<b>if</b> (!<a class="ref fn" href="../../include/linux/tick.h.html#tick_nohz_full_enabled" title='tick_nohz_full_enabled' data-ref="tick_nohz_full_enabled">tick_nohz_full_enabled</a>())</td></tr>
<tr><th id="2597">2597</th><td>		<b>return</b>;</td></tr>
<tr><th id="2598">2598</th><td>	<a class="ref fn" href="../../include/linux/tick.h.html#housekeeping_affine" title='housekeeping_affine' data-ref="housekeeping_affine">housekeeping_affine</a>(<a class="macro" href="../../arch/x86/include/asm/current.h.html#18" title="get_current()" data-ref="_M/current">current</a>);</td></tr>
<tr><th id="2599">2599</th><td>}</td></tr>
<tr><th id="2600">2600</th><td></td></tr>
<tr><th id="2601">2601</th><td><i>/* Record the current task on dyntick-idle entry. */</i></td></tr>
<tr><th id="2602">2602</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_dynticks_task_enter" title='rcu_dynticks_task_enter' data-ref="rcu_dynticks_task_enter">rcu_dynticks_task_enter</dfn>(<em>void</em>)</td></tr>
<tr><th id="2603">2603</th><td>{</td></tr>
<tr><th id="2604">2604</th><td><u>#<span data-ppcond="2604">if</span> defined(<span class="macro" data-ref="_M/CONFIG_TASKS_RCU">CONFIG_TASKS_RCU</span>) &amp;&amp; defined(<span class="macro" data-ref="_M/CONFIG_NO_HZ_FULL">CONFIG_NO_HZ_FULL</span>)</u></td></tr>
<tr><th id="2605">2605</th><td>	WRITE_ONCE(current-&gt;rcu_tasks_idle_cpu, smp_processor_id());</td></tr>
<tr><th id="2606">2606</th><td><u>#<span data-ppcond="2604">endif</span> /* #if defined(CONFIG_TASKS_RCU) &amp;&amp; defined(CONFIG_NO_HZ_FULL) */</u></td></tr>
<tr><th id="2607">2607</th><td>}</td></tr>
<tr><th id="2608">2608</th><td></td></tr>
<tr><th id="2609">2609</th><td><i>/* Record no current task on dyntick-idle exit. */</i></td></tr>
<tr><th id="2610">2610</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_dynticks_task_exit" title='rcu_dynticks_task_exit' data-ref="rcu_dynticks_task_exit">rcu_dynticks_task_exit</dfn>(<em>void</em>)</td></tr>
<tr><th id="2611">2611</th><td>{</td></tr>
<tr><th id="2612">2612</th><td><u>#<span data-ppcond="2612">if</span> defined(<span class="macro" data-ref="_M/CONFIG_TASKS_RCU">CONFIG_TASKS_RCU</span>) &amp;&amp; defined(<span class="macro" data-ref="_M/CONFIG_NO_HZ_FULL">CONFIG_NO_HZ_FULL</span>)</u></td></tr>
<tr><th id="2613">2613</th><td>	WRITE_ONCE(current-&gt;rcu_tasks_idle_cpu, -<var>1</var>);</td></tr>
<tr><th id="2614">2614</th><td><u>#<span data-ppcond="2612">endif</span> /* #if defined(CONFIG_TASKS_RCU) &amp;&amp; defined(CONFIG_NO_HZ_FULL) */</u></td></tr>
<tr><th id="2615">2615</th><td>}</td></tr>
<tr><th id="2616">2616</th><td></td></tr>
</table><hr/><p id='footer'>
Generated while processing <a href='tree.c.html'>linux-4.14.y/kernel/rcu/tree.c</a><br/>Generated on <em>2018-Aug-02</em> from project linux-4.14.y revision <em>linux-4.14.y</em><br />Powered by <a href='https://woboq.com'><img alt='Woboq' src='https://code.woboq.org/woboq-16.png' width='41' height='16' /></a> <a href='https://code.woboq.org'>Code Browser</a> 2.1
<br/>Generator usage only permitted with license.</p>
</div></body></html>
