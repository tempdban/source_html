<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>compaction.c source code [linux-4.14.y/mm/compaction.c] - Woboq Code Browser</title>
<link rel="stylesheet" href="../../../data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="../../../data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="../../../data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="../../../data/jquery/jquery-ui.min.js"></script>
<script>var file = 'linux-4.14.y/mm/compaction.c'; var root_path = '../..'; var data_path = '../../../data';</script>
<script src='../../../data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='..'>linux-4.14.y</a>/<a href='./'>mm</a>/<a href='compaction.c.html'>compaction.c</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>// SPDX-License-Identifier: GPL-2.0</i></td></tr>
<tr><th id="2">2</th><td><i>/*</i></td></tr>
<tr><th id="3">3</th><td><i> * linux/mm/compaction.c</i></td></tr>
<tr><th id="4">4</th><td><i> *</i></td></tr>
<tr><th id="5">5</th><td><i> * Memory compaction for the reduction of external fragmentation. Note that</i></td></tr>
<tr><th id="6">6</th><td><i> * this heavily depends upon page migration to do all the real heavy</i></td></tr>
<tr><th id="7">7</th><td><i> * lifting</i></td></tr>
<tr><th id="8">8</th><td><i> *</i></td></tr>
<tr><th id="9">9</th><td><i> * Copyright IBM Corp. 2007-2010 Mel Gorman &lt;mel@csn.ul.ie&gt;</i></td></tr>
<tr><th id="10">10</th><td><i> */</i></td></tr>
<tr><th id="11">11</th><td><u>#include <a href="../include/linux/cpu.h.html">&lt;linux/cpu.h&gt;</a></u></td></tr>
<tr><th id="12">12</th><td><u>#include <a href="../include/linux/swap.h.html">&lt;linux/swap.h&gt;</a></u></td></tr>
<tr><th id="13">13</th><td><u>#include <a href="../include/linux/migrate.h.html">&lt;linux/migrate.h&gt;</a></u></td></tr>
<tr><th id="14">14</th><td><u>#include <a href="../include/linux/compaction.h.html">&lt;linux/compaction.h&gt;</a></u></td></tr>
<tr><th id="15">15</th><td><u>#include <a href="../include/linux/mm_inline.h.html">&lt;linux/mm_inline.h&gt;</a></u></td></tr>
<tr><th id="16">16</th><td><u>#include <a href="../include/linux/sched/signal.h.html">&lt;linux/sched/signal.h&gt;</a></u></td></tr>
<tr><th id="17">17</th><td><u>#include <a href="../include/linux/backing-dev.h.html">&lt;linux/backing-dev.h&gt;</a></u></td></tr>
<tr><th id="18">18</th><td><u>#include <a href="../include/linux/sysctl.h.html">&lt;linux/sysctl.h&gt;</a></u></td></tr>
<tr><th id="19">19</th><td><u>#include <a href="../include/linux/sysfs.h.html">&lt;linux/sysfs.h&gt;</a></u></td></tr>
<tr><th id="20">20</th><td><u>#include <a href="../include/linux/page-isolation.h.html">&lt;linux/page-isolation.h&gt;</a></u></td></tr>
<tr><th id="21">21</th><td><u>#include <a href="../include/linux/kasan.h.html">&lt;linux/kasan.h&gt;</a></u></td></tr>
<tr><th id="22">22</th><td><u>#include <a href="../include/linux/kthread.h.html">&lt;linux/kthread.h&gt;</a></u></td></tr>
<tr><th id="23">23</th><td><u>#include <a href="../include/linux/freezer.h.html">&lt;linux/freezer.h&gt;</a></u></td></tr>
<tr><th id="24">24</th><td><u>#include <a href="../include/linux/page_owner.h.html">&lt;linux/page_owner.h&gt;</a></u></td></tr>
<tr><th id="25">25</th><td><u>#include <a href="internal.h.html">"internal.h"</a></u></td></tr>
<tr><th id="26">26</th><td></td></tr>
<tr><th id="27">27</th><td><u>#<span data-ppcond="27">ifdef</span> <span class="macro" data-ref="_M/CONFIG_COMPACTION">CONFIG_COMPACTION</span></u></td></tr>
<tr><th id="28">28</th><td><em>static</em> <b>inline</b> <em>void</em> count_compact_event(<b>enum</b> vm_event_item item)</td></tr>
<tr><th id="29">29</th><td>{</td></tr>
<tr><th id="30">30</th><td>	count_vm_event(item);</td></tr>
<tr><th id="31">31</th><td>}</td></tr>
<tr><th id="32">32</th><td></td></tr>
<tr><th id="33">33</th><td><em>static</em> <b>inline</b> <em>void</em> count_compact_events(<b>enum</b> vm_event_item item, <em>long</em> delta)</td></tr>
<tr><th id="34">34</th><td>{</td></tr>
<tr><th id="35">35</th><td>	count_vm_events(item, delta);</td></tr>
<tr><th id="36">36</th><td>}</td></tr>
<tr><th id="37">37</th><td><u>#<span data-ppcond="27">else</span></u></td></tr>
<tr><th id="38">38</th><td><u>#define <dfn class="macro" id="_M/count_compact_event" data-ref="_M/count_compact_event">count_compact_event</dfn>(item) do { } while (0)</u></td></tr>
<tr><th id="39">39</th><td><u>#define <dfn class="macro" id="_M/count_compact_events" data-ref="_M/count_compact_events">count_compact_events</dfn>(item, delta) do { } while (0)</u></td></tr>
<tr><th id="40">40</th><td><u>#<span data-ppcond="27">endif</span></u></td></tr>
<tr><th id="41">41</th><td></td></tr>
<tr><th id="42">42</th><td><u>#<span data-ppcond="42">if</span> defined <span class="macro" data-ref="_M/CONFIG_COMPACTION">CONFIG_COMPACTION</span> || defined <span class="macro" data-ref="_M/CONFIG_CMA">CONFIG_CMA</span></u></td></tr>
<tr><th id="43">43</th><td></td></tr>
<tr><th id="44">44</th><td><u>#define CREATE_TRACE_POINTS</u></td></tr>
<tr><th id="45">45</th><td><u>#include &lt;trace/events/compaction.h&gt;</u></td></tr>
<tr><th id="46">46</th><td></td></tr>
<tr><th id="47">47</th><td><u>#define block_start_pfn(pfn, order)	round_down(pfn, 1UL &lt;&lt; (order))</u></td></tr>
<tr><th id="48">48</th><td><u>#define block_end_pfn(pfn, order)	ALIGN((pfn) + 1, 1UL &lt;&lt; (order))</u></td></tr>
<tr><th id="49">49</th><td><u>#define pageblock_start_pfn(pfn)	block_start_pfn(pfn, pageblock_order)</u></td></tr>
<tr><th id="50">50</th><td><u>#define pageblock_end_pfn(pfn)		block_end_pfn(pfn, pageblock_order)</u></td></tr>
<tr><th id="51">51</th><td></td></tr>
<tr><th id="52">52</th><td><em>static</em> <em>unsigned</em> <em>long</em> release_freepages(<b>struct</b> list_head *freelist)</td></tr>
<tr><th id="53">53</th><td>{</td></tr>
<tr><th id="54">54</th><td>	<b>struct</b> page *page, *next;</td></tr>
<tr><th id="55">55</th><td>	<em>unsigned</em> <em>long</em> high_pfn = <var>0</var>;</td></tr>
<tr><th id="56">56</th><td></td></tr>
<tr><th id="57">57</th><td>	list_for_each_entry_safe(page, next, freelist, lru) {</td></tr>
<tr><th id="58">58</th><td>		<em>unsigned</em> <em>long</em> pfn = page_to_pfn(page);</td></tr>
<tr><th id="59">59</th><td>		list_del(&amp;page-&gt;lru);</td></tr>
<tr><th id="60">60</th><td>		__free_page(page);</td></tr>
<tr><th id="61">61</th><td>		<b>if</b> (pfn &gt; high_pfn)</td></tr>
<tr><th id="62">62</th><td>			high_pfn = pfn;</td></tr>
<tr><th id="63">63</th><td>	}</td></tr>
<tr><th id="64">64</th><td></td></tr>
<tr><th id="65">65</th><td>	<b>return</b> high_pfn;</td></tr>
<tr><th id="66">66</th><td>}</td></tr>
<tr><th id="67">67</th><td></td></tr>
<tr><th id="68">68</th><td><em>static</em> <em>void</em> map_pages(<b>struct</b> list_head *list)</td></tr>
<tr><th id="69">69</th><td>{</td></tr>
<tr><th id="70">70</th><td>	<em>unsigned</em> <em>int</em> i, order, nr_pages;</td></tr>
<tr><th id="71">71</th><td>	<b>struct</b> page *page, *next;</td></tr>
<tr><th id="72">72</th><td>	LIST_HEAD(tmp_list);</td></tr>
<tr><th id="73">73</th><td></td></tr>
<tr><th id="74">74</th><td>	list_for_each_entry_safe(page, next, list, lru) {</td></tr>
<tr><th id="75">75</th><td>		list_del(&amp;page-&gt;lru);</td></tr>
<tr><th id="76">76</th><td></td></tr>
<tr><th id="77">77</th><td>		order = page_private(page);</td></tr>
<tr><th id="78">78</th><td>		nr_pages = <var>1</var> &lt;&lt; order;</td></tr>
<tr><th id="79">79</th><td></td></tr>
<tr><th id="80">80</th><td>		post_alloc_hook(page, order, __GFP_MOVABLE);</td></tr>
<tr><th id="81">81</th><td>		<b>if</b> (order)</td></tr>
<tr><th id="82">82</th><td>			split_page(page, order);</td></tr>
<tr><th id="83">83</th><td></td></tr>
<tr><th id="84">84</th><td>		<b>for</b> (i = <var>0</var>; i &lt; nr_pages; i++) {</td></tr>
<tr><th id="85">85</th><td>			list_add(&amp;page-&gt;lru, &amp;tmp_list);</td></tr>
<tr><th id="86">86</th><td>			page++;</td></tr>
<tr><th id="87">87</th><td>		}</td></tr>
<tr><th id="88">88</th><td>	}</td></tr>
<tr><th id="89">89</th><td></td></tr>
<tr><th id="90">90</th><td>	list_splice(&amp;tmp_list, list);</td></tr>
<tr><th id="91">91</th><td>}</td></tr>
<tr><th id="92">92</th><td></td></tr>
<tr><th id="93">93</th><td><u>#ifdef CONFIG_COMPACTION</u></td></tr>
<tr><th id="94">94</th><td></td></tr>
<tr><th id="95">95</th><td><em>int</em> PageMovable(<b>struct</b> page *page)</td></tr>
<tr><th id="96">96</th><td>{</td></tr>
<tr><th id="97">97</th><td>	<b>struct</b> address_space *mapping;</td></tr>
<tr><th id="98">98</th><td></td></tr>
<tr><th id="99">99</th><td>	VM_BUG_ON_PAGE(!PageLocked(page), page);</td></tr>
<tr><th id="100">100</th><td>	<b>if</b> (!__PageMovable(page))</td></tr>
<tr><th id="101">101</th><td>		<b>return</b> <var>0</var>;</td></tr>
<tr><th id="102">102</th><td></td></tr>
<tr><th id="103">103</th><td>	mapping = page_mapping(page);</td></tr>
<tr><th id="104">104</th><td>	<b>if</b> (mapping &amp;&amp; mapping-&gt;a_ops &amp;&amp; mapping-&gt;a_ops-&gt;isolate_page)</td></tr>
<tr><th id="105">105</th><td>		<b>return</b> <var>1</var>;</td></tr>
<tr><th id="106">106</th><td></td></tr>
<tr><th id="107">107</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="108">108</th><td>}</td></tr>
<tr><th id="109">109</th><td>EXPORT_SYMBOL(PageMovable);</td></tr>
<tr><th id="110">110</th><td></td></tr>
<tr><th id="111">111</th><td><em>void</em> __SetPageMovable(<b>struct</b> page *page, <b>struct</b> address_space *mapping)</td></tr>
<tr><th id="112">112</th><td>{</td></tr>
<tr><th id="113">113</th><td>	VM_BUG_ON_PAGE(!PageLocked(page), page);</td></tr>
<tr><th id="114">114</th><td>	VM_BUG_ON_PAGE((<em>unsigned</em> <em>long</em>)mapping &amp; PAGE_MAPPING_MOVABLE, page);</td></tr>
<tr><th id="115">115</th><td>	page-&gt;mapping = (<em>void</em> *)((<em>unsigned</em> <em>long</em>)mapping | PAGE_MAPPING_MOVABLE);</td></tr>
<tr><th id="116">116</th><td>}</td></tr>
<tr><th id="117">117</th><td>EXPORT_SYMBOL(__SetPageMovable);</td></tr>
<tr><th id="118">118</th><td></td></tr>
<tr><th id="119">119</th><td><em>void</em> __ClearPageMovable(<b>struct</b> page *page)</td></tr>
<tr><th id="120">120</th><td>{</td></tr>
<tr><th id="121">121</th><td>	VM_BUG_ON_PAGE(!PageLocked(page), page);</td></tr>
<tr><th id="122">122</th><td>	VM_BUG_ON_PAGE(!PageMovable(page), page);</td></tr>
<tr><th id="123">123</th><td>	<i>/*</i></td></tr>
<tr><th id="124">124</th><td><i>	 * Clear registered address_space val with keeping PAGE_MAPPING_MOVABLE</i></td></tr>
<tr><th id="125">125</th><td><i>	 * flag so that VM can catch up released page by driver after isolation.</i></td></tr>
<tr><th id="126">126</th><td><i>	 * With it, VM migration doesn't try to put it back.</i></td></tr>
<tr><th id="127">127</th><td><i>	 */</i></td></tr>
<tr><th id="128">128</th><td>	page-&gt;mapping = (<em>void</em> *)((<em>unsigned</em> <em>long</em>)page-&gt;mapping &amp;</td></tr>
<tr><th id="129">129</th><td>				PAGE_MAPPING_MOVABLE);</td></tr>
<tr><th id="130">130</th><td>}</td></tr>
<tr><th id="131">131</th><td>EXPORT_SYMBOL(__ClearPageMovable);</td></tr>
<tr><th id="132">132</th><td></td></tr>
<tr><th id="133">133</th><td><i>/* Do not skip compaction more than 64 times */</i></td></tr>
<tr><th id="134">134</th><td><u>#define COMPACT_MAX_DEFER_SHIFT 6</u></td></tr>
<tr><th id="135">135</th><td></td></tr>
<tr><th id="136">136</th><td><i>/*</i></td></tr>
<tr><th id="137">137</th><td><i> * Compaction is deferred when compaction fails to result in a page</i></td></tr>
<tr><th id="138">138</th><td><i> * allocation success. 1 &lt;&lt; compact_defer_limit compactions are skipped up</i></td></tr>
<tr><th id="139">139</th><td><i> * to a limit of 1 &lt;&lt; COMPACT_MAX_DEFER_SHIFT</i></td></tr>
<tr><th id="140">140</th><td><i> */</i></td></tr>
<tr><th id="141">141</th><td><em>void</em> defer_compaction(<b>struct</b> zone *zone, <em>int</em> order)</td></tr>
<tr><th id="142">142</th><td>{</td></tr>
<tr><th id="143">143</th><td>	zone-&gt;compact_considered = <var>0</var>;</td></tr>
<tr><th id="144">144</th><td>	zone-&gt;compact_defer_shift++;</td></tr>
<tr><th id="145">145</th><td></td></tr>
<tr><th id="146">146</th><td>	<b>if</b> (order &lt; zone-&gt;compact_order_failed)</td></tr>
<tr><th id="147">147</th><td>		zone-&gt;compact_order_failed = order;</td></tr>
<tr><th id="148">148</th><td></td></tr>
<tr><th id="149">149</th><td>	<b>if</b> (zone-&gt;compact_defer_shift &gt; COMPACT_MAX_DEFER_SHIFT)</td></tr>
<tr><th id="150">150</th><td>		zone-&gt;compact_defer_shift = COMPACT_MAX_DEFER_SHIFT;</td></tr>
<tr><th id="151">151</th><td></td></tr>
<tr><th id="152">152</th><td>	trace_mm_compaction_defer_compaction(zone, order);</td></tr>
<tr><th id="153">153</th><td>}</td></tr>
<tr><th id="154">154</th><td></td></tr>
<tr><th id="155">155</th><td><i>/* Returns true if compaction should be skipped this time */</i></td></tr>
<tr><th id="156">156</th><td>bool compaction_deferred(<b>struct</b> zone *zone, <em>int</em> order)</td></tr>
<tr><th id="157">157</th><td>{</td></tr>
<tr><th id="158">158</th><td>	<em>unsigned</em> <em>long</em> defer_limit = <var>1UL</var> &lt;&lt; zone-&gt;compact_defer_shift;</td></tr>
<tr><th id="159">159</th><td></td></tr>
<tr><th id="160">160</th><td>	<b>if</b> (order &lt; zone-&gt;compact_order_failed)</td></tr>
<tr><th id="161">161</th><td>		<b>return</b> false;</td></tr>
<tr><th id="162">162</th><td></td></tr>
<tr><th id="163">163</th><td>	<i>/* Avoid possible overflow */</i></td></tr>
<tr><th id="164">164</th><td>	<b>if</b> (++zone-&gt;compact_considered &gt; defer_limit)</td></tr>
<tr><th id="165">165</th><td>		zone-&gt;compact_considered = defer_limit;</td></tr>
<tr><th id="166">166</th><td></td></tr>
<tr><th id="167">167</th><td>	<b>if</b> (zone-&gt;compact_considered &gt;= defer_limit)</td></tr>
<tr><th id="168">168</th><td>		<b>return</b> false;</td></tr>
<tr><th id="169">169</th><td></td></tr>
<tr><th id="170">170</th><td>	trace_mm_compaction_deferred(zone, order);</td></tr>
<tr><th id="171">171</th><td></td></tr>
<tr><th id="172">172</th><td>	<b>return</b> true;</td></tr>
<tr><th id="173">173</th><td>}</td></tr>
<tr><th id="174">174</th><td></td></tr>
<tr><th id="175">175</th><td><i>/*</i></td></tr>
<tr><th id="176">176</th><td><i> * Update defer tracking counters after successful compaction of given order,</i></td></tr>
<tr><th id="177">177</th><td><i> * which means an allocation either succeeded (alloc_success == true) or is</i></td></tr>
<tr><th id="178">178</th><td><i> * expected to succeed.</i></td></tr>
<tr><th id="179">179</th><td><i> */</i></td></tr>
<tr><th id="180">180</th><td><em>void</em> compaction_defer_reset(<b>struct</b> zone *zone, <em>int</em> order,</td></tr>
<tr><th id="181">181</th><td>		bool alloc_success)</td></tr>
<tr><th id="182">182</th><td>{</td></tr>
<tr><th id="183">183</th><td>	<b>if</b> (alloc_success) {</td></tr>
<tr><th id="184">184</th><td>		zone-&gt;compact_considered = <var>0</var>;</td></tr>
<tr><th id="185">185</th><td>		zone-&gt;compact_defer_shift = <var>0</var>;</td></tr>
<tr><th id="186">186</th><td>	}</td></tr>
<tr><th id="187">187</th><td>	<b>if</b> (order &gt;= zone-&gt;compact_order_failed)</td></tr>
<tr><th id="188">188</th><td>		zone-&gt;compact_order_failed = order + <var>1</var>;</td></tr>
<tr><th id="189">189</th><td></td></tr>
<tr><th id="190">190</th><td>	trace_mm_compaction_defer_reset(zone, order);</td></tr>
<tr><th id="191">191</th><td>}</td></tr>
<tr><th id="192">192</th><td></td></tr>
<tr><th id="193">193</th><td><i>/* Returns true if restarting compaction after many failures */</i></td></tr>
<tr><th id="194">194</th><td>bool compaction_restarting(<b>struct</b> zone *zone, <em>int</em> order)</td></tr>
<tr><th id="195">195</th><td>{</td></tr>
<tr><th id="196">196</th><td>	<b>if</b> (order &lt; zone-&gt;compact_order_failed)</td></tr>
<tr><th id="197">197</th><td>		<b>return</b> false;</td></tr>
<tr><th id="198">198</th><td></td></tr>
<tr><th id="199">199</th><td>	<b>return</b> zone-&gt;compact_defer_shift == COMPACT_MAX_DEFER_SHIFT &amp;&amp;</td></tr>
<tr><th id="200">200</th><td>		zone-&gt;compact_considered &gt;= <var>1UL</var> &lt;&lt; zone-&gt;compact_defer_shift;</td></tr>
<tr><th id="201">201</th><td>}</td></tr>
<tr><th id="202">202</th><td></td></tr>
<tr><th id="203">203</th><td><i>/* Returns true if the pageblock should be scanned for pages to isolate. */</i></td></tr>
<tr><th id="204">204</th><td><em>static</em> <b>inline</b> bool isolation_suitable(<b>struct</b> compact_control *cc,</td></tr>
<tr><th id="205">205</th><td>					<b>struct</b> page *page)</td></tr>
<tr><th id="206">206</th><td>{</td></tr>
<tr><th id="207">207</th><td>	<b>if</b> (cc-&gt;ignore_skip_hint)</td></tr>
<tr><th id="208">208</th><td>		<b>return</b> true;</td></tr>
<tr><th id="209">209</th><td></td></tr>
<tr><th id="210">210</th><td>	<b>return</b> !get_pageblock_skip(page);</td></tr>
<tr><th id="211">211</th><td>}</td></tr>
<tr><th id="212">212</th><td></td></tr>
<tr><th id="213">213</th><td><em>static</em> <em>void</em> reset_cached_positions(<b>struct</b> zone *zone)</td></tr>
<tr><th id="214">214</th><td>{</td></tr>
<tr><th id="215">215</th><td>	zone-&gt;compact_cached_migrate_pfn[<var>0</var>] = zone-&gt;zone_start_pfn;</td></tr>
<tr><th id="216">216</th><td>	zone-&gt;compact_cached_migrate_pfn[<var>1</var>] = zone-&gt;zone_start_pfn;</td></tr>
<tr><th id="217">217</th><td>	zone-&gt;compact_cached_free_pfn =</td></tr>
<tr><th id="218">218</th><td>				pageblock_start_pfn(zone_end_pfn(zone) - <var>1</var>);</td></tr>
<tr><th id="219">219</th><td>}</td></tr>
<tr><th id="220">220</th><td></td></tr>
<tr><th id="221">221</th><td><i>/*</i></td></tr>
<tr><th id="222">222</th><td><i> * This function is called to clear all cached information on pageblocks that</i></td></tr>
<tr><th id="223">223</th><td><i> * should be skipped for page isolation when the migrate and free page scanner</i></td></tr>
<tr><th id="224">224</th><td><i> * meet.</i></td></tr>
<tr><th id="225">225</th><td><i> */</i></td></tr>
<tr><th id="226">226</th><td><em>static</em> <em>void</em> __reset_isolation_suitable(<b>struct</b> zone *zone)</td></tr>
<tr><th id="227">227</th><td>{</td></tr>
<tr><th id="228">228</th><td>	<em>unsigned</em> <em>long</em> start_pfn = zone-&gt;zone_start_pfn;</td></tr>
<tr><th id="229">229</th><td>	<em>unsigned</em> <em>long</em> end_pfn = zone_end_pfn(zone);</td></tr>
<tr><th id="230">230</th><td>	<em>unsigned</em> <em>long</em> pfn;</td></tr>
<tr><th id="231">231</th><td></td></tr>
<tr><th id="232">232</th><td>	zone-&gt;compact_blockskip_flush = false;</td></tr>
<tr><th id="233">233</th><td></td></tr>
<tr><th id="234">234</th><td>	<i>/* Walk the zone and mark every pageblock as suitable for isolation */</i></td></tr>
<tr><th id="235">235</th><td>	<b>for</b> (pfn = start_pfn; pfn &lt; end_pfn; pfn += pageblock_nr_pages) {</td></tr>
<tr><th id="236">236</th><td>		<b>struct</b> page *page;</td></tr>
<tr><th id="237">237</th><td></td></tr>
<tr><th id="238">238</th><td>		cond_resched();</td></tr>
<tr><th id="239">239</th><td></td></tr>
<tr><th id="240">240</th><td>		page = pfn_to_online_page(pfn);</td></tr>
<tr><th id="241">241</th><td>		<b>if</b> (!page)</td></tr>
<tr><th id="242">242</th><td>			<b>continue</b>;</td></tr>
<tr><th id="243">243</th><td>		<b>if</b> (zone != page_zone(page))</td></tr>
<tr><th id="244">244</th><td>			<b>continue</b>;</td></tr>
<tr><th id="245">245</th><td></td></tr>
<tr><th id="246">246</th><td>		clear_pageblock_skip(page);</td></tr>
<tr><th id="247">247</th><td>	}</td></tr>
<tr><th id="248">248</th><td></td></tr>
<tr><th id="249">249</th><td>	reset_cached_positions(zone);</td></tr>
<tr><th id="250">250</th><td>}</td></tr>
<tr><th id="251">251</th><td></td></tr>
<tr><th id="252">252</th><td><em>void</em> reset_isolation_suitable(pg_data_t *pgdat)</td></tr>
<tr><th id="253">253</th><td>{</td></tr>
<tr><th id="254">254</th><td>	<em>int</em> zoneid;</td></tr>
<tr><th id="255">255</th><td></td></tr>
<tr><th id="256">256</th><td>	<b>for</b> (zoneid = <var>0</var>; zoneid &lt; MAX_NR_ZONES; zoneid++) {</td></tr>
<tr><th id="257">257</th><td>		<b>struct</b> zone *zone = &amp;pgdat-&gt;node_zones[zoneid];</td></tr>
<tr><th id="258">258</th><td>		<b>if</b> (!populated_zone(zone))</td></tr>
<tr><th id="259">259</th><td>			<b>continue</b>;</td></tr>
<tr><th id="260">260</th><td></td></tr>
<tr><th id="261">261</th><td>		<i>/* Only flush if a full compaction finished recently */</i></td></tr>
<tr><th id="262">262</th><td>		<b>if</b> (zone-&gt;compact_blockskip_flush)</td></tr>
<tr><th id="263">263</th><td>			__reset_isolation_suitable(zone);</td></tr>
<tr><th id="264">264</th><td>	}</td></tr>
<tr><th id="265">265</th><td>}</td></tr>
<tr><th id="266">266</th><td></td></tr>
<tr><th id="267">267</th><td><i>/*</i></td></tr>
<tr><th id="268">268</th><td><i> * If no pages were isolated then mark this pageblock to be skipped in the</i></td></tr>
<tr><th id="269">269</th><td><i> * future. The information is later cleared by __reset_isolation_suitable().</i></td></tr>
<tr><th id="270">270</th><td><i> */</i></td></tr>
<tr><th id="271">271</th><td><em>static</em> <em>void</em> update_pageblock_skip(<b>struct</b> compact_control *cc,</td></tr>
<tr><th id="272">272</th><td>			<b>struct</b> page *page, <em>unsigned</em> <em>long</em> nr_isolated,</td></tr>
<tr><th id="273">273</th><td>			bool migrate_scanner)</td></tr>
<tr><th id="274">274</th><td>{</td></tr>
<tr><th id="275">275</th><td>	<b>struct</b> zone *zone = cc-&gt;zone;</td></tr>
<tr><th id="276">276</th><td>	<em>unsigned</em> <em>long</em> pfn;</td></tr>
<tr><th id="277">277</th><td></td></tr>
<tr><th id="278">278</th><td>	<b>if</b> (cc-&gt;ignore_skip_hint)</td></tr>
<tr><th id="279">279</th><td>		<b>return</b>;</td></tr>
<tr><th id="280">280</th><td></td></tr>
<tr><th id="281">281</th><td>	<b>if</b> (!page)</td></tr>
<tr><th id="282">282</th><td>		<b>return</b>;</td></tr>
<tr><th id="283">283</th><td></td></tr>
<tr><th id="284">284</th><td>	<b>if</b> (nr_isolated)</td></tr>
<tr><th id="285">285</th><td>		<b>return</b>;</td></tr>
<tr><th id="286">286</th><td></td></tr>
<tr><th id="287">287</th><td>	set_pageblock_skip(page);</td></tr>
<tr><th id="288">288</th><td></td></tr>
<tr><th id="289">289</th><td>	pfn = page_to_pfn(page);</td></tr>
<tr><th id="290">290</th><td></td></tr>
<tr><th id="291">291</th><td>	<i>/* Update where async and sync compaction should restart */</i></td></tr>
<tr><th id="292">292</th><td>	<b>if</b> (migrate_scanner) {</td></tr>
<tr><th id="293">293</th><td>		<b>if</b> (pfn &gt; zone-&gt;compact_cached_migrate_pfn[<var>0</var>])</td></tr>
<tr><th id="294">294</th><td>			zone-&gt;compact_cached_migrate_pfn[<var>0</var>] = pfn;</td></tr>
<tr><th id="295">295</th><td>		<b>if</b> (cc-&gt;mode != MIGRATE_ASYNC &amp;&amp;</td></tr>
<tr><th id="296">296</th><td>		    pfn &gt; zone-&gt;compact_cached_migrate_pfn[<var>1</var>])</td></tr>
<tr><th id="297">297</th><td>			zone-&gt;compact_cached_migrate_pfn[<var>1</var>] = pfn;</td></tr>
<tr><th id="298">298</th><td>	} <b>else</b> {</td></tr>
<tr><th id="299">299</th><td>		<b>if</b> (pfn &lt; zone-&gt;compact_cached_free_pfn)</td></tr>
<tr><th id="300">300</th><td>			zone-&gt;compact_cached_free_pfn = pfn;</td></tr>
<tr><th id="301">301</th><td>	}</td></tr>
<tr><th id="302">302</th><td>}</td></tr>
<tr><th id="303">303</th><td><u>#else</u></td></tr>
<tr><th id="304">304</th><td><em>static</em> <b>inline</b> bool isolation_suitable(<b>struct</b> compact_control *cc,</td></tr>
<tr><th id="305">305</th><td>					<b>struct</b> page *page)</td></tr>
<tr><th id="306">306</th><td>{</td></tr>
<tr><th id="307">307</th><td>	<b>return</b> true;</td></tr>
<tr><th id="308">308</th><td>}</td></tr>
<tr><th id="309">309</th><td></td></tr>
<tr><th id="310">310</th><td><em>static</em> <em>void</em> update_pageblock_skip(<b>struct</b> compact_control *cc,</td></tr>
<tr><th id="311">311</th><td>			<b>struct</b> page *page, <em>unsigned</em> <em>long</em> nr_isolated,</td></tr>
<tr><th id="312">312</th><td>			bool migrate_scanner)</td></tr>
<tr><th id="313">313</th><td>{</td></tr>
<tr><th id="314">314</th><td>}</td></tr>
<tr><th id="315">315</th><td><u>#endif /* CONFIG_COMPACTION */</u></td></tr>
<tr><th id="316">316</th><td></td></tr>
<tr><th id="317">317</th><td><i>/*</i></td></tr>
<tr><th id="318">318</th><td><i> * Compaction requires the taking of some coarse locks that are potentially</i></td></tr>
<tr><th id="319">319</th><td><i> * very heavily contended. For async compaction, back out if the lock cannot</i></td></tr>
<tr><th id="320">320</th><td><i> * be taken immediately. For sync compaction, spin on the lock if needed.</i></td></tr>
<tr><th id="321">321</th><td><i> *</i></td></tr>
<tr><th id="322">322</th><td><i> * Returns true if the lock is held</i></td></tr>
<tr><th id="323">323</th><td><i> * Returns false if the lock is not held and compaction should abort</i></td></tr>
<tr><th id="324">324</th><td><i> */</i></td></tr>
<tr><th id="325">325</th><td><em>static</em> bool compact_trylock_irqsave(spinlock_t *lock, <em>unsigned</em> <em>long</em> *flags,</td></tr>
<tr><th id="326">326</th><td>						<b>struct</b> compact_control *cc)</td></tr>
<tr><th id="327">327</th><td>{</td></tr>
<tr><th id="328">328</th><td>	<b>if</b> (cc-&gt;mode == MIGRATE_ASYNC) {</td></tr>
<tr><th id="329">329</th><td>		<b>if</b> (!spin_trylock_irqsave(lock, *flags)) {</td></tr>
<tr><th id="330">330</th><td>			cc-&gt;contended = true;</td></tr>
<tr><th id="331">331</th><td>			<b>return</b> false;</td></tr>
<tr><th id="332">332</th><td>		}</td></tr>
<tr><th id="333">333</th><td>	} <b>else</b> {</td></tr>
<tr><th id="334">334</th><td>		spin_lock_irqsave(lock, *flags);</td></tr>
<tr><th id="335">335</th><td>	}</td></tr>
<tr><th id="336">336</th><td></td></tr>
<tr><th id="337">337</th><td>	<b>return</b> true;</td></tr>
<tr><th id="338">338</th><td>}</td></tr>
<tr><th id="339">339</th><td></td></tr>
<tr><th id="340">340</th><td><i>/*</i></td></tr>
<tr><th id="341">341</th><td><i> * Compaction requires the taking of some coarse locks that are potentially</i></td></tr>
<tr><th id="342">342</th><td><i> * very heavily contended. The lock should be periodically unlocked to avoid</i></td></tr>
<tr><th id="343">343</th><td><i> * having disabled IRQs for a long time, even when there is nobody waiting on</i></td></tr>
<tr><th id="344">344</th><td><i> * the lock. It might also be that allowing the IRQs will result in</i></td></tr>
<tr><th id="345">345</th><td><i> * need_resched() becoming true. If scheduling is needed, async compaction</i></td></tr>
<tr><th id="346">346</th><td><i> * aborts. Sync compaction schedules.</i></td></tr>
<tr><th id="347">347</th><td><i> * Either compaction type will also abort if a fatal signal is pending.</i></td></tr>
<tr><th id="348">348</th><td><i> * In either case if the lock was locked, it is dropped and not regained.</i></td></tr>
<tr><th id="349">349</th><td><i> *</i></td></tr>
<tr><th id="350">350</th><td><i> * Returns true if compaction should abort due to fatal signal pending, or</i></td></tr>
<tr><th id="351">351</th><td><i> *		async compaction due to need_resched()</i></td></tr>
<tr><th id="352">352</th><td><i> * Returns false when compaction can continue (sync compaction might have</i></td></tr>
<tr><th id="353">353</th><td><i> *		scheduled)</i></td></tr>
<tr><th id="354">354</th><td><i> */</i></td></tr>
<tr><th id="355">355</th><td><em>static</em> bool compact_unlock_should_abort(spinlock_t *lock,</td></tr>
<tr><th id="356">356</th><td>		<em>unsigned</em> <em>long</em> flags, bool *locked, <b>struct</b> compact_control *cc)</td></tr>
<tr><th id="357">357</th><td>{</td></tr>
<tr><th id="358">358</th><td>	<b>if</b> (*locked) {</td></tr>
<tr><th id="359">359</th><td>		spin_unlock_irqrestore(lock, flags);</td></tr>
<tr><th id="360">360</th><td>		*locked = false;</td></tr>
<tr><th id="361">361</th><td>	}</td></tr>
<tr><th id="362">362</th><td></td></tr>
<tr><th id="363">363</th><td>	<b>if</b> (fatal_signal_pending(current)) {</td></tr>
<tr><th id="364">364</th><td>		cc-&gt;contended = true;</td></tr>
<tr><th id="365">365</th><td>		<b>return</b> true;</td></tr>
<tr><th id="366">366</th><td>	}</td></tr>
<tr><th id="367">367</th><td></td></tr>
<tr><th id="368">368</th><td>	<b>if</b> (need_resched()) {</td></tr>
<tr><th id="369">369</th><td>		<b>if</b> (cc-&gt;mode == MIGRATE_ASYNC) {</td></tr>
<tr><th id="370">370</th><td>			cc-&gt;contended = true;</td></tr>
<tr><th id="371">371</th><td>			<b>return</b> true;</td></tr>
<tr><th id="372">372</th><td>		}</td></tr>
<tr><th id="373">373</th><td>		cond_resched();</td></tr>
<tr><th id="374">374</th><td>	}</td></tr>
<tr><th id="375">375</th><td></td></tr>
<tr><th id="376">376</th><td>	<b>return</b> false;</td></tr>
<tr><th id="377">377</th><td>}</td></tr>
<tr><th id="378">378</th><td></td></tr>
<tr><th id="379">379</th><td><i>/*</i></td></tr>
<tr><th id="380">380</th><td><i> * Aside from avoiding lock contention, compaction also periodically checks</i></td></tr>
<tr><th id="381">381</th><td><i> * need_resched() and either schedules in sync compaction or aborts async</i></td></tr>
<tr><th id="382">382</th><td><i> * compaction. This is similar to what compact_unlock_should_abort() does, but</i></td></tr>
<tr><th id="383">383</th><td><i> * is used where no lock is concerned.</i></td></tr>
<tr><th id="384">384</th><td><i> *</i></td></tr>
<tr><th id="385">385</th><td><i> * Returns false when no scheduling was needed, or sync compaction scheduled.</i></td></tr>
<tr><th id="386">386</th><td><i> * Returns true when async compaction should abort.</i></td></tr>
<tr><th id="387">387</th><td><i> */</i></td></tr>
<tr><th id="388">388</th><td><em>static</em> <b>inline</b> bool compact_should_abort(<b>struct</b> compact_control *cc)</td></tr>
<tr><th id="389">389</th><td>{</td></tr>
<tr><th id="390">390</th><td>	<i>/* async compaction aborts if contended */</i></td></tr>
<tr><th id="391">391</th><td>	<b>if</b> (need_resched()) {</td></tr>
<tr><th id="392">392</th><td>		<b>if</b> (cc-&gt;mode == MIGRATE_ASYNC) {</td></tr>
<tr><th id="393">393</th><td>			cc-&gt;contended = true;</td></tr>
<tr><th id="394">394</th><td>			<b>return</b> true;</td></tr>
<tr><th id="395">395</th><td>		}</td></tr>
<tr><th id="396">396</th><td></td></tr>
<tr><th id="397">397</th><td>		cond_resched();</td></tr>
<tr><th id="398">398</th><td>	}</td></tr>
<tr><th id="399">399</th><td></td></tr>
<tr><th id="400">400</th><td>	<b>return</b> false;</td></tr>
<tr><th id="401">401</th><td>}</td></tr>
<tr><th id="402">402</th><td></td></tr>
<tr><th id="403">403</th><td><i>/*</i></td></tr>
<tr><th id="404">404</th><td><i> * Isolate free pages onto a private freelist. If @strict is true, will abort</i></td></tr>
<tr><th id="405">405</th><td><i> * returning 0 on any invalid PFNs or non-free pages inside of the pageblock</i></td></tr>
<tr><th id="406">406</th><td><i> * (even though it may still end up isolating some pages).</i></td></tr>
<tr><th id="407">407</th><td><i> */</i></td></tr>
<tr><th id="408">408</th><td><em>static</em> <em>unsigned</em> <em>long</em> isolate_freepages_block(<b>struct</b> compact_control *cc,</td></tr>
<tr><th id="409">409</th><td>				<em>unsigned</em> <em>long</em> *start_pfn,</td></tr>
<tr><th id="410">410</th><td>				<em>unsigned</em> <em>long</em> end_pfn,</td></tr>
<tr><th id="411">411</th><td>				<b>struct</b> list_head *freelist,</td></tr>
<tr><th id="412">412</th><td>				bool strict)</td></tr>
<tr><th id="413">413</th><td>{</td></tr>
<tr><th id="414">414</th><td>	<em>int</em> nr_scanned = <var>0</var>, total_isolated = <var>0</var>;</td></tr>
<tr><th id="415">415</th><td>	<b>struct</b> page *cursor, *valid_page = NULL;</td></tr>
<tr><th id="416">416</th><td>	<em>unsigned</em> <em>long</em> flags = <var>0</var>;</td></tr>
<tr><th id="417">417</th><td>	bool locked = false;</td></tr>
<tr><th id="418">418</th><td>	<em>unsigned</em> <em>long</em> blockpfn = *start_pfn;</td></tr>
<tr><th id="419">419</th><td>	<em>unsigned</em> <em>int</em> order;</td></tr>
<tr><th id="420">420</th><td></td></tr>
<tr><th id="421">421</th><td>	cursor = pfn_to_page(blockpfn);</td></tr>
<tr><th id="422">422</th><td></td></tr>
<tr><th id="423">423</th><td>	<i>/* Isolate free pages. */</i></td></tr>
<tr><th id="424">424</th><td>	<b>for</b> (; blockpfn &lt; end_pfn; blockpfn++, cursor++) {</td></tr>
<tr><th id="425">425</th><td>		<em>int</em> isolated;</td></tr>
<tr><th id="426">426</th><td>		<b>struct</b> page *page = cursor;</td></tr>
<tr><th id="427">427</th><td></td></tr>
<tr><th id="428">428</th><td>		<i>/*</i></td></tr>
<tr><th id="429">429</th><td><i>		 * Periodically drop the lock (if held) regardless of its</i></td></tr>
<tr><th id="430">430</th><td><i>		 * contention, to give chance to IRQs. Abort if fatal signal</i></td></tr>
<tr><th id="431">431</th><td><i>		 * pending or async compaction detects need_resched()</i></td></tr>
<tr><th id="432">432</th><td><i>		 */</i></td></tr>
<tr><th id="433">433</th><td>		<b>if</b> (!(blockpfn % SWAP_CLUSTER_MAX)</td></tr>
<tr><th id="434">434</th><td>		    &amp;&amp; compact_unlock_should_abort(&amp;cc-&gt;zone-&gt;lock, flags,</td></tr>
<tr><th id="435">435</th><td>								&amp;locked, cc))</td></tr>
<tr><th id="436">436</th><td>			<b>break</b>;</td></tr>
<tr><th id="437">437</th><td></td></tr>
<tr><th id="438">438</th><td>		nr_scanned++;</td></tr>
<tr><th id="439">439</th><td>		<b>if</b> (!pfn_valid_within(blockpfn))</td></tr>
<tr><th id="440">440</th><td>			<b>goto</b> isolate_fail;</td></tr>
<tr><th id="441">441</th><td></td></tr>
<tr><th id="442">442</th><td>		<b>if</b> (!valid_page)</td></tr>
<tr><th id="443">443</th><td>			valid_page = page;</td></tr>
<tr><th id="444">444</th><td></td></tr>
<tr><th id="445">445</th><td>		<i>/*</i></td></tr>
<tr><th id="446">446</th><td><i>		 * For compound pages such as THP and hugetlbfs, we can save</i></td></tr>
<tr><th id="447">447</th><td><i>		 * potentially a lot of iterations if we skip them at once.</i></td></tr>
<tr><th id="448">448</th><td><i>		 * The check is racy, but we can consider only valid values</i></td></tr>
<tr><th id="449">449</th><td><i>		 * and the only danger is skipping too much.</i></td></tr>
<tr><th id="450">450</th><td><i>		 */</i></td></tr>
<tr><th id="451">451</th><td>		<b>if</b> (PageCompound(page)) {</td></tr>
<tr><th id="452">452</th><td>			<em>unsigned</em> <em>int</em> comp_order = compound_order(page);</td></tr>
<tr><th id="453">453</th><td></td></tr>
<tr><th id="454">454</th><td>			<b>if</b> (likely(comp_order &lt; MAX_ORDER)) {</td></tr>
<tr><th id="455">455</th><td>				blockpfn += (<var>1UL</var> &lt;&lt; comp_order) - <var>1</var>;</td></tr>
<tr><th id="456">456</th><td>				cursor += (<var>1UL</var> &lt;&lt; comp_order) - <var>1</var>;</td></tr>
<tr><th id="457">457</th><td>			}</td></tr>
<tr><th id="458">458</th><td></td></tr>
<tr><th id="459">459</th><td>			<b>goto</b> isolate_fail;</td></tr>
<tr><th id="460">460</th><td>		}</td></tr>
<tr><th id="461">461</th><td></td></tr>
<tr><th id="462">462</th><td>		<b>if</b> (!PageBuddy(page))</td></tr>
<tr><th id="463">463</th><td>			<b>goto</b> isolate_fail;</td></tr>
<tr><th id="464">464</th><td></td></tr>
<tr><th id="465">465</th><td>		<i>/*</i></td></tr>
<tr><th id="466">466</th><td><i>		 * If we already hold the lock, we can skip some rechecking.</i></td></tr>
<tr><th id="467">467</th><td><i>		 * Note that if we hold the lock now, checked_pageblock was</i></td></tr>
<tr><th id="468">468</th><td><i>		 * already set in some previous iteration (or strict is true),</i></td></tr>
<tr><th id="469">469</th><td><i>		 * so it is correct to skip the suitable migration target</i></td></tr>
<tr><th id="470">470</th><td><i>		 * recheck as well.</i></td></tr>
<tr><th id="471">471</th><td><i>		 */</i></td></tr>
<tr><th id="472">472</th><td>		<b>if</b> (!locked) {</td></tr>
<tr><th id="473">473</th><td>			<i>/*</i></td></tr>
<tr><th id="474">474</th><td><i>			 * The zone lock must be held to isolate freepages.</i></td></tr>
<tr><th id="475">475</th><td><i>			 * Unfortunately this is a very coarse lock and can be</i></td></tr>
<tr><th id="476">476</th><td><i>			 * heavily contended if there are parallel allocations</i></td></tr>
<tr><th id="477">477</th><td><i>			 * or parallel compactions. For async compaction do not</i></td></tr>
<tr><th id="478">478</th><td><i>			 * spin on the lock and we acquire the lock as late as</i></td></tr>
<tr><th id="479">479</th><td><i>			 * possible.</i></td></tr>
<tr><th id="480">480</th><td><i>			 */</i></td></tr>
<tr><th id="481">481</th><td>			locked = compact_trylock_irqsave(&amp;cc-&gt;zone-&gt;lock,</td></tr>
<tr><th id="482">482</th><td>								&amp;flags, cc);</td></tr>
<tr><th id="483">483</th><td>			<b>if</b> (!locked)</td></tr>
<tr><th id="484">484</th><td>				<b>break</b>;</td></tr>
<tr><th id="485">485</th><td></td></tr>
<tr><th id="486">486</th><td>			<i>/* Recheck this is a buddy page under lock */</i></td></tr>
<tr><th id="487">487</th><td>			<b>if</b> (!PageBuddy(page))</td></tr>
<tr><th id="488">488</th><td>				<b>goto</b> isolate_fail;</td></tr>
<tr><th id="489">489</th><td>		}</td></tr>
<tr><th id="490">490</th><td></td></tr>
<tr><th id="491">491</th><td>		<i>/* Found a free page, will break it into order-0 pages */</i></td></tr>
<tr><th id="492">492</th><td>		order = page_order(page);</td></tr>
<tr><th id="493">493</th><td>		isolated = __isolate_free_page(page, order);</td></tr>
<tr><th id="494">494</th><td>		<b>if</b> (!isolated)</td></tr>
<tr><th id="495">495</th><td>			<b>break</b>;</td></tr>
<tr><th id="496">496</th><td>		set_page_private(page, order);</td></tr>
<tr><th id="497">497</th><td></td></tr>
<tr><th id="498">498</th><td>		total_isolated += isolated;</td></tr>
<tr><th id="499">499</th><td>		cc-&gt;nr_freepages += isolated;</td></tr>
<tr><th id="500">500</th><td>		list_add_tail(&amp;page-&gt;lru, freelist);</td></tr>
<tr><th id="501">501</th><td></td></tr>
<tr><th id="502">502</th><td>		<b>if</b> (!strict &amp;&amp; cc-&gt;nr_migratepages &lt;= cc-&gt;nr_freepages) {</td></tr>
<tr><th id="503">503</th><td>			blockpfn += isolated;</td></tr>
<tr><th id="504">504</th><td>			<b>break</b>;</td></tr>
<tr><th id="505">505</th><td>		}</td></tr>
<tr><th id="506">506</th><td>		<i>/* Advance to the end of split page */</i></td></tr>
<tr><th id="507">507</th><td>		blockpfn += isolated - <var>1</var>;</td></tr>
<tr><th id="508">508</th><td>		cursor += isolated - <var>1</var>;</td></tr>
<tr><th id="509">509</th><td>		<b>continue</b>;</td></tr>
<tr><th id="510">510</th><td></td></tr>
<tr><th id="511">511</th><td>isolate_fail:</td></tr>
<tr><th id="512">512</th><td>		<b>if</b> (strict)</td></tr>
<tr><th id="513">513</th><td>			<b>break</b>;</td></tr>
<tr><th id="514">514</th><td>		<b>else</b></td></tr>
<tr><th id="515">515</th><td>			<b>continue</b>;</td></tr>
<tr><th id="516">516</th><td></td></tr>
<tr><th id="517">517</th><td>	}</td></tr>
<tr><th id="518">518</th><td></td></tr>
<tr><th id="519">519</th><td>	<b>if</b> (locked)</td></tr>
<tr><th id="520">520</th><td>		spin_unlock_irqrestore(&amp;cc-&gt;zone-&gt;lock, flags);</td></tr>
<tr><th id="521">521</th><td></td></tr>
<tr><th id="522">522</th><td>	<i>/*</i></td></tr>
<tr><th id="523">523</th><td><i>	 * There is a tiny chance that we have read bogus compound_order(),</i></td></tr>
<tr><th id="524">524</th><td><i>	 * so be careful to not go outside of the pageblock.</i></td></tr>
<tr><th id="525">525</th><td><i>	 */</i></td></tr>
<tr><th id="526">526</th><td>	<b>if</b> (unlikely(blockpfn &gt; end_pfn))</td></tr>
<tr><th id="527">527</th><td>		blockpfn = end_pfn;</td></tr>
<tr><th id="528">528</th><td></td></tr>
<tr><th id="529">529</th><td>	trace_mm_compaction_isolate_freepages(*start_pfn, blockpfn,</td></tr>
<tr><th id="530">530</th><td>					nr_scanned, total_isolated);</td></tr>
<tr><th id="531">531</th><td></td></tr>
<tr><th id="532">532</th><td>	<i>/* Record how far we have got within the block */</i></td></tr>
<tr><th id="533">533</th><td>	*start_pfn = blockpfn;</td></tr>
<tr><th id="534">534</th><td></td></tr>
<tr><th id="535">535</th><td>	<i>/*</i></td></tr>
<tr><th id="536">536</th><td><i>	 * If strict isolation is requested by CMA then check that all the</i></td></tr>
<tr><th id="537">537</th><td><i>	 * pages requested were isolated. If there were any failures, 0 is</i></td></tr>
<tr><th id="538">538</th><td><i>	 * returned and CMA will fail.</i></td></tr>
<tr><th id="539">539</th><td><i>	 */</i></td></tr>
<tr><th id="540">540</th><td>	<b>if</b> (strict &amp;&amp; blockpfn &lt; end_pfn)</td></tr>
<tr><th id="541">541</th><td>		total_isolated = <var>0</var>;</td></tr>
<tr><th id="542">542</th><td></td></tr>
<tr><th id="543">543</th><td>	<i>/* Update the pageblock-skip if the whole pageblock was scanned */</i></td></tr>
<tr><th id="544">544</th><td>	<b>if</b> (blockpfn == end_pfn)</td></tr>
<tr><th id="545">545</th><td>		update_pageblock_skip(cc, valid_page, total_isolated, false);</td></tr>
<tr><th id="546">546</th><td></td></tr>
<tr><th id="547">547</th><td>	cc-&gt;total_free_scanned += nr_scanned;</td></tr>
<tr><th id="548">548</th><td>	<b>if</b> (total_isolated)</td></tr>
<tr><th id="549">549</th><td>		count_compact_events(COMPACTISOLATED, total_isolated);</td></tr>
<tr><th id="550">550</th><td>	<b>return</b> total_isolated;</td></tr>
<tr><th id="551">551</th><td>}</td></tr>
<tr><th id="552">552</th><td></td></tr>
<tr><th id="553">553</th><td><i class="doc">/**</i></td></tr>
<tr><th id="554">554</th><td><i class="doc"> * isolate_freepages_range() - isolate free pages.</i></td></tr>
<tr><th id="555">555</th><td><i class="doc"> *<span class="command"> @start</span>_pfn: The first PFN to start isolating.</i></td></tr>
<tr><th id="556">556</th><td><i class="doc"> *<span class="command"> @end</span>_pfn:   The one-past-last PFN.</i></td></tr>
<tr><th id="557">557</th><td><i class="doc"> *</i></td></tr>
<tr><th id="558">558</th><td><i class="doc"> * Non-free pages, invalid PFNs, or zone boundaries within the</i></td></tr>
<tr><th id="559">559</th><td><i class="doc"> * [start_pfn, end_pfn) range are considered errors, cause function to</i></td></tr>
<tr><th id="560">560</th><td><i class="doc"> * undo its actions and return zero.</i></td></tr>
<tr><th id="561">561</th><td><i class="doc"> *</i></td></tr>
<tr><th id="562">562</th><td><i class="doc"> * Otherwise, function returns one-past-the-last PFN of isolated page</i></td></tr>
<tr><th id="563">563</th><td><i class="doc"> * (which may be greater then end_pfn if end fell in a middle of</i></td></tr>
<tr><th id="564">564</th><td><i class="doc"> * a free page).</i></td></tr>
<tr><th id="565">565</th><td><i class="doc"> */</i></td></tr>
<tr><th id="566">566</th><td><em>unsigned</em> <em>long</em></td></tr>
<tr><th id="567">567</th><td>isolate_freepages_range(<b>struct</b> compact_control *cc,</td></tr>
<tr><th id="568">568</th><td>			<em>unsigned</em> <em>long</em> start_pfn, <em>unsigned</em> <em>long</em> end_pfn)</td></tr>
<tr><th id="569">569</th><td>{</td></tr>
<tr><th id="570">570</th><td>	<em>unsigned</em> <em>long</em> isolated, pfn, block_start_pfn, block_end_pfn;</td></tr>
<tr><th id="571">571</th><td>	LIST_HEAD(freelist);</td></tr>
<tr><th id="572">572</th><td></td></tr>
<tr><th id="573">573</th><td>	pfn = start_pfn;</td></tr>
<tr><th id="574">574</th><td>	block_start_pfn = pageblock_start_pfn(pfn);</td></tr>
<tr><th id="575">575</th><td>	<b>if</b> (block_start_pfn &lt; cc-&gt;zone-&gt;zone_start_pfn)</td></tr>
<tr><th id="576">576</th><td>		block_start_pfn = cc-&gt;zone-&gt;zone_start_pfn;</td></tr>
<tr><th id="577">577</th><td>	block_end_pfn = pageblock_end_pfn(pfn);</td></tr>
<tr><th id="578">578</th><td></td></tr>
<tr><th id="579">579</th><td>	<b>for</b> (; pfn &lt; end_pfn; pfn += isolated,</td></tr>
<tr><th id="580">580</th><td>				block_start_pfn = block_end_pfn,</td></tr>
<tr><th id="581">581</th><td>				block_end_pfn += pageblock_nr_pages) {</td></tr>
<tr><th id="582">582</th><td>		<i>/* Protect pfn from changing by isolate_freepages_block */</i></td></tr>
<tr><th id="583">583</th><td>		<em>unsigned</em> <em>long</em> isolate_start_pfn = pfn;</td></tr>
<tr><th id="584">584</th><td></td></tr>
<tr><th id="585">585</th><td>		block_end_pfn = min(block_end_pfn, end_pfn);</td></tr>
<tr><th id="586">586</th><td></td></tr>
<tr><th id="587">587</th><td>		<i>/*</i></td></tr>
<tr><th id="588">588</th><td><i>		 * pfn could pass the block_end_pfn if isolated freepage</i></td></tr>
<tr><th id="589">589</th><td><i>		 * is more than pageblock order. In this case, we adjust</i></td></tr>
<tr><th id="590">590</th><td><i>		 * scanning range to right one.</i></td></tr>
<tr><th id="591">591</th><td><i>		 */</i></td></tr>
<tr><th id="592">592</th><td>		<b>if</b> (pfn &gt;= block_end_pfn) {</td></tr>
<tr><th id="593">593</th><td>			block_start_pfn = pageblock_start_pfn(pfn);</td></tr>
<tr><th id="594">594</th><td>			block_end_pfn = pageblock_end_pfn(pfn);</td></tr>
<tr><th id="595">595</th><td>			block_end_pfn = min(block_end_pfn, end_pfn);</td></tr>
<tr><th id="596">596</th><td>		}</td></tr>
<tr><th id="597">597</th><td></td></tr>
<tr><th id="598">598</th><td>		<b>if</b> (!pageblock_pfn_to_page(block_start_pfn,</td></tr>
<tr><th id="599">599</th><td>					block_end_pfn, cc-&gt;zone))</td></tr>
<tr><th id="600">600</th><td>			<b>break</b>;</td></tr>
<tr><th id="601">601</th><td></td></tr>
<tr><th id="602">602</th><td>		isolated = isolate_freepages_block(cc, &amp;isolate_start_pfn,</td></tr>
<tr><th id="603">603</th><td>						block_end_pfn, &amp;freelist, true);</td></tr>
<tr><th id="604">604</th><td></td></tr>
<tr><th id="605">605</th><td>		<i>/*</i></td></tr>
<tr><th id="606">606</th><td><i>		 * In strict mode, isolate_freepages_block() returns 0 if</i></td></tr>
<tr><th id="607">607</th><td><i>		 * there are any holes in the block (ie. invalid PFNs or</i></td></tr>
<tr><th id="608">608</th><td><i>		 * non-free pages).</i></td></tr>
<tr><th id="609">609</th><td><i>		 */</i></td></tr>
<tr><th id="610">610</th><td>		<b>if</b> (!isolated)</td></tr>
<tr><th id="611">611</th><td>			<b>break</b>;</td></tr>
<tr><th id="612">612</th><td></td></tr>
<tr><th id="613">613</th><td>		<i>/*</i></td></tr>
<tr><th id="614">614</th><td><i>		 * If we managed to isolate pages, it is always (1 &lt;&lt; n) *</i></td></tr>
<tr><th id="615">615</th><td><i>		 * pageblock_nr_pages for some non-negative n.  (Max order</i></td></tr>
<tr><th id="616">616</th><td><i>		 * page may span two pageblocks).</i></td></tr>
<tr><th id="617">617</th><td><i>		 */</i></td></tr>
<tr><th id="618">618</th><td>	}</td></tr>
<tr><th id="619">619</th><td></td></tr>
<tr><th id="620">620</th><td>	<i>/* __isolate_free_page() does not map the pages */</i></td></tr>
<tr><th id="621">621</th><td>	map_pages(&amp;freelist);</td></tr>
<tr><th id="622">622</th><td></td></tr>
<tr><th id="623">623</th><td>	<b>if</b> (pfn &lt; end_pfn) {</td></tr>
<tr><th id="624">624</th><td>		<i>/* Loop terminated early, cleanup. */</i></td></tr>
<tr><th id="625">625</th><td>		release_freepages(&amp;freelist);</td></tr>
<tr><th id="626">626</th><td>		<b>return</b> <var>0</var>;</td></tr>
<tr><th id="627">627</th><td>	}</td></tr>
<tr><th id="628">628</th><td></td></tr>
<tr><th id="629">629</th><td>	<i>/* We don't use freelists for anything. */</i></td></tr>
<tr><th id="630">630</th><td>	<b>return</b> pfn;</td></tr>
<tr><th id="631">631</th><td>}</td></tr>
<tr><th id="632">632</th><td></td></tr>
<tr><th id="633">633</th><td><i>/* Similar to reclaim, but different enough that they don't share logic */</i></td></tr>
<tr><th id="634">634</th><td><em>static</em> bool too_many_isolated(<b>struct</b> zone *zone)</td></tr>
<tr><th id="635">635</th><td>{</td></tr>
<tr><th id="636">636</th><td>	<em>unsigned</em> <em>long</em> active, inactive, isolated;</td></tr>
<tr><th id="637">637</th><td></td></tr>
<tr><th id="638">638</th><td>	inactive = node_page_state(zone-&gt;zone_pgdat, NR_INACTIVE_FILE) +</td></tr>
<tr><th id="639">639</th><td>			node_page_state(zone-&gt;zone_pgdat, NR_INACTIVE_ANON);</td></tr>
<tr><th id="640">640</th><td>	active = node_page_state(zone-&gt;zone_pgdat, NR_ACTIVE_FILE) +</td></tr>
<tr><th id="641">641</th><td>			node_page_state(zone-&gt;zone_pgdat, NR_ACTIVE_ANON);</td></tr>
<tr><th id="642">642</th><td>	isolated = node_page_state(zone-&gt;zone_pgdat, NR_ISOLATED_FILE) +</td></tr>
<tr><th id="643">643</th><td>			node_page_state(zone-&gt;zone_pgdat, NR_ISOLATED_ANON);</td></tr>
<tr><th id="644">644</th><td></td></tr>
<tr><th id="645">645</th><td>	<b>return</b> isolated &gt; (inactive + active) / <var>2</var>;</td></tr>
<tr><th id="646">646</th><td>}</td></tr>
<tr><th id="647">647</th><td></td></tr>
<tr><th id="648">648</th><td><i class="doc">/**</i></td></tr>
<tr><th id="649">649</th><td><i class="doc"> * isolate_migratepages_block() - isolate all migrate-able pages within</i></td></tr>
<tr><th id="650">650</th><td><i class="doc"> *				  a single pageblock</i></td></tr>
<tr><th id="651">651</th><td><i class="doc"> *<span class="command"> @cc</span><span class="arg">:</span>		Compaction control structure.</i></td></tr>
<tr><th id="652">652</th><td><i class="doc"> *<span class="command"> @low</span>_pfn:	The first PFN to isolate</i></td></tr>
<tr><th id="653">653</th><td><i class="doc"> *<span class="command"> @end</span>_pfn:	The one-past-the-last PFN to isolate, within same pageblock</i></td></tr>
<tr><th id="654">654</th><td><i class="doc"> *<span class="command"> @isolate</span>_mode: Isolation mode to be used.</i></td></tr>
<tr><th id="655">655</th><td><i class="doc"> *</i></td></tr>
<tr><th id="656">656</th><td><i class="doc"> * Isolate all pages that can be migrated from the range specified by</i></td></tr>
<tr><th id="657">657</th><td><i class="doc"> * [low_pfn, end_pfn). The range is expected to be within same pageblock.</i></td></tr>
<tr><th id="658">658</th><td><i class="doc"> * Returns zero if there is a fatal signal pending, otherwise PFN of the</i></td></tr>
<tr><th id="659">659</th><td><i class="doc"> * first page that was not scanned (which may be both less, equal to or more</i></td></tr>
<tr><th id="660">660</th><td><i class="doc"> * than end_pfn).</i></td></tr>
<tr><th id="661">661</th><td><i class="doc"> *</i></td></tr>
<tr><th id="662">662</th><td><i class="doc"> * The pages are isolated on cc-&gt;migratepages list (not required to be empty),</i></td></tr>
<tr><th id="663">663</th><td><i class="doc"> * and cc-&gt;nr_migratepages is updated accordingly. The cc-&gt;migrate_pfn field</i></td></tr>
<tr><th id="664">664</th><td><i class="doc"> * is neither read nor updated.</i></td></tr>
<tr><th id="665">665</th><td><i class="doc"> */</i></td></tr>
<tr><th id="666">666</th><td><em>static</em> <em>unsigned</em> <em>long</em></td></tr>
<tr><th id="667">667</th><td>isolate_migratepages_block(<b>struct</b> compact_control *cc, <em>unsigned</em> <em>long</em> low_pfn,</td></tr>
<tr><th id="668">668</th><td>			<em>unsigned</em> <em>long</em> end_pfn, isolate_mode_t isolate_mode)</td></tr>
<tr><th id="669">669</th><td>{</td></tr>
<tr><th id="670">670</th><td>	<b>struct</b> zone *zone = cc-&gt;zone;</td></tr>
<tr><th id="671">671</th><td>	<em>unsigned</em> <em>long</em> nr_scanned = <var>0</var>, nr_isolated = <var>0</var>;</td></tr>
<tr><th id="672">672</th><td>	<b>struct</b> lruvec *lruvec;</td></tr>
<tr><th id="673">673</th><td>	<em>unsigned</em> <em>long</em> flags = <var>0</var>;</td></tr>
<tr><th id="674">674</th><td>	bool locked = false;</td></tr>
<tr><th id="675">675</th><td>	<b>struct</b> page *page = NULL, *valid_page = NULL;</td></tr>
<tr><th id="676">676</th><td>	<em>unsigned</em> <em>long</em> start_pfn = low_pfn;</td></tr>
<tr><th id="677">677</th><td>	bool skip_on_failure = false;</td></tr>
<tr><th id="678">678</th><td>	<em>unsigned</em> <em>long</em> next_skip_pfn = <var>0</var>;</td></tr>
<tr><th id="679">679</th><td></td></tr>
<tr><th id="680">680</th><td>	<i>/*</i></td></tr>
<tr><th id="681">681</th><td><i>	 * Ensure that there are not too many pages isolated from the LRU</i></td></tr>
<tr><th id="682">682</th><td><i>	 * list by either parallel reclaimers or compaction. If there are,</i></td></tr>
<tr><th id="683">683</th><td><i>	 * delay for some time until fewer pages are isolated</i></td></tr>
<tr><th id="684">684</th><td><i>	 */</i></td></tr>
<tr><th id="685">685</th><td>	<b>while</b> (unlikely(too_many_isolated(zone))) {</td></tr>
<tr><th id="686">686</th><td>		<i>/* async migration should just abort */</i></td></tr>
<tr><th id="687">687</th><td>		<b>if</b> (cc-&gt;mode == MIGRATE_ASYNC)</td></tr>
<tr><th id="688">688</th><td>			<b>return</b> <var>0</var>;</td></tr>
<tr><th id="689">689</th><td></td></tr>
<tr><th id="690">690</th><td>		congestion_wait(BLK_RW_ASYNC, HZ/<var>10</var>);</td></tr>
<tr><th id="691">691</th><td></td></tr>
<tr><th id="692">692</th><td>		<b>if</b> (fatal_signal_pending(current))</td></tr>
<tr><th id="693">693</th><td>			<b>return</b> <var>0</var>;</td></tr>
<tr><th id="694">694</th><td>	}</td></tr>
<tr><th id="695">695</th><td></td></tr>
<tr><th id="696">696</th><td>	<b>if</b> (compact_should_abort(cc))</td></tr>
<tr><th id="697">697</th><td>		<b>return</b> <var>0</var>;</td></tr>
<tr><th id="698">698</th><td></td></tr>
<tr><th id="699">699</th><td>	<b>if</b> (cc-&gt;direct_compaction &amp;&amp; (cc-&gt;mode == MIGRATE_ASYNC)) {</td></tr>
<tr><th id="700">700</th><td>		skip_on_failure = true;</td></tr>
<tr><th id="701">701</th><td>		next_skip_pfn = block_end_pfn(low_pfn, cc-&gt;order);</td></tr>
<tr><th id="702">702</th><td>	}</td></tr>
<tr><th id="703">703</th><td></td></tr>
<tr><th id="704">704</th><td>	<i>/* Time to isolate some pages for migration */</i></td></tr>
<tr><th id="705">705</th><td>	<b>for</b> (; low_pfn &lt; end_pfn; low_pfn++) {</td></tr>
<tr><th id="706">706</th><td></td></tr>
<tr><th id="707">707</th><td>		<b>if</b> (skip_on_failure &amp;&amp; low_pfn &gt;= next_skip_pfn) {</td></tr>
<tr><th id="708">708</th><td>			<i>/*</i></td></tr>
<tr><th id="709">709</th><td><i>			 * We have isolated all migration candidates in the</i></td></tr>
<tr><th id="710">710</th><td><i>			 * previous order-aligned block, and did not skip it due</i></td></tr>
<tr><th id="711">711</th><td><i>			 * to failure. We should migrate the pages now and</i></td></tr>
<tr><th id="712">712</th><td><i>			 * hopefully succeed compaction.</i></td></tr>
<tr><th id="713">713</th><td><i>			 */</i></td></tr>
<tr><th id="714">714</th><td>			<b>if</b> (nr_isolated)</td></tr>
<tr><th id="715">715</th><td>				<b>break</b>;</td></tr>
<tr><th id="716">716</th><td></td></tr>
<tr><th id="717">717</th><td>			<i>/*</i></td></tr>
<tr><th id="718">718</th><td><i>			 * We failed to isolate in the previous order-aligned</i></td></tr>
<tr><th id="719">719</th><td><i>			 * block. Set the new boundary to the end of the</i></td></tr>
<tr><th id="720">720</th><td><i>			 * current block. Note we can't simply increase</i></td></tr>
<tr><th id="721">721</th><td><i>			 * next_skip_pfn by 1 &lt;&lt; order, as low_pfn might have</i></td></tr>
<tr><th id="722">722</th><td><i>			 * been incremented by a higher number due to skipping</i></td></tr>
<tr><th id="723">723</th><td><i>			 * a compound or a high-order buddy page in the</i></td></tr>
<tr><th id="724">724</th><td><i>			 * previous loop iteration.</i></td></tr>
<tr><th id="725">725</th><td><i>			 */</i></td></tr>
<tr><th id="726">726</th><td>			next_skip_pfn = block_end_pfn(low_pfn, cc-&gt;order);</td></tr>
<tr><th id="727">727</th><td>		}</td></tr>
<tr><th id="728">728</th><td></td></tr>
<tr><th id="729">729</th><td>		<i>/*</i></td></tr>
<tr><th id="730">730</th><td><i>		 * Periodically drop the lock (if held) regardless of its</i></td></tr>
<tr><th id="731">731</th><td><i>		 * contention, to give chance to IRQs. Abort async compaction</i></td></tr>
<tr><th id="732">732</th><td><i>		 * if contended.</i></td></tr>
<tr><th id="733">733</th><td><i>		 */</i></td></tr>
<tr><th id="734">734</th><td>		<b>if</b> (!(low_pfn % SWAP_CLUSTER_MAX)</td></tr>
<tr><th id="735">735</th><td>		    &amp;&amp; compact_unlock_should_abort(zone_lru_lock(zone), flags,</td></tr>
<tr><th id="736">736</th><td>								&amp;locked, cc))</td></tr>
<tr><th id="737">737</th><td>			<b>break</b>;</td></tr>
<tr><th id="738">738</th><td></td></tr>
<tr><th id="739">739</th><td>		<b>if</b> (!pfn_valid_within(low_pfn))</td></tr>
<tr><th id="740">740</th><td>			<b>goto</b> isolate_fail;</td></tr>
<tr><th id="741">741</th><td>		nr_scanned++;</td></tr>
<tr><th id="742">742</th><td></td></tr>
<tr><th id="743">743</th><td>		page = pfn_to_page(low_pfn);</td></tr>
<tr><th id="744">744</th><td></td></tr>
<tr><th id="745">745</th><td>		<b>if</b> (!valid_page)</td></tr>
<tr><th id="746">746</th><td>			valid_page = page;</td></tr>
<tr><th id="747">747</th><td></td></tr>
<tr><th id="748">748</th><td>		<i>/*</i></td></tr>
<tr><th id="749">749</th><td><i>		 * Skip if free. We read page order here without zone lock</i></td></tr>
<tr><th id="750">750</th><td><i>		 * which is generally unsafe, but the race window is small and</i></td></tr>
<tr><th id="751">751</th><td><i>		 * the worst thing that can happen is that we skip some</i></td></tr>
<tr><th id="752">752</th><td><i>		 * potential isolation targets.</i></td></tr>
<tr><th id="753">753</th><td><i>		 */</i></td></tr>
<tr><th id="754">754</th><td>		<b>if</b> (PageBuddy(page)) {</td></tr>
<tr><th id="755">755</th><td>			<em>unsigned</em> <em>long</em> freepage_order = page_order_unsafe(page);</td></tr>
<tr><th id="756">756</th><td></td></tr>
<tr><th id="757">757</th><td>			<i>/*</i></td></tr>
<tr><th id="758">758</th><td><i>			 * Without lock, we cannot be sure that what we got is</i></td></tr>
<tr><th id="759">759</th><td><i>			 * a valid page order. Consider only values in the</i></td></tr>
<tr><th id="760">760</th><td><i>			 * valid order range to prevent low_pfn overflow.</i></td></tr>
<tr><th id="761">761</th><td><i>			 */</i></td></tr>
<tr><th id="762">762</th><td>			<b>if</b> (freepage_order &gt; <var>0</var> &amp;&amp; freepage_order &lt; MAX_ORDER)</td></tr>
<tr><th id="763">763</th><td>				low_pfn += (<var>1UL</var> &lt;&lt; freepage_order) - <var>1</var>;</td></tr>
<tr><th id="764">764</th><td>			<b>continue</b>;</td></tr>
<tr><th id="765">765</th><td>		}</td></tr>
<tr><th id="766">766</th><td></td></tr>
<tr><th id="767">767</th><td>		<i>/*</i></td></tr>
<tr><th id="768">768</th><td><i>		 * Regardless of being on LRU, compound pages such as THP and</i></td></tr>
<tr><th id="769">769</th><td><i>		 * hugetlbfs are not to be compacted. We can potentially save</i></td></tr>
<tr><th id="770">770</th><td><i>		 * a lot of iterations if we skip them at once. The check is</i></td></tr>
<tr><th id="771">771</th><td><i>		 * racy, but we can consider only valid values and the only</i></td></tr>
<tr><th id="772">772</th><td><i>		 * danger is skipping too much.</i></td></tr>
<tr><th id="773">773</th><td><i>		 */</i></td></tr>
<tr><th id="774">774</th><td>		<b>if</b> (PageCompound(page)) {</td></tr>
<tr><th id="775">775</th><td>			<em>unsigned</em> <em>int</em> comp_order = compound_order(page);</td></tr>
<tr><th id="776">776</th><td></td></tr>
<tr><th id="777">777</th><td>			<b>if</b> (likely(comp_order &lt; MAX_ORDER))</td></tr>
<tr><th id="778">778</th><td>				low_pfn += (<var>1UL</var> &lt;&lt; comp_order) - <var>1</var>;</td></tr>
<tr><th id="779">779</th><td></td></tr>
<tr><th id="780">780</th><td>			<b>goto</b> isolate_fail;</td></tr>
<tr><th id="781">781</th><td>		}</td></tr>
<tr><th id="782">782</th><td></td></tr>
<tr><th id="783">783</th><td>		<i>/*</i></td></tr>
<tr><th id="784">784</th><td><i>		 * Check may be lockless but that's ok as we recheck later.</i></td></tr>
<tr><th id="785">785</th><td><i>		 * It's possible to migrate LRU and non-lru movable pages.</i></td></tr>
<tr><th id="786">786</th><td><i>		 * Skip any other type of page</i></td></tr>
<tr><th id="787">787</th><td><i>		 */</i></td></tr>
<tr><th id="788">788</th><td>		<b>if</b> (!PageLRU(page)) {</td></tr>
<tr><th id="789">789</th><td>			<i>/*</i></td></tr>
<tr><th id="790">790</th><td><i>			 * __PageMovable can return false positive so we need</i></td></tr>
<tr><th id="791">791</th><td><i>			 * to verify it under page_lock.</i></td></tr>
<tr><th id="792">792</th><td><i>			 */</i></td></tr>
<tr><th id="793">793</th><td>			<b>if</b> (unlikely(__PageMovable(page)) &amp;&amp;</td></tr>
<tr><th id="794">794</th><td>					!PageIsolated(page)) {</td></tr>
<tr><th id="795">795</th><td>				<b>if</b> (locked) {</td></tr>
<tr><th id="796">796</th><td>					spin_unlock_irqrestore(zone_lru_lock(zone),</td></tr>
<tr><th id="797">797</th><td>									flags);</td></tr>
<tr><th id="798">798</th><td>					locked = false;</td></tr>
<tr><th id="799">799</th><td>				}</td></tr>
<tr><th id="800">800</th><td></td></tr>
<tr><th id="801">801</th><td>				<b>if</b> (!isolate_movable_page(page, isolate_mode))</td></tr>
<tr><th id="802">802</th><td>					<b>goto</b> isolate_success;</td></tr>
<tr><th id="803">803</th><td>			}</td></tr>
<tr><th id="804">804</th><td></td></tr>
<tr><th id="805">805</th><td>			<b>goto</b> isolate_fail;</td></tr>
<tr><th id="806">806</th><td>		}</td></tr>
<tr><th id="807">807</th><td></td></tr>
<tr><th id="808">808</th><td>		<i>/*</i></td></tr>
<tr><th id="809">809</th><td><i>		 * Migration will fail if an anonymous page is pinned in memory,</i></td></tr>
<tr><th id="810">810</th><td><i>		 * so avoid taking lru_lock and isolating it unnecessarily in an</i></td></tr>
<tr><th id="811">811</th><td><i>		 * admittedly racy check.</i></td></tr>
<tr><th id="812">812</th><td><i>		 */</i></td></tr>
<tr><th id="813">813</th><td>		<b>if</b> (!page_mapping(page) &amp;&amp;</td></tr>
<tr><th id="814">814</th><td>		    page_count(page) &gt; page_mapcount(page))</td></tr>
<tr><th id="815">815</th><td>			<b>goto</b> isolate_fail;</td></tr>
<tr><th id="816">816</th><td></td></tr>
<tr><th id="817">817</th><td>		<i>/*</i></td></tr>
<tr><th id="818">818</th><td><i>		 * Only allow to migrate anonymous pages in GFP_NOFS context</i></td></tr>
<tr><th id="819">819</th><td><i>		 * because those do not depend on fs locks.</i></td></tr>
<tr><th id="820">820</th><td><i>		 */</i></td></tr>
<tr><th id="821">821</th><td>		<b>if</b> (!(cc-&gt;gfp_mask &amp; __GFP_FS) &amp;&amp; page_mapping(page))</td></tr>
<tr><th id="822">822</th><td>			<b>goto</b> isolate_fail;</td></tr>
<tr><th id="823">823</th><td></td></tr>
<tr><th id="824">824</th><td>		<i>/* If we already hold the lock, we can skip some rechecking */</i></td></tr>
<tr><th id="825">825</th><td>		<b>if</b> (!locked) {</td></tr>
<tr><th id="826">826</th><td>			locked = compact_trylock_irqsave(zone_lru_lock(zone),</td></tr>
<tr><th id="827">827</th><td>								&amp;flags, cc);</td></tr>
<tr><th id="828">828</th><td>			<b>if</b> (!locked)</td></tr>
<tr><th id="829">829</th><td>				<b>break</b>;</td></tr>
<tr><th id="830">830</th><td></td></tr>
<tr><th id="831">831</th><td>			<i>/* Recheck PageLRU and PageCompound under lock */</i></td></tr>
<tr><th id="832">832</th><td>			<b>if</b> (!PageLRU(page))</td></tr>
<tr><th id="833">833</th><td>				<b>goto</b> isolate_fail;</td></tr>
<tr><th id="834">834</th><td></td></tr>
<tr><th id="835">835</th><td>			<i>/*</i></td></tr>
<tr><th id="836">836</th><td><i>			 * Page become compound since the non-locked check,</i></td></tr>
<tr><th id="837">837</th><td><i>			 * and it's on LRU. It can only be a THP so the order</i></td></tr>
<tr><th id="838">838</th><td><i>			 * is safe to read and it's 0 for tail pages.</i></td></tr>
<tr><th id="839">839</th><td><i>			 */</i></td></tr>
<tr><th id="840">840</th><td>			<b>if</b> (unlikely(PageCompound(page))) {</td></tr>
<tr><th id="841">841</th><td>				low_pfn += (<var>1UL</var> &lt;&lt; compound_order(page)) - <var>1</var>;</td></tr>
<tr><th id="842">842</th><td>				<b>goto</b> isolate_fail;</td></tr>
<tr><th id="843">843</th><td>			}</td></tr>
<tr><th id="844">844</th><td>		}</td></tr>
<tr><th id="845">845</th><td></td></tr>
<tr><th id="846">846</th><td>		lruvec = mem_cgroup_page_lruvec(page, zone-&gt;zone_pgdat);</td></tr>
<tr><th id="847">847</th><td></td></tr>
<tr><th id="848">848</th><td>		<i>/* Try isolate the page */</i></td></tr>
<tr><th id="849">849</th><td>		<b>if</b> (__isolate_lru_page(page, isolate_mode) != <var>0</var>)</td></tr>
<tr><th id="850">850</th><td>			<b>goto</b> isolate_fail;</td></tr>
<tr><th id="851">851</th><td></td></tr>
<tr><th id="852">852</th><td>		VM_BUG_ON_PAGE(PageCompound(page), page);</td></tr>
<tr><th id="853">853</th><td></td></tr>
<tr><th id="854">854</th><td>		<i>/* Successfully isolated */</i></td></tr>
<tr><th id="855">855</th><td>		del_page_from_lru_list(page, lruvec, page_lru(page));</td></tr>
<tr><th id="856">856</th><td>		inc_node_page_state(page,</td></tr>
<tr><th id="857">857</th><td>				NR_ISOLATED_ANON + page_is_file_cache(page));</td></tr>
<tr><th id="858">858</th><td></td></tr>
<tr><th id="859">859</th><td>isolate_success:</td></tr>
<tr><th id="860">860</th><td>		list_add(&amp;page-&gt;lru, &amp;cc-&gt;migratepages);</td></tr>
<tr><th id="861">861</th><td>		cc-&gt;nr_migratepages++;</td></tr>
<tr><th id="862">862</th><td>		nr_isolated++;</td></tr>
<tr><th id="863">863</th><td></td></tr>
<tr><th id="864">864</th><td>		<i>/*</i></td></tr>
<tr><th id="865">865</th><td><i>		 * Record where we could have freed pages by migration and not</i></td></tr>
<tr><th id="866">866</th><td><i>		 * yet flushed them to buddy allocator.</i></td></tr>
<tr><th id="867">867</th><td><i>		 * - this is the lowest page that was isolated and likely be</i></td></tr>
<tr><th id="868">868</th><td><i>		 * then freed by migration.</i></td></tr>
<tr><th id="869">869</th><td><i>		 */</i></td></tr>
<tr><th id="870">870</th><td>		<b>if</b> (!cc-&gt;last_migrated_pfn)</td></tr>
<tr><th id="871">871</th><td>			cc-&gt;last_migrated_pfn = low_pfn;</td></tr>
<tr><th id="872">872</th><td></td></tr>
<tr><th id="873">873</th><td>		<i>/* Avoid isolating too much */</i></td></tr>
<tr><th id="874">874</th><td>		<b>if</b> (cc-&gt;nr_migratepages == COMPACT_CLUSTER_MAX) {</td></tr>
<tr><th id="875">875</th><td>			++low_pfn;</td></tr>
<tr><th id="876">876</th><td>			<b>break</b>;</td></tr>
<tr><th id="877">877</th><td>		}</td></tr>
<tr><th id="878">878</th><td></td></tr>
<tr><th id="879">879</th><td>		<b>continue</b>;</td></tr>
<tr><th id="880">880</th><td>isolate_fail:</td></tr>
<tr><th id="881">881</th><td>		<b>if</b> (!skip_on_failure)</td></tr>
<tr><th id="882">882</th><td>			<b>continue</b>;</td></tr>
<tr><th id="883">883</th><td></td></tr>
<tr><th id="884">884</th><td>		<i>/*</i></td></tr>
<tr><th id="885">885</th><td><i>		 * We have isolated some pages, but then failed. Release them</i></td></tr>
<tr><th id="886">886</th><td><i>		 * instead of migrating, as we cannot form the cc-&gt;order buddy</i></td></tr>
<tr><th id="887">887</th><td><i>		 * page anyway.</i></td></tr>
<tr><th id="888">888</th><td><i>		 */</i></td></tr>
<tr><th id="889">889</th><td>		<b>if</b> (nr_isolated) {</td></tr>
<tr><th id="890">890</th><td>			<b>if</b> (locked) {</td></tr>
<tr><th id="891">891</th><td>				spin_unlock_irqrestore(zone_lru_lock(zone), flags);</td></tr>
<tr><th id="892">892</th><td>				locked = false;</td></tr>
<tr><th id="893">893</th><td>			}</td></tr>
<tr><th id="894">894</th><td>			putback_movable_pages(&amp;cc-&gt;migratepages);</td></tr>
<tr><th id="895">895</th><td>			cc-&gt;nr_migratepages = <var>0</var>;</td></tr>
<tr><th id="896">896</th><td>			cc-&gt;last_migrated_pfn = <var>0</var>;</td></tr>
<tr><th id="897">897</th><td>			nr_isolated = <var>0</var>;</td></tr>
<tr><th id="898">898</th><td>		}</td></tr>
<tr><th id="899">899</th><td></td></tr>
<tr><th id="900">900</th><td>		<b>if</b> (low_pfn &lt; next_skip_pfn) {</td></tr>
<tr><th id="901">901</th><td>			low_pfn = next_skip_pfn - <var>1</var>;</td></tr>
<tr><th id="902">902</th><td>			<i>/*</i></td></tr>
<tr><th id="903">903</th><td><i>			 * The check near the loop beginning would have updated</i></td></tr>
<tr><th id="904">904</th><td><i>			 * next_skip_pfn too, but this is a bit simpler.</i></td></tr>
<tr><th id="905">905</th><td><i>			 */</i></td></tr>
<tr><th id="906">906</th><td>			next_skip_pfn += <var>1UL</var> &lt;&lt; cc-&gt;order;</td></tr>
<tr><th id="907">907</th><td>		}</td></tr>
<tr><th id="908">908</th><td>	}</td></tr>
<tr><th id="909">909</th><td></td></tr>
<tr><th id="910">910</th><td>	<i>/*</i></td></tr>
<tr><th id="911">911</th><td><i>	 * The PageBuddy() check could have potentially brought us outside</i></td></tr>
<tr><th id="912">912</th><td><i>	 * the range to be scanned.</i></td></tr>
<tr><th id="913">913</th><td><i>	 */</i></td></tr>
<tr><th id="914">914</th><td>	<b>if</b> (unlikely(low_pfn &gt; end_pfn))</td></tr>
<tr><th id="915">915</th><td>		low_pfn = end_pfn;</td></tr>
<tr><th id="916">916</th><td></td></tr>
<tr><th id="917">917</th><td>	<b>if</b> (locked)</td></tr>
<tr><th id="918">918</th><td>		spin_unlock_irqrestore(zone_lru_lock(zone), flags);</td></tr>
<tr><th id="919">919</th><td></td></tr>
<tr><th id="920">920</th><td>	<i>/*</i></td></tr>
<tr><th id="921">921</th><td><i>	 * Update the pageblock-skip information and cached scanner pfn,</i></td></tr>
<tr><th id="922">922</th><td><i>	 * if the whole pageblock was scanned without isolating any page.</i></td></tr>
<tr><th id="923">923</th><td><i>	 */</i></td></tr>
<tr><th id="924">924</th><td>	<b>if</b> (low_pfn == end_pfn)</td></tr>
<tr><th id="925">925</th><td>		update_pageblock_skip(cc, valid_page, nr_isolated, true);</td></tr>
<tr><th id="926">926</th><td></td></tr>
<tr><th id="927">927</th><td>	trace_mm_compaction_isolate_migratepages(start_pfn, low_pfn,</td></tr>
<tr><th id="928">928</th><td>						nr_scanned, nr_isolated);</td></tr>
<tr><th id="929">929</th><td></td></tr>
<tr><th id="930">930</th><td>	cc-&gt;total_migrate_scanned += nr_scanned;</td></tr>
<tr><th id="931">931</th><td>	<b>if</b> (nr_isolated)</td></tr>
<tr><th id="932">932</th><td>		count_compact_events(COMPACTISOLATED, nr_isolated);</td></tr>
<tr><th id="933">933</th><td></td></tr>
<tr><th id="934">934</th><td>	<b>return</b> low_pfn;</td></tr>
<tr><th id="935">935</th><td>}</td></tr>
<tr><th id="936">936</th><td></td></tr>
<tr><th id="937">937</th><td><i class="doc">/**</i></td></tr>
<tr><th id="938">938</th><td><i class="doc"> * isolate_migratepages_range() - isolate migrate-able pages in a PFN range</i></td></tr>
<tr><th id="939">939</th><td><i class="doc"> *<span class="command"> @cc</span><span class="arg">:</span>        Compaction control structure.</i></td></tr>
<tr><th id="940">940</th><td><i class="doc"> *<span class="command"> @start</span>_pfn: The first PFN to start isolating.</i></td></tr>
<tr><th id="941">941</th><td><i class="doc"> *<span class="command"> @end</span>_pfn:   The one-past-last PFN.</i></td></tr>
<tr><th id="942">942</th><td><i class="doc"> *</i></td></tr>
<tr><th id="943">943</th><td><i class="doc"> * Returns zero if isolation fails fatally due to e.g. pending signal.</i></td></tr>
<tr><th id="944">944</th><td><i class="doc"> * Otherwise, function returns one-past-the-last PFN of isolated page</i></td></tr>
<tr><th id="945">945</th><td><i class="doc"> * (which may be greater than end_pfn if end fell in a middle of a THP page).</i></td></tr>
<tr><th id="946">946</th><td><i class="doc"> */</i></td></tr>
<tr><th id="947">947</th><td><em>unsigned</em> <em>long</em></td></tr>
<tr><th id="948">948</th><td>isolate_migratepages_range(<b>struct</b> compact_control *cc, <em>unsigned</em> <em>long</em> start_pfn,</td></tr>
<tr><th id="949">949</th><td>							<em>unsigned</em> <em>long</em> end_pfn)</td></tr>
<tr><th id="950">950</th><td>{</td></tr>
<tr><th id="951">951</th><td>	<em>unsigned</em> <em>long</em> pfn, block_start_pfn, block_end_pfn;</td></tr>
<tr><th id="952">952</th><td></td></tr>
<tr><th id="953">953</th><td>	<i>/* Scan block by block. First and last block may be incomplete */</i></td></tr>
<tr><th id="954">954</th><td>	pfn = start_pfn;</td></tr>
<tr><th id="955">955</th><td>	block_start_pfn = pageblock_start_pfn(pfn);</td></tr>
<tr><th id="956">956</th><td>	<b>if</b> (block_start_pfn &lt; cc-&gt;zone-&gt;zone_start_pfn)</td></tr>
<tr><th id="957">957</th><td>		block_start_pfn = cc-&gt;zone-&gt;zone_start_pfn;</td></tr>
<tr><th id="958">958</th><td>	block_end_pfn = pageblock_end_pfn(pfn);</td></tr>
<tr><th id="959">959</th><td></td></tr>
<tr><th id="960">960</th><td>	<b>for</b> (; pfn &lt; end_pfn; pfn = block_end_pfn,</td></tr>
<tr><th id="961">961</th><td>				block_start_pfn = block_end_pfn,</td></tr>
<tr><th id="962">962</th><td>				block_end_pfn += pageblock_nr_pages) {</td></tr>
<tr><th id="963">963</th><td></td></tr>
<tr><th id="964">964</th><td>		block_end_pfn = min(block_end_pfn, end_pfn);</td></tr>
<tr><th id="965">965</th><td></td></tr>
<tr><th id="966">966</th><td>		<b>if</b> (!pageblock_pfn_to_page(block_start_pfn,</td></tr>
<tr><th id="967">967</th><td>					block_end_pfn, cc-&gt;zone))</td></tr>
<tr><th id="968">968</th><td>			<b>continue</b>;</td></tr>
<tr><th id="969">969</th><td></td></tr>
<tr><th id="970">970</th><td>		pfn = isolate_migratepages_block(cc, pfn, block_end_pfn,</td></tr>
<tr><th id="971">971</th><td>							ISOLATE_UNEVICTABLE);</td></tr>
<tr><th id="972">972</th><td></td></tr>
<tr><th id="973">973</th><td>		<b>if</b> (!pfn)</td></tr>
<tr><th id="974">974</th><td>			<b>break</b>;</td></tr>
<tr><th id="975">975</th><td></td></tr>
<tr><th id="976">976</th><td>		<b>if</b> (cc-&gt;nr_migratepages == COMPACT_CLUSTER_MAX)</td></tr>
<tr><th id="977">977</th><td>			<b>break</b>;</td></tr>
<tr><th id="978">978</th><td>	}</td></tr>
<tr><th id="979">979</th><td></td></tr>
<tr><th id="980">980</th><td>	<b>return</b> pfn;</td></tr>
<tr><th id="981">981</th><td>}</td></tr>
<tr><th id="982">982</th><td></td></tr>
<tr><th id="983">983</th><td><u>#<span data-ppcond="42">endif</span> /* CONFIG_COMPACTION || CONFIG_CMA */</u></td></tr>
<tr><th id="984">984</th><td><u>#<span data-ppcond="984">ifdef</span> <span class="macro" data-ref="_M/CONFIG_COMPACTION">CONFIG_COMPACTION</span></u></td></tr>
<tr><th id="985">985</th><td></td></tr>
<tr><th id="986">986</th><td><em>static</em> bool suitable_migration_source(<b>struct</b> compact_control *cc,</td></tr>
<tr><th id="987">987</th><td>							<b>struct</b> page *page)</td></tr>
<tr><th id="988">988</th><td>{</td></tr>
<tr><th id="989">989</th><td>	<em>int</em> block_mt;</td></tr>
<tr><th id="990">990</th><td></td></tr>
<tr><th id="991">991</th><td>	<b>if</b> ((cc-&gt;mode != MIGRATE_ASYNC) || !cc-&gt;direct_compaction)</td></tr>
<tr><th id="992">992</th><td>		<b>return</b> true;</td></tr>
<tr><th id="993">993</th><td></td></tr>
<tr><th id="994">994</th><td>	block_mt = get_pageblock_migratetype(page);</td></tr>
<tr><th id="995">995</th><td></td></tr>
<tr><th id="996">996</th><td>	<b>if</b> (cc-&gt;migratetype == MIGRATE_MOVABLE)</td></tr>
<tr><th id="997">997</th><td>		<b>return</b> is_migrate_movable(block_mt);</td></tr>
<tr><th id="998">998</th><td>	<b>else</b></td></tr>
<tr><th id="999">999</th><td>		<b>return</b> block_mt == cc-&gt;migratetype;</td></tr>
<tr><th id="1000">1000</th><td>}</td></tr>
<tr><th id="1001">1001</th><td></td></tr>
<tr><th id="1002">1002</th><td><i>/* Returns true if the page is within a block suitable for migration to */</i></td></tr>
<tr><th id="1003">1003</th><td><em>static</em> bool suitable_migration_target(<b>struct</b> compact_control *cc,</td></tr>
<tr><th id="1004">1004</th><td>							<b>struct</b> page *page)</td></tr>
<tr><th id="1005">1005</th><td>{</td></tr>
<tr><th id="1006">1006</th><td>	<i>/* If the page is a large free page, then disallow migration */</i></td></tr>
<tr><th id="1007">1007</th><td>	<b>if</b> (PageBuddy(page)) {</td></tr>
<tr><th id="1008">1008</th><td>		<i>/*</i></td></tr>
<tr><th id="1009">1009</th><td><i>		 * We are checking page_order without zone-&gt;lock taken. But</i></td></tr>
<tr><th id="1010">1010</th><td><i>		 * the only small danger is that we skip a potentially suitable</i></td></tr>
<tr><th id="1011">1011</th><td><i>		 * pageblock, so it's not worth to check order for valid range.</i></td></tr>
<tr><th id="1012">1012</th><td><i>		 */</i></td></tr>
<tr><th id="1013">1013</th><td>		<b>if</b> (page_order_unsafe(page) &gt;= pageblock_order)</td></tr>
<tr><th id="1014">1014</th><td>			<b>return</b> false;</td></tr>
<tr><th id="1015">1015</th><td>	}</td></tr>
<tr><th id="1016">1016</th><td></td></tr>
<tr><th id="1017">1017</th><td>	<b>if</b> (cc-&gt;ignore_block_suitable)</td></tr>
<tr><th id="1018">1018</th><td>		<b>return</b> true;</td></tr>
<tr><th id="1019">1019</th><td></td></tr>
<tr><th id="1020">1020</th><td>	<i>/* If the block is MIGRATE_MOVABLE or MIGRATE_CMA, allow migration */</i></td></tr>
<tr><th id="1021">1021</th><td>	<b>if</b> (is_migrate_movable(get_pageblock_migratetype(page)))</td></tr>
<tr><th id="1022">1022</th><td>		<b>return</b> true;</td></tr>
<tr><th id="1023">1023</th><td></td></tr>
<tr><th id="1024">1024</th><td>	<i>/* Otherwise skip the block */</i></td></tr>
<tr><th id="1025">1025</th><td>	<b>return</b> false;</td></tr>
<tr><th id="1026">1026</th><td>}</td></tr>
<tr><th id="1027">1027</th><td></td></tr>
<tr><th id="1028">1028</th><td><i>/*</i></td></tr>
<tr><th id="1029">1029</th><td><i> * Test whether the free scanner has reached the same or lower pageblock than</i></td></tr>
<tr><th id="1030">1030</th><td><i> * the migration scanner, and compaction should thus terminate.</i></td></tr>
<tr><th id="1031">1031</th><td><i> */</i></td></tr>
<tr><th id="1032">1032</th><td><em>static</em> <b>inline</b> bool compact_scanners_met(<b>struct</b> compact_control *cc)</td></tr>
<tr><th id="1033">1033</th><td>{</td></tr>
<tr><th id="1034">1034</th><td>	<b>return</b> (cc-&gt;free_pfn &gt;&gt; pageblock_order)</td></tr>
<tr><th id="1035">1035</th><td>		&lt;= (cc-&gt;migrate_pfn &gt;&gt; pageblock_order);</td></tr>
<tr><th id="1036">1036</th><td>}</td></tr>
<tr><th id="1037">1037</th><td></td></tr>
<tr><th id="1038">1038</th><td><i>/*</i></td></tr>
<tr><th id="1039">1039</th><td><i> * Based on information in the current compact_control, find blocks</i></td></tr>
<tr><th id="1040">1040</th><td><i> * suitable for isolating free pages from and then isolate them.</i></td></tr>
<tr><th id="1041">1041</th><td><i> */</i></td></tr>
<tr><th id="1042">1042</th><td><em>static</em> <em>void</em> isolate_freepages(<b>struct</b> compact_control *cc)</td></tr>
<tr><th id="1043">1043</th><td>{</td></tr>
<tr><th id="1044">1044</th><td>	<b>struct</b> zone *zone = cc-&gt;zone;</td></tr>
<tr><th id="1045">1045</th><td>	<b>struct</b> page *page;</td></tr>
<tr><th id="1046">1046</th><td>	<em>unsigned</em> <em>long</em> block_start_pfn;	<i>/* start of current pageblock */</i></td></tr>
<tr><th id="1047">1047</th><td>	<em>unsigned</em> <em>long</em> isolate_start_pfn; <i>/* exact pfn we start at */</i></td></tr>
<tr><th id="1048">1048</th><td>	<em>unsigned</em> <em>long</em> block_end_pfn;	<i>/* end of current pageblock */</i></td></tr>
<tr><th id="1049">1049</th><td>	<em>unsigned</em> <em>long</em> low_pfn;	     <i>/* lowest pfn scanner is able to scan */</i></td></tr>
<tr><th id="1050">1050</th><td>	<b>struct</b> list_head *freelist = &amp;cc-&gt;freepages;</td></tr>
<tr><th id="1051">1051</th><td></td></tr>
<tr><th id="1052">1052</th><td>	<i>/*</i></td></tr>
<tr><th id="1053">1053</th><td><i>	 * Initialise the free scanner. The starting point is where we last</i></td></tr>
<tr><th id="1054">1054</th><td><i>	 * successfully isolated from, zone-cached value, or the end of the</i></td></tr>
<tr><th id="1055">1055</th><td><i>	 * zone when isolating for the first time. For looping we also need</i></td></tr>
<tr><th id="1056">1056</th><td><i>	 * this pfn aligned down to the pageblock boundary, because we do</i></td></tr>
<tr><th id="1057">1057</th><td><i>	 * block_start_pfn -= pageblock_nr_pages in the for loop.</i></td></tr>
<tr><th id="1058">1058</th><td><i>	 * For ending point, take care when isolating in last pageblock of a</i></td></tr>
<tr><th id="1059">1059</th><td><i>	 * a zone which ends in the middle of a pageblock.</i></td></tr>
<tr><th id="1060">1060</th><td><i>	 * The low boundary is the end of the pageblock the migration scanner</i></td></tr>
<tr><th id="1061">1061</th><td><i>	 * is using.</i></td></tr>
<tr><th id="1062">1062</th><td><i>	 */</i></td></tr>
<tr><th id="1063">1063</th><td>	isolate_start_pfn = cc-&gt;free_pfn;</td></tr>
<tr><th id="1064">1064</th><td>	block_start_pfn = pageblock_start_pfn(cc-&gt;free_pfn);</td></tr>
<tr><th id="1065">1065</th><td>	block_end_pfn = min(block_start_pfn + pageblock_nr_pages,</td></tr>
<tr><th id="1066">1066</th><td>						zone_end_pfn(zone));</td></tr>
<tr><th id="1067">1067</th><td>	low_pfn = pageblock_end_pfn(cc-&gt;migrate_pfn);</td></tr>
<tr><th id="1068">1068</th><td></td></tr>
<tr><th id="1069">1069</th><td>	<i>/*</i></td></tr>
<tr><th id="1070">1070</th><td><i>	 * Isolate free pages until enough are available to migrate the</i></td></tr>
<tr><th id="1071">1071</th><td><i>	 * pages on cc-&gt;migratepages. We stop searching if the migrate</i></td></tr>
<tr><th id="1072">1072</th><td><i>	 * and free page scanners meet or enough free pages are isolated.</i></td></tr>
<tr><th id="1073">1073</th><td><i>	 */</i></td></tr>
<tr><th id="1074">1074</th><td>	<b>for</b> (; block_start_pfn &gt;= low_pfn;</td></tr>
<tr><th id="1075">1075</th><td>				block_end_pfn = block_start_pfn,</td></tr>
<tr><th id="1076">1076</th><td>				block_start_pfn -= pageblock_nr_pages,</td></tr>
<tr><th id="1077">1077</th><td>				isolate_start_pfn = block_start_pfn) {</td></tr>
<tr><th id="1078">1078</th><td>		<i>/*</i></td></tr>
<tr><th id="1079">1079</th><td><i>		 * This can iterate a massively long zone without finding any</i></td></tr>
<tr><th id="1080">1080</th><td><i>		 * suitable migration targets, so periodically check if we need</i></td></tr>
<tr><th id="1081">1081</th><td><i>		 * to schedule, or even abort async compaction.</i></td></tr>
<tr><th id="1082">1082</th><td><i>		 */</i></td></tr>
<tr><th id="1083">1083</th><td>		<b>if</b> (!(block_start_pfn % (SWAP_CLUSTER_MAX * pageblock_nr_pages))</td></tr>
<tr><th id="1084">1084</th><td>						&amp;&amp; compact_should_abort(cc))</td></tr>
<tr><th id="1085">1085</th><td>			<b>break</b>;</td></tr>
<tr><th id="1086">1086</th><td></td></tr>
<tr><th id="1087">1087</th><td>		page = pageblock_pfn_to_page(block_start_pfn, block_end_pfn,</td></tr>
<tr><th id="1088">1088</th><td>									zone);</td></tr>
<tr><th id="1089">1089</th><td>		<b>if</b> (!page)</td></tr>
<tr><th id="1090">1090</th><td>			<b>continue</b>;</td></tr>
<tr><th id="1091">1091</th><td></td></tr>
<tr><th id="1092">1092</th><td>		<i>/* Check the block is suitable for migration */</i></td></tr>
<tr><th id="1093">1093</th><td>		<b>if</b> (!suitable_migration_target(cc, page))</td></tr>
<tr><th id="1094">1094</th><td>			<b>continue</b>;</td></tr>
<tr><th id="1095">1095</th><td></td></tr>
<tr><th id="1096">1096</th><td>		<i>/* If isolation recently failed, do not retry */</i></td></tr>
<tr><th id="1097">1097</th><td>		<b>if</b> (!isolation_suitable(cc, page))</td></tr>
<tr><th id="1098">1098</th><td>			<b>continue</b>;</td></tr>
<tr><th id="1099">1099</th><td></td></tr>
<tr><th id="1100">1100</th><td>		<i>/* Found a block suitable for isolating free pages from. */</i></td></tr>
<tr><th id="1101">1101</th><td>		isolate_freepages_block(cc, &amp;isolate_start_pfn, block_end_pfn,</td></tr>
<tr><th id="1102">1102</th><td>					freelist, false);</td></tr>
<tr><th id="1103">1103</th><td></td></tr>
<tr><th id="1104">1104</th><td>		<i>/*</i></td></tr>
<tr><th id="1105">1105</th><td><i>		 * If we isolated enough freepages, or aborted due to lock</i></td></tr>
<tr><th id="1106">1106</th><td><i>		 * contention, terminate.</i></td></tr>
<tr><th id="1107">1107</th><td><i>		 */</i></td></tr>
<tr><th id="1108">1108</th><td>		<b>if</b> ((cc-&gt;nr_freepages &gt;= cc-&gt;nr_migratepages)</td></tr>
<tr><th id="1109">1109</th><td>							|| cc-&gt;contended) {</td></tr>
<tr><th id="1110">1110</th><td>			<b>if</b> (isolate_start_pfn &gt;= block_end_pfn) {</td></tr>
<tr><th id="1111">1111</th><td>				<i>/*</i></td></tr>
<tr><th id="1112">1112</th><td><i>				 * Restart at previous pageblock if more</i></td></tr>
<tr><th id="1113">1113</th><td><i>				 * freepages can be isolated next time.</i></td></tr>
<tr><th id="1114">1114</th><td><i>				 */</i></td></tr>
<tr><th id="1115">1115</th><td>				isolate_start_pfn =</td></tr>
<tr><th id="1116">1116</th><td>					block_start_pfn - pageblock_nr_pages;</td></tr>
<tr><th id="1117">1117</th><td>			}</td></tr>
<tr><th id="1118">1118</th><td>			<b>break</b>;</td></tr>
<tr><th id="1119">1119</th><td>		} <b>else</b> <b>if</b> (isolate_start_pfn &lt; block_end_pfn) {</td></tr>
<tr><th id="1120">1120</th><td>			<i>/*</i></td></tr>
<tr><th id="1121">1121</th><td><i>			 * If isolation failed early, do not continue</i></td></tr>
<tr><th id="1122">1122</th><td><i>			 * needlessly.</i></td></tr>
<tr><th id="1123">1123</th><td><i>			 */</i></td></tr>
<tr><th id="1124">1124</th><td>			<b>break</b>;</td></tr>
<tr><th id="1125">1125</th><td>		}</td></tr>
<tr><th id="1126">1126</th><td>	}</td></tr>
<tr><th id="1127">1127</th><td></td></tr>
<tr><th id="1128">1128</th><td>	<i>/* __isolate_free_page() does not map the pages */</i></td></tr>
<tr><th id="1129">1129</th><td>	map_pages(freelist);</td></tr>
<tr><th id="1130">1130</th><td></td></tr>
<tr><th id="1131">1131</th><td>	<i>/*</i></td></tr>
<tr><th id="1132">1132</th><td><i>	 * Record where the free scanner will restart next time. Either we</i></td></tr>
<tr><th id="1133">1133</th><td><i>	 * broke from the loop and set isolate_start_pfn based on the last</i></td></tr>
<tr><th id="1134">1134</th><td><i>	 * call to isolate_freepages_block(), or we met the migration scanner</i></td></tr>
<tr><th id="1135">1135</th><td><i>	 * and the loop terminated due to isolate_start_pfn &lt; low_pfn</i></td></tr>
<tr><th id="1136">1136</th><td><i>	 */</i></td></tr>
<tr><th id="1137">1137</th><td>	cc-&gt;free_pfn = isolate_start_pfn;</td></tr>
<tr><th id="1138">1138</th><td>}</td></tr>
<tr><th id="1139">1139</th><td></td></tr>
<tr><th id="1140">1140</th><td><i>/*</i></td></tr>
<tr><th id="1141">1141</th><td><i> * This is a migrate-callback that "allocates" freepages by taking pages</i></td></tr>
<tr><th id="1142">1142</th><td><i> * from the isolated freelists in the block we are migrating to.</i></td></tr>
<tr><th id="1143">1143</th><td><i> */</i></td></tr>
<tr><th id="1144">1144</th><td><em>static</em> <b>struct</b> page *compaction_alloc(<b>struct</b> page *migratepage,</td></tr>
<tr><th id="1145">1145</th><td>					<em>unsigned</em> <em>long</em> data,</td></tr>
<tr><th id="1146">1146</th><td>					<em>int</em> **result)</td></tr>
<tr><th id="1147">1147</th><td>{</td></tr>
<tr><th id="1148">1148</th><td>	<b>struct</b> compact_control *cc = (<b>struct</b> compact_control *)data;</td></tr>
<tr><th id="1149">1149</th><td>	<b>struct</b> page *freepage;</td></tr>
<tr><th id="1150">1150</th><td></td></tr>
<tr><th id="1151">1151</th><td>	<i>/*</i></td></tr>
<tr><th id="1152">1152</th><td><i>	 * Isolate free pages if necessary, and if we are not aborting due to</i></td></tr>
<tr><th id="1153">1153</th><td><i>	 * contention.</i></td></tr>
<tr><th id="1154">1154</th><td><i>	 */</i></td></tr>
<tr><th id="1155">1155</th><td>	<b>if</b> (list_empty(&amp;cc-&gt;freepages)) {</td></tr>
<tr><th id="1156">1156</th><td>		<b>if</b> (!cc-&gt;contended)</td></tr>
<tr><th id="1157">1157</th><td>			isolate_freepages(cc);</td></tr>
<tr><th id="1158">1158</th><td></td></tr>
<tr><th id="1159">1159</th><td>		<b>if</b> (list_empty(&amp;cc-&gt;freepages))</td></tr>
<tr><th id="1160">1160</th><td>			<b>return</b> NULL;</td></tr>
<tr><th id="1161">1161</th><td>	}</td></tr>
<tr><th id="1162">1162</th><td></td></tr>
<tr><th id="1163">1163</th><td>	freepage = list_entry(cc-&gt;freepages.next, <b>struct</b> page, lru);</td></tr>
<tr><th id="1164">1164</th><td>	list_del(&amp;freepage-&gt;lru);</td></tr>
<tr><th id="1165">1165</th><td>	cc-&gt;nr_freepages--;</td></tr>
<tr><th id="1166">1166</th><td></td></tr>
<tr><th id="1167">1167</th><td>	<b>return</b> freepage;</td></tr>
<tr><th id="1168">1168</th><td>}</td></tr>
<tr><th id="1169">1169</th><td></td></tr>
<tr><th id="1170">1170</th><td><i>/*</i></td></tr>
<tr><th id="1171">1171</th><td><i> * This is a migrate-callback that "frees" freepages back to the isolated</i></td></tr>
<tr><th id="1172">1172</th><td><i> * freelist.  All pages on the freelist are from the same zone, so there is no</i></td></tr>
<tr><th id="1173">1173</th><td><i> * special handling needed for NUMA.</i></td></tr>
<tr><th id="1174">1174</th><td><i> */</i></td></tr>
<tr><th id="1175">1175</th><td><em>static</em> <em>void</em> compaction_free(<b>struct</b> page *page, <em>unsigned</em> <em>long</em> data)</td></tr>
<tr><th id="1176">1176</th><td>{</td></tr>
<tr><th id="1177">1177</th><td>	<b>struct</b> compact_control *cc = (<b>struct</b> compact_control *)data;</td></tr>
<tr><th id="1178">1178</th><td></td></tr>
<tr><th id="1179">1179</th><td>	list_add(&amp;page-&gt;lru, &amp;cc-&gt;freepages);</td></tr>
<tr><th id="1180">1180</th><td>	cc-&gt;nr_freepages++;</td></tr>
<tr><th id="1181">1181</th><td>}</td></tr>
<tr><th id="1182">1182</th><td></td></tr>
<tr><th id="1183">1183</th><td><i>/* possible outcome of isolate_migratepages */</i></td></tr>
<tr><th id="1184">1184</th><td><b>typedef</b> <b>enum</b> {</td></tr>
<tr><th id="1185">1185</th><td>	ISOLATE_ABORT,		<i>/* Abort compaction now */</i></td></tr>
<tr><th id="1186">1186</th><td>	ISOLATE_NONE,		<i>/* No pages isolated, continue scanning */</i></td></tr>
<tr><th id="1187">1187</th><td>	ISOLATE_SUCCESS,	<i>/* Pages isolated, migrate */</i></td></tr>
<tr><th id="1188">1188</th><td>} isolate_migrate_t;</td></tr>
<tr><th id="1189">1189</th><td></td></tr>
<tr><th id="1190">1190</th><td><i>/*</i></td></tr>
<tr><th id="1191">1191</th><td><i> * Allow userspace to control policy on scanning the unevictable LRU for</i></td></tr>
<tr><th id="1192">1192</th><td><i> * compactable pages.</i></td></tr>
<tr><th id="1193">1193</th><td><i> */</i></td></tr>
<tr><th id="1194">1194</th><td><em>int</em> sysctl_compact_unevictable_allowed __read_mostly = <var>1</var>;</td></tr>
<tr><th id="1195">1195</th><td></td></tr>
<tr><th id="1196">1196</th><td><i>/*</i></td></tr>
<tr><th id="1197">1197</th><td><i> * Isolate all pages that can be migrated from the first suitable block,</i></td></tr>
<tr><th id="1198">1198</th><td><i> * starting at the block pointed to by the migrate scanner pfn within</i></td></tr>
<tr><th id="1199">1199</th><td><i> * compact_control.</i></td></tr>
<tr><th id="1200">1200</th><td><i> */</i></td></tr>
<tr><th id="1201">1201</th><td><em>static</em> isolate_migrate_t isolate_migratepages(<b>struct</b> zone *zone,</td></tr>
<tr><th id="1202">1202</th><td>					<b>struct</b> compact_control *cc)</td></tr>
<tr><th id="1203">1203</th><td>{</td></tr>
<tr><th id="1204">1204</th><td>	<em>unsigned</em> <em>long</em> block_start_pfn;</td></tr>
<tr><th id="1205">1205</th><td>	<em>unsigned</em> <em>long</em> block_end_pfn;</td></tr>
<tr><th id="1206">1206</th><td>	<em>unsigned</em> <em>long</em> low_pfn;</td></tr>
<tr><th id="1207">1207</th><td>	<b>struct</b> page *page;</td></tr>
<tr><th id="1208">1208</th><td>	<em>const</em> isolate_mode_t isolate_mode =</td></tr>
<tr><th id="1209">1209</th><td>		(sysctl_compact_unevictable_allowed ? ISOLATE_UNEVICTABLE : <var>0</var>) |</td></tr>
<tr><th id="1210">1210</th><td>		(cc-&gt;mode != MIGRATE_SYNC ? ISOLATE_ASYNC_MIGRATE : <var>0</var>);</td></tr>
<tr><th id="1211">1211</th><td></td></tr>
<tr><th id="1212">1212</th><td>	<i>/*</i></td></tr>
<tr><th id="1213">1213</th><td><i>	 * Start at where we last stopped, or beginning of the zone as</i></td></tr>
<tr><th id="1214">1214</th><td><i>	 * initialized by compact_zone()</i></td></tr>
<tr><th id="1215">1215</th><td><i>	 */</i></td></tr>
<tr><th id="1216">1216</th><td>	low_pfn = cc-&gt;migrate_pfn;</td></tr>
<tr><th id="1217">1217</th><td>	block_start_pfn = pageblock_start_pfn(low_pfn);</td></tr>
<tr><th id="1218">1218</th><td>	<b>if</b> (block_start_pfn &lt; zone-&gt;zone_start_pfn)</td></tr>
<tr><th id="1219">1219</th><td>		block_start_pfn = zone-&gt;zone_start_pfn;</td></tr>
<tr><th id="1220">1220</th><td></td></tr>
<tr><th id="1221">1221</th><td>	<i>/* Only scan within a pageblock boundary */</i></td></tr>
<tr><th id="1222">1222</th><td>	block_end_pfn = pageblock_end_pfn(low_pfn);</td></tr>
<tr><th id="1223">1223</th><td></td></tr>
<tr><th id="1224">1224</th><td>	<i>/*</i></td></tr>
<tr><th id="1225">1225</th><td><i>	 * Iterate over whole pageblocks until we find the first suitable.</i></td></tr>
<tr><th id="1226">1226</th><td><i>	 * Do not cross the free scanner.</i></td></tr>
<tr><th id="1227">1227</th><td><i>	 */</i></td></tr>
<tr><th id="1228">1228</th><td>	<b>for</b> (; block_end_pfn &lt;= cc-&gt;free_pfn;</td></tr>
<tr><th id="1229">1229</th><td>			low_pfn = block_end_pfn,</td></tr>
<tr><th id="1230">1230</th><td>			block_start_pfn = block_end_pfn,</td></tr>
<tr><th id="1231">1231</th><td>			block_end_pfn += pageblock_nr_pages) {</td></tr>
<tr><th id="1232">1232</th><td></td></tr>
<tr><th id="1233">1233</th><td>		<i>/*</i></td></tr>
<tr><th id="1234">1234</th><td><i>		 * This can potentially iterate a massively long zone with</i></td></tr>
<tr><th id="1235">1235</th><td><i>		 * many pageblocks unsuitable, so periodically check if we</i></td></tr>
<tr><th id="1236">1236</th><td><i>		 * need to schedule, or even abort async compaction.</i></td></tr>
<tr><th id="1237">1237</th><td><i>		 */</i></td></tr>
<tr><th id="1238">1238</th><td>		<b>if</b> (!(low_pfn % (SWAP_CLUSTER_MAX * pageblock_nr_pages))</td></tr>
<tr><th id="1239">1239</th><td>						&amp;&amp; compact_should_abort(cc))</td></tr>
<tr><th id="1240">1240</th><td>			<b>break</b>;</td></tr>
<tr><th id="1241">1241</th><td></td></tr>
<tr><th id="1242">1242</th><td>		page = pageblock_pfn_to_page(block_start_pfn, block_end_pfn,</td></tr>
<tr><th id="1243">1243</th><td>									zone);</td></tr>
<tr><th id="1244">1244</th><td>		<b>if</b> (!page)</td></tr>
<tr><th id="1245">1245</th><td>			<b>continue</b>;</td></tr>
<tr><th id="1246">1246</th><td></td></tr>
<tr><th id="1247">1247</th><td>		<i>/* If isolation recently failed, do not retry */</i></td></tr>
<tr><th id="1248">1248</th><td>		<b>if</b> (!isolation_suitable(cc, page))</td></tr>
<tr><th id="1249">1249</th><td>			<b>continue</b>;</td></tr>
<tr><th id="1250">1250</th><td></td></tr>
<tr><th id="1251">1251</th><td>		<i>/*</i></td></tr>
<tr><th id="1252">1252</th><td><i>		 * For async compaction, also only scan in MOVABLE blocks.</i></td></tr>
<tr><th id="1253">1253</th><td><i>		 * Async compaction is optimistic to see if the minimum amount</i></td></tr>
<tr><th id="1254">1254</th><td><i>		 * of work satisfies the allocation.</i></td></tr>
<tr><th id="1255">1255</th><td><i>		 */</i></td></tr>
<tr><th id="1256">1256</th><td>		<b>if</b> (!suitable_migration_source(cc, page))</td></tr>
<tr><th id="1257">1257</th><td>			<b>continue</b>;</td></tr>
<tr><th id="1258">1258</th><td></td></tr>
<tr><th id="1259">1259</th><td>		<i>/* Perform the isolation */</i></td></tr>
<tr><th id="1260">1260</th><td>		low_pfn = isolate_migratepages_block(cc, low_pfn,</td></tr>
<tr><th id="1261">1261</th><td>						block_end_pfn, isolate_mode);</td></tr>
<tr><th id="1262">1262</th><td></td></tr>
<tr><th id="1263">1263</th><td>		<b>if</b> (!low_pfn || cc-&gt;contended)</td></tr>
<tr><th id="1264">1264</th><td>			<b>return</b> ISOLATE_ABORT;</td></tr>
<tr><th id="1265">1265</th><td></td></tr>
<tr><th id="1266">1266</th><td>		<i>/*</i></td></tr>
<tr><th id="1267">1267</th><td><i>		 * Either we isolated something and proceed with migration. Or</i></td></tr>
<tr><th id="1268">1268</th><td><i>		 * we failed and compact_zone should decide if we should</i></td></tr>
<tr><th id="1269">1269</th><td><i>		 * continue or not.</i></td></tr>
<tr><th id="1270">1270</th><td><i>		 */</i></td></tr>
<tr><th id="1271">1271</th><td>		<b>break</b>;</td></tr>
<tr><th id="1272">1272</th><td>	}</td></tr>
<tr><th id="1273">1273</th><td></td></tr>
<tr><th id="1274">1274</th><td>	<i>/* Record where migration scanner will be restarted. */</i></td></tr>
<tr><th id="1275">1275</th><td>	cc-&gt;migrate_pfn = low_pfn;</td></tr>
<tr><th id="1276">1276</th><td></td></tr>
<tr><th id="1277">1277</th><td>	<b>return</b> cc-&gt;nr_migratepages ? ISOLATE_SUCCESS : ISOLATE_NONE;</td></tr>
<tr><th id="1278">1278</th><td>}</td></tr>
<tr><th id="1279">1279</th><td></td></tr>
<tr><th id="1280">1280</th><td><i>/*</i></td></tr>
<tr><th id="1281">1281</th><td><i> * order == -1 is expected when compacting via</i></td></tr>
<tr><th id="1282">1282</th><td><i> * /proc/sys/vm/compact_memory</i></td></tr>
<tr><th id="1283">1283</th><td><i> */</i></td></tr>
<tr><th id="1284">1284</th><td><em>static</em> <b>inline</b> bool is_via_compact_memory(<em>int</em> order)</td></tr>
<tr><th id="1285">1285</th><td>{</td></tr>
<tr><th id="1286">1286</th><td>	<b>return</b> order == -<var>1</var>;</td></tr>
<tr><th id="1287">1287</th><td>}</td></tr>
<tr><th id="1288">1288</th><td></td></tr>
<tr><th id="1289">1289</th><td><em>static</em> <b>enum</b> compact_result __compact_finished(<b>struct</b> zone *zone,</td></tr>
<tr><th id="1290">1290</th><td>						<b>struct</b> compact_control *cc)</td></tr>
<tr><th id="1291">1291</th><td>{</td></tr>
<tr><th id="1292">1292</th><td>	<em>unsigned</em> <em>int</em> order;</td></tr>
<tr><th id="1293">1293</th><td>	<em>const</em> <em>int</em> migratetype = cc-&gt;migratetype;</td></tr>
<tr><th id="1294">1294</th><td></td></tr>
<tr><th id="1295">1295</th><td>	<b>if</b> (cc-&gt;contended || fatal_signal_pending(current))</td></tr>
<tr><th id="1296">1296</th><td>		<b>return</b> COMPACT_CONTENDED;</td></tr>
<tr><th id="1297">1297</th><td></td></tr>
<tr><th id="1298">1298</th><td>	<i>/* Compaction run completes if the migrate and free scanner meet */</i></td></tr>
<tr><th id="1299">1299</th><td>	<b>if</b> (compact_scanners_met(cc)) {</td></tr>
<tr><th id="1300">1300</th><td>		<i>/* Let the next compaction start anew. */</i></td></tr>
<tr><th id="1301">1301</th><td>		reset_cached_positions(zone);</td></tr>
<tr><th id="1302">1302</th><td></td></tr>
<tr><th id="1303">1303</th><td>		<i>/*</i></td></tr>
<tr><th id="1304">1304</th><td><i>		 * Mark that the PG_migrate_skip information should be cleared</i></td></tr>
<tr><th id="1305">1305</th><td><i>		 * by kswapd when it goes to sleep. kcompactd does not set the</i></td></tr>
<tr><th id="1306">1306</th><td><i>		 * flag itself as the decision to be clear should be directly</i></td></tr>
<tr><th id="1307">1307</th><td><i>		 * based on an allocation request.</i></td></tr>
<tr><th id="1308">1308</th><td><i>		 */</i></td></tr>
<tr><th id="1309">1309</th><td>		<b>if</b> (cc-&gt;direct_compaction)</td></tr>
<tr><th id="1310">1310</th><td>			zone-&gt;compact_blockskip_flush = true;</td></tr>
<tr><th id="1311">1311</th><td></td></tr>
<tr><th id="1312">1312</th><td>		<b>if</b> (cc-&gt;whole_zone)</td></tr>
<tr><th id="1313">1313</th><td>			<b>return</b> COMPACT_COMPLETE;</td></tr>
<tr><th id="1314">1314</th><td>		<b>else</b></td></tr>
<tr><th id="1315">1315</th><td>			<b>return</b> COMPACT_PARTIAL_SKIPPED;</td></tr>
<tr><th id="1316">1316</th><td>	}</td></tr>
<tr><th id="1317">1317</th><td></td></tr>
<tr><th id="1318">1318</th><td>	<b>if</b> (is_via_compact_memory(cc-&gt;order))</td></tr>
<tr><th id="1319">1319</th><td>		<b>return</b> COMPACT_CONTINUE;</td></tr>
<tr><th id="1320">1320</th><td></td></tr>
<tr><th id="1321">1321</th><td>	<b>if</b> (cc-&gt;finishing_block) {</td></tr>
<tr><th id="1322">1322</th><td>		<i>/*</i></td></tr>
<tr><th id="1323">1323</th><td><i>		 * We have finished the pageblock, but better check again that</i></td></tr>
<tr><th id="1324">1324</th><td><i>		 * we really succeeded.</i></td></tr>
<tr><th id="1325">1325</th><td><i>		 */</i></td></tr>
<tr><th id="1326">1326</th><td>		<b>if</b> (IS_ALIGNED(cc-&gt;migrate_pfn, pageblock_nr_pages))</td></tr>
<tr><th id="1327">1327</th><td>			cc-&gt;finishing_block = false;</td></tr>
<tr><th id="1328">1328</th><td>		<b>else</b></td></tr>
<tr><th id="1329">1329</th><td>			<b>return</b> COMPACT_CONTINUE;</td></tr>
<tr><th id="1330">1330</th><td>	}</td></tr>
<tr><th id="1331">1331</th><td></td></tr>
<tr><th id="1332">1332</th><td>	<i>/* Direct compactor: Is a suitable page free? */</i></td></tr>
<tr><th id="1333">1333</th><td>	<b>for</b> (order = cc-&gt;order; order &lt; MAX_ORDER; order++) {</td></tr>
<tr><th id="1334">1334</th><td>		<b>struct</b> free_area *area = &amp;zone-&gt;free_area[order];</td></tr>
<tr><th id="1335">1335</th><td>		bool can_steal;</td></tr>
<tr><th id="1336">1336</th><td></td></tr>
<tr><th id="1337">1337</th><td>		<i>/* Job done if page is free of the right migratetype */</i></td></tr>
<tr><th id="1338">1338</th><td>		<b>if</b> (!list_empty(&amp;area-&gt;free_list[migratetype]))</td></tr>
<tr><th id="1339">1339</th><td>			<b>return</b> COMPACT_SUCCESS;</td></tr>
<tr><th id="1340">1340</th><td></td></tr>
<tr><th id="1341">1341</th><td><u>#ifdef CONFIG_CMA</u></td></tr>
<tr><th id="1342">1342</th><td>		<i>/* MIGRATE_MOVABLE can fallback on MIGRATE_CMA */</i></td></tr>
<tr><th id="1343">1343</th><td>		<b>if</b> (migratetype == MIGRATE_MOVABLE &amp;&amp;</td></tr>
<tr><th id="1344">1344</th><td>			!list_empty(&amp;area-&gt;free_list[MIGRATE_CMA]))</td></tr>
<tr><th id="1345">1345</th><td>			<b>return</b> COMPACT_SUCCESS;</td></tr>
<tr><th id="1346">1346</th><td><u>#endif</u></td></tr>
<tr><th id="1347">1347</th><td>		<i>/*</i></td></tr>
<tr><th id="1348">1348</th><td><i>		 * Job done if allocation would steal freepages from</i></td></tr>
<tr><th id="1349">1349</th><td><i>		 * other migratetype buddy lists.</i></td></tr>
<tr><th id="1350">1350</th><td><i>		 */</i></td></tr>
<tr><th id="1351">1351</th><td>		<b>if</b> (find_suitable_fallback(area, order, migratetype,</td></tr>
<tr><th id="1352">1352</th><td>						true, &amp;can_steal) != -<var>1</var>) {</td></tr>
<tr><th id="1353">1353</th><td></td></tr>
<tr><th id="1354">1354</th><td>			<i>/* movable pages are OK in any pageblock */</i></td></tr>
<tr><th id="1355">1355</th><td>			<b>if</b> (migratetype == MIGRATE_MOVABLE)</td></tr>
<tr><th id="1356">1356</th><td>				<b>return</b> COMPACT_SUCCESS;</td></tr>
<tr><th id="1357">1357</th><td></td></tr>
<tr><th id="1358">1358</th><td>			<i>/*</i></td></tr>
<tr><th id="1359">1359</th><td><i>			 * We are stealing for a non-movable allocation. Make</i></td></tr>
<tr><th id="1360">1360</th><td><i>			 * sure we finish compacting the current pageblock</i></td></tr>
<tr><th id="1361">1361</th><td><i>			 * first so it is as free as possible and we won't</i></td></tr>
<tr><th id="1362">1362</th><td><i>			 * have to steal another one soon. This only applies</i></td></tr>
<tr><th id="1363">1363</th><td><i>			 * to sync compaction, as async compaction operates</i></td></tr>
<tr><th id="1364">1364</th><td><i>			 * on pageblocks of the same migratetype.</i></td></tr>
<tr><th id="1365">1365</th><td><i>			 */</i></td></tr>
<tr><th id="1366">1366</th><td>			<b>if</b> (cc-&gt;mode == MIGRATE_ASYNC ||</td></tr>
<tr><th id="1367">1367</th><td>					IS_ALIGNED(cc-&gt;migrate_pfn,</td></tr>
<tr><th id="1368">1368</th><td>							pageblock_nr_pages)) {</td></tr>
<tr><th id="1369">1369</th><td>				<b>return</b> COMPACT_SUCCESS;</td></tr>
<tr><th id="1370">1370</th><td>			}</td></tr>
<tr><th id="1371">1371</th><td></td></tr>
<tr><th id="1372">1372</th><td>			cc-&gt;finishing_block = true;</td></tr>
<tr><th id="1373">1373</th><td>			<b>return</b> COMPACT_CONTINUE;</td></tr>
<tr><th id="1374">1374</th><td>		}</td></tr>
<tr><th id="1375">1375</th><td>	}</td></tr>
<tr><th id="1376">1376</th><td></td></tr>
<tr><th id="1377">1377</th><td>	<b>return</b> COMPACT_NO_SUITABLE_PAGE;</td></tr>
<tr><th id="1378">1378</th><td>}</td></tr>
<tr><th id="1379">1379</th><td></td></tr>
<tr><th id="1380">1380</th><td><em>static</em> <b>enum</b> compact_result compact_finished(<b>struct</b> zone *zone,</td></tr>
<tr><th id="1381">1381</th><td>			<b>struct</b> compact_control *cc)</td></tr>
<tr><th id="1382">1382</th><td>{</td></tr>
<tr><th id="1383">1383</th><td>	<em>int</em> ret;</td></tr>
<tr><th id="1384">1384</th><td></td></tr>
<tr><th id="1385">1385</th><td>	ret = __compact_finished(zone, cc);</td></tr>
<tr><th id="1386">1386</th><td>	trace_mm_compaction_finished(zone, cc-&gt;order, ret);</td></tr>
<tr><th id="1387">1387</th><td>	<b>if</b> (ret == COMPACT_NO_SUITABLE_PAGE)</td></tr>
<tr><th id="1388">1388</th><td>		ret = COMPACT_CONTINUE;</td></tr>
<tr><th id="1389">1389</th><td></td></tr>
<tr><th id="1390">1390</th><td>	<b>return</b> ret;</td></tr>
<tr><th id="1391">1391</th><td>}</td></tr>
<tr><th id="1392">1392</th><td></td></tr>
<tr><th id="1393">1393</th><td><i>/*</i></td></tr>
<tr><th id="1394">1394</th><td><i> * compaction_suitable: Is this suitable to run compaction on this zone now?</i></td></tr>
<tr><th id="1395">1395</th><td><i> * Returns</i></td></tr>
<tr><th id="1396">1396</th><td><i> *   COMPACT_SKIPPED  - If there are too few free pages for compaction</i></td></tr>
<tr><th id="1397">1397</th><td><i> *   COMPACT_SUCCESS  - If the allocation would succeed without compaction</i></td></tr>
<tr><th id="1398">1398</th><td><i> *   COMPACT_CONTINUE - If compaction should run now</i></td></tr>
<tr><th id="1399">1399</th><td><i> */</i></td></tr>
<tr><th id="1400">1400</th><td><em>static</em> <b>enum</b> compact_result __compaction_suitable(<b>struct</b> zone *zone, <em>int</em> order,</td></tr>
<tr><th id="1401">1401</th><td>					<em>unsigned</em> <em>int</em> alloc_flags,</td></tr>
<tr><th id="1402">1402</th><td>					<em>int</em> classzone_idx,</td></tr>
<tr><th id="1403">1403</th><td>					<em>unsigned</em> <em>long</em> wmark_target)</td></tr>
<tr><th id="1404">1404</th><td>{</td></tr>
<tr><th id="1405">1405</th><td>	<em>unsigned</em> <em>long</em> watermark;</td></tr>
<tr><th id="1406">1406</th><td></td></tr>
<tr><th id="1407">1407</th><td>	<b>if</b> (is_via_compact_memory(order))</td></tr>
<tr><th id="1408">1408</th><td>		<b>return</b> COMPACT_CONTINUE;</td></tr>
<tr><th id="1409">1409</th><td></td></tr>
<tr><th id="1410">1410</th><td>	watermark = zone-&gt;watermark[alloc_flags &amp; ALLOC_WMARK_MASK];</td></tr>
<tr><th id="1411">1411</th><td>	<i>/*</i></td></tr>
<tr><th id="1412">1412</th><td><i>	 * If watermarks for high-order allocation are already met, there</i></td></tr>
<tr><th id="1413">1413</th><td><i>	 * should be no need for compaction at all.</i></td></tr>
<tr><th id="1414">1414</th><td><i>	 */</i></td></tr>
<tr><th id="1415">1415</th><td>	<b>if</b> (zone_watermark_ok(zone, order, watermark, classzone_idx,</td></tr>
<tr><th id="1416">1416</th><td>								alloc_flags))</td></tr>
<tr><th id="1417">1417</th><td>		<b>return</b> COMPACT_SUCCESS;</td></tr>
<tr><th id="1418">1418</th><td></td></tr>
<tr><th id="1419">1419</th><td>	<i>/*</i></td></tr>
<tr><th id="1420">1420</th><td><i>	 * Watermarks for order-0 must be met for compaction to be able to</i></td></tr>
<tr><th id="1421">1421</th><td><i>	 * isolate free pages for migration targets. This means that the</i></td></tr>
<tr><th id="1422">1422</th><td><i>	 * watermark and alloc_flags have to match, or be more pessimistic than</i></td></tr>
<tr><th id="1423">1423</th><td><i>	 * the check in __isolate_free_page(). We don't use the direct</i></td></tr>
<tr><th id="1424">1424</th><td><i>	 * compactor's alloc_flags, as they are not relevant for freepage</i></td></tr>
<tr><th id="1425">1425</th><td><i>	 * isolation. We however do use the direct compactor's classzone_idx to</i></td></tr>
<tr><th id="1426">1426</th><td><i>	 * skip over zones where lowmem reserves would prevent allocation even</i></td></tr>
<tr><th id="1427">1427</th><td><i>	 * if compaction succeeds.</i></td></tr>
<tr><th id="1428">1428</th><td><i>	 * For costly orders, we require low watermark instead of min for</i></td></tr>
<tr><th id="1429">1429</th><td><i>	 * compaction to proceed to increase its chances.</i></td></tr>
<tr><th id="1430">1430</th><td><i>	 * ALLOC_CMA is used, as pages in CMA pageblocks are considered</i></td></tr>
<tr><th id="1431">1431</th><td><i>	 * suitable migration targets</i></td></tr>
<tr><th id="1432">1432</th><td><i>	 */</i></td></tr>
<tr><th id="1433">1433</th><td>	watermark = (order &gt; PAGE_ALLOC_COSTLY_ORDER) ?</td></tr>
<tr><th id="1434">1434</th><td>				low_wmark_pages(zone) : min_wmark_pages(zone);</td></tr>
<tr><th id="1435">1435</th><td>	watermark += compact_gap(order);</td></tr>
<tr><th id="1436">1436</th><td>	<b>if</b> (!__zone_watermark_ok(zone, <var>0</var>, watermark, classzone_idx,</td></tr>
<tr><th id="1437">1437</th><td>						ALLOC_CMA, wmark_target))</td></tr>
<tr><th id="1438">1438</th><td>		<b>return</b> COMPACT_SKIPPED;</td></tr>
<tr><th id="1439">1439</th><td></td></tr>
<tr><th id="1440">1440</th><td>	<b>return</b> COMPACT_CONTINUE;</td></tr>
<tr><th id="1441">1441</th><td>}</td></tr>
<tr><th id="1442">1442</th><td></td></tr>
<tr><th id="1443">1443</th><td><b>enum</b> compact_result compaction_suitable(<b>struct</b> zone *zone, <em>int</em> order,</td></tr>
<tr><th id="1444">1444</th><td>					<em>unsigned</em> <em>int</em> alloc_flags,</td></tr>
<tr><th id="1445">1445</th><td>					<em>int</em> classzone_idx)</td></tr>
<tr><th id="1446">1446</th><td>{</td></tr>
<tr><th id="1447">1447</th><td>	<b>enum</b> compact_result ret;</td></tr>
<tr><th id="1448">1448</th><td>	<em>int</em> fragindex;</td></tr>
<tr><th id="1449">1449</th><td></td></tr>
<tr><th id="1450">1450</th><td>	ret = __compaction_suitable(zone, order, alloc_flags, classzone_idx,</td></tr>
<tr><th id="1451">1451</th><td>				    zone_page_state(zone, NR_FREE_PAGES));</td></tr>
<tr><th id="1452">1452</th><td>	<i>/*</i></td></tr>
<tr><th id="1453">1453</th><td><i>	 * fragmentation index determines if allocation failures are due to</i></td></tr>
<tr><th id="1454">1454</th><td><i>	 * low memory or external fragmentation</i></td></tr>
<tr><th id="1455">1455</th><td><i>	 *</i></td></tr>
<tr><th id="1456">1456</th><td><i>	 * index of -1000 would imply allocations might succeed depending on</i></td></tr>
<tr><th id="1457">1457</th><td><i>	 * watermarks, but we already failed the high-order watermark check</i></td></tr>
<tr><th id="1458">1458</th><td><i>	 * index towards 0 implies failure is due to lack of memory</i></td></tr>
<tr><th id="1459">1459</th><td><i>	 * index towards 1000 implies failure is due to fragmentation</i></td></tr>
<tr><th id="1460">1460</th><td><i>	 *</i></td></tr>
<tr><th id="1461">1461</th><td><i>	 * Only compact if a failure would be due to fragmentation. Also</i></td></tr>
<tr><th id="1462">1462</th><td><i>	 * ignore fragindex for non-costly orders where the alternative to</i></td></tr>
<tr><th id="1463">1463</th><td><i>	 * a successful reclaim/compaction is OOM. Fragindex and the</i></td></tr>
<tr><th id="1464">1464</th><td><i>	 * vm.extfrag_threshold sysctl is meant as a heuristic to prevent</i></td></tr>
<tr><th id="1465">1465</th><td><i>	 * excessive compaction for costly orders, but it should not be at the</i></td></tr>
<tr><th id="1466">1466</th><td><i>	 * expense of system stability.</i></td></tr>
<tr><th id="1467">1467</th><td><i>	 */</i></td></tr>
<tr><th id="1468">1468</th><td>	<b>if</b> (ret == COMPACT_CONTINUE &amp;&amp; (order &gt; PAGE_ALLOC_COSTLY_ORDER)) {</td></tr>
<tr><th id="1469">1469</th><td>		fragindex = fragmentation_index(zone, order);</td></tr>
<tr><th id="1470">1470</th><td>		<b>if</b> (fragindex &gt;= <var>0</var> &amp;&amp; fragindex &lt;= sysctl_extfrag_threshold)</td></tr>
<tr><th id="1471">1471</th><td>			ret = COMPACT_NOT_SUITABLE_ZONE;</td></tr>
<tr><th id="1472">1472</th><td>	}</td></tr>
<tr><th id="1473">1473</th><td></td></tr>
<tr><th id="1474">1474</th><td>	trace_mm_compaction_suitable(zone, order, ret);</td></tr>
<tr><th id="1475">1475</th><td>	<b>if</b> (ret == COMPACT_NOT_SUITABLE_ZONE)</td></tr>
<tr><th id="1476">1476</th><td>		ret = COMPACT_SKIPPED;</td></tr>
<tr><th id="1477">1477</th><td></td></tr>
<tr><th id="1478">1478</th><td>	<b>return</b> ret;</td></tr>
<tr><th id="1479">1479</th><td>}</td></tr>
<tr><th id="1480">1480</th><td></td></tr>
<tr><th id="1481">1481</th><td>bool compaction_zonelist_suitable(<b>struct</b> alloc_context *ac, <em>int</em> order,</td></tr>
<tr><th id="1482">1482</th><td>		<em>int</em> alloc_flags)</td></tr>
<tr><th id="1483">1483</th><td>{</td></tr>
<tr><th id="1484">1484</th><td>	<b>struct</b> zone *zone;</td></tr>
<tr><th id="1485">1485</th><td>	<b>struct</b> zoneref *z;</td></tr>
<tr><th id="1486">1486</th><td></td></tr>
<tr><th id="1487">1487</th><td>	<i>/*</i></td></tr>
<tr><th id="1488">1488</th><td><i>	 * Make sure at least one zone would pass __compaction_suitable if we continue</i></td></tr>
<tr><th id="1489">1489</th><td><i>	 * retrying the reclaim.</i></td></tr>
<tr><th id="1490">1490</th><td><i>	 */</i></td></tr>
<tr><th id="1491">1491</th><td>	for_each_zone_zonelist_nodemask(zone, z, ac-&gt;zonelist, ac-&gt;high_zoneidx,</td></tr>
<tr><th id="1492">1492</th><td>					ac-&gt;nodemask) {</td></tr>
<tr><th id="1493">1493</th><td>		<em>unsigned</em> <em>long</em> available;</td></tr>
<tr><th id="1494">1494</th><td>		<b>enum</b> compact_result compact_result;</td></tr>
<tr><th id="1495">1495</th><td></td></tr>
<tr><th id="1496">1496</th><td>		<i>/*</i></td></tr>
<tr><th id="1497">1497</th><td><i>		 * Do not consider all the reclaimable memory because we do not</i></td></tr>
<tr><th id="1498">1498</th><td><i>		 * want to trash just for a single high order allocation which</i></td></tr>
<tr><th id="1499">1499</th><td><i>		 * is even not guaranteed to appear even if __compaction_suitable</i></td></tr>
<tr><th id="1500">1500</th><td><i>		 * is happy about the watermark check.</i></td></tr>
<tr><th id="1501">1501</th><td><i>		 */</i></td></tr>
<tr><th id="1502">1502</th><td>		available = zone_reclaimable_pages(zone) / order;</td></tr>
<tr><th id="1503">1503</th><td>		available += zone_page_state_snapshot(zone, NR_FREE_PAGES);</td></tr>
<tr><th id="1504">1504</th><td>		compact_result = __compaction_suitable(zone, order, alloc_flags,</td></tr>
<tr><th id="1505">1505</th><td>				ac_classzone_idx(ac), available);</td></tr>
<tr><th id="1506">1506</th><td>		<b>if</b> (compact_result != COMPACT_SKIPPED)</td></tr>
<tr><th id="1507">1507</th><td>			<b>return</b> true;</td></tr>
<tr><th id="1508">1508</th><td>	}</td></tr>
<tr><th id="1509">1509</th><td></td></tr>
<tr><th id="1510">1510</th><td>	<b>return</b> false;</td></tr>
<tr><th id="1511">1511</th><td>}</td></tr>
<tr><th id="1512">1512</th><td></td></tr>
<tr><th id="1513">1513</th><td><em>static</em> <b>enum</b> compact_result compact_zone(<b>struct</b> zone *zone, <b>struct</b> compact_control *cc)</td></tr>
<tr><th id="1514">1514</th><td>{</td></tr>
<tr><th id="1515">1515</th><td>	<b>enum</b> compact_result ret;</td></tr>
<tr><th id="1516">1516</th><td>	<em>unsigned</em> <em>long</em> start_pfn = zone-&gt;zone_start_pfn;</td></tr>
<tr><th id="1517">1517</th><td>	<em>unsigned</em> <em>long</em> end_pfn = zone_end_pfn(zone);</td></tr>
<tr><th id="1518">1518</th><td>	<em>const</em> bool sync = cc-&gt;mode != MIGRATE_ASYNC;</td></tr>
<tr><th id="1519">1519</th><td></td></tr>
<tr><th id="1520">1520</th><td>	cc-&gt;migratetype = gfpflags_to_migratetype(cc-&gt;gfp_mask);</td></tr>
<tr><th id="1521">1521</th><td>	ret = compaction_suitable(zone, cc-&gt;order, cc-&gt;alloc_flags,</td></tr>
<tr><th id="1522">1522</th><td>							cc-&gt;classzone_idx);</td></tr>
<tr><th id="1523">1523</th><td>	<i>/* Compaction is likely to fail */</i></td></tr>
<tr><th id="1524">1524</th><td>	<b>if</b> (ret == COMPACT_SUCCESS || ret == COMPACT_SKIPPED)</td></tr>
<tr><th id="1525">1525</th><td>		<b>return</b> ret;</td></tr>
<tr><th id="1526">1526</th><td></td></tr>
<tr><th id="1527">1527</th><td>	<i>/* huh, compaction_suitable is returning something unexpected */</i></td></tr>
<tr><th id="1528">1528</th><td>	VM_BUG_ON(ret != COMPACT_CONTINUE);</td></tr>
<tr><th id="1529">1529</th><td></td></tr>
<tr><th id="1530">1530</th><td>	<i>/*</i></td></tr>
<tr><th id="1531">1531</th><td><i>	 * Clear pageblock skip if there were failures recently and compaction</i></td></tr>
<tr><th id="1532">1532</th><td><i>	 * is about to be retried after being deferred.</i></td></tr>
<tr><th id="1533">1533</th><td><i>	 */</i></td></tr>
<tr><th id="1534">1534</th><td>	<b>if</b> (compaction_restarting(zone, cc-&gt;order))</td></tr>
<tr><th id="1535">1535</th><td>		__reset_isolation_suitable(zone);</td></tr>
<tr><th id="1536">1536</th><td></td></tr>
<tr><th id="1537">1537</th><td>	<i>/*</i></td></tr>
<tr><th id="1538">1538</th><td><i>	 * Setup to move all movable pages to the end of the zone. Used cached</i></td></tr>
<tr><th id="1539">1539</th><td><i>	 * information on where the scanners should start (unless we explicitly</i></td></tr>
<tr><th id="1540">1540</th><td><i>	 * want to compact the whole zone), but check that it is initialised</i></td></tr>
<tr><th id="1541">1541</th><td><i>	 * by ensuring the values are within zone boundaries.</i></td></tr>
<tr><th id="1542">1542</th><td><i>	 */</i></td></tr>
<tr><th id="1543">1543</th><td>	<b>if</b> (cc-&gt;whole_zone) {</td></tr>
<tr><th id="1544">1544</th><td>		cc-&gt;migrate_pfn = start_pfn;</td></tr>
<tr><th id="1545">1545</th><td>		cc-&gt;free_pfn = pageblock_start_pfn(end_pfn - <var>1</var>);</td></tr>
<tr><th id="1546">1546</th><td>	} <b>else</b> {</td></tr>
<tr><th id="1547">1547</th><td>		cc-&gt;migrate_pfn = zone-&gt;compact_cached_migrate_pfn[sync];</td></tr>
<tr><th id="1548">1548</th><td>		cc-&gt;free_pfn = zone-&gt;compact_cached_free_pfn;</td></tr>
<tr><th id="1549">1549</th><td>		<b>if</b> (cc-&gt;free_pfn &lt; start_pfn || cc-&gt;free_pfn &gt;= end_pfn) {</td></tr>
<tr><th id="1550">1550</th><td>			cc-&gt;free_pfn = pageblock_start_pfn(end_pfn - <var>1</var>);</td></tr>
<tr><th id="1551">1551</th><td>			zone-&gt;compact_cached_free_pfn = cc-&gt;free_pfn;</td></tr>
<tr><th id="1552">1552</th><td>		}</td></tr>
<tr><th id="1553">1553</th><td>		<b>if</b> (cc-&gt;migrate_pfn &lt; start_pfn || cc-&gt;migrate_pfn &gt;= end_pfn) {</td></tr>
<tr><th id="1554">1554</th><td>			cc-&gt;migrate_pfn = start_pfn;</td></tr>
<tr><th id="1555">1555</th><td>			zone-&gt;compact_cached_migrate_pfn[<var>0</var>] = cc-&gt;migrate_pfn;</td></tr>
<tr><th id="1556">1556</th><td>			zone-&gt;compact_cached_migrate_pfn[<var>1</var>] = cc-&gt;migrate_pfn;</td></tr>
<tr><th id="1557">1557</th><td>		}</td></tr>
<tr><th id="1558">1558</th><td></td></tr>
<tr><th id="1559">1559</th><td>		<b>if</b> (cc-&gt;migrate_pfn == start_pfn)</td></tr>
<tr><th id="1560">1560</th><td>			cc-&gt;whole_zone = true;</td></tr>
<tr><th id="1561">1561</th><td>	}</td></tr>
<tr><th id="1562">1562</th><td></td></tr>
<tr><th id="1563">1563</th><td>	cc-&gt;last_migrated_pfn = <var>0</var>;</td></tr>
<tr><th id="1564">1564</th><td></td></tr>
<tr><th id="1565">1565</th><td>	trace_mm_compaction_begin(start_pfn, cc-&gt;migrate_pfn,</td></tr>
<tr><th id="1566">1566</th><td>				cc-&gt;free_pfn, end_pfn, sync);</td></tr>
<tr><th id="1567">1567</th><td></td></tr>
<tr><th id="1568">1568</th><td>	migrate_prep_local();</td></tr>
<tr><th id="1569">1569</th><td></td></tr>
<tr><th id="1570">1570</th><td>	<b>while</b> ((ret = compact_finished(zone, cc)) == COMPACT_CONTINUE) {</td></tr>
<tr><th id="1571">1571</th><td>		<em>int</em> err;</td></tr>
<tr><th id="1572">1572</th><td></td></tr>
<tr><th id="1573">1573</th><td>		<b>switch</b> (isolate_migratepages(zone, cc)) {</td></tr>
<tr><th id="1574">1574</th><td>		<b>case</b> ISOLATE_ABORT:</td></tr>
<tr><th id="1575">1575</th><td>			ret = COMPACT_CONTENDED;</td></tr>
<tr><th id="1576">1576</th><td>			putback_movable_pages(&amp;cc-&gt;migratepages);</td></tr>
<tr><th id="1577">1577</th><td>			cc-&gt;nr_migratepages = <var>0</var>;</td></tr>
<tr><th id="1578">1578</th><td>			<b>goto</b> out;</td></tr>
<tr><th id="1579">1579</th><td>		<b>case</b> ISOLATE_NONE:</td></tr>
<tr><th id="1580">1580</th><td>			<i>/*</i></td></tr>
<tr><th id="1581">1581</th><td><i>			 * We haven't isolated and migrated anything, but</i></td></tr>
<tr><th id="1582">1582</th><td><i>			 * there might still be unflushed migrations from</i></td></tr>
<tr><th id="1583">1583</th><td><i>			 * previous cc-&gt;order aligned block.</i></td></tr>
<tr><th id="1584">1584</th><td><i>			 */</i></td></tr>
<tr><th id="1585">1585</th><td>			<b>goto</b> check_drain;</td></tr>
<tr><th id="1586">1586</th><td>		<b>case</b> ISOLATE_SUCCESS:</td></tr>
<tr><th id="1587">1587</th><td>			;</td></tr>
<tr><th id="1588">1588</th><td>		}</td></tr>
<tr><th id="1589">1589</th><td></td></tr>
<tr><th id="1590">1590</th><td>		err = migrate_pages(&amp;cc-&gt;migratepages, compaction_alloc,</td></tr>
<tr><th id="1591">1591</th><td>				compaction_free, (<em>unsigned</em> <em>long</em>)cc, cc-&gt;mode,</td></tr>
<tr><th id="1592">1592</th><td>				MR_COMPACTION);</td></tr>
<tr><th id="1593">1593</th><td></td></tr>
<tr><th id="1594">1594</th><td>		trace_mm_compaction_migratepages(cc-&gt;nr_migratepages, err,</td></tr>
<tr><th id="1595">1595</th><td>							&amp;cc-&gt;migratepages);</td></tr>
<tr><th id="1596">1596</th><td></td></tr>
<tr><th id="1597">1597</th><td>		<i>/* All pages were either migrated or will be released */</i></td></tr>
<tr><th id="1598">1598</th><td>		cc-&gt;nr_migratepages = <var>0</var>;</td></tr>
<tr><th id="1599">1599</th><td>		<b>if</b> (err) {</td></tr>
<tr><th id="1600">1600</th><td>			putback_movable_pages(&amp;cc-&gt;migratepages);</td></tr>
<tr><th id="1601">1601</th><td>			<i>/*</i></td></tr>
<tr><th id="1602">1602</th><td><i>			 * migrate_pages() may return -ENOMEM when scanners meet</i></td></tr>
<tr><th id="1603">1603</th><td><i>			 * and we want compact_finished() to detect it</i></td></tr>
<tr><th id="1604">1604</th><td><i>			 */</i></td></tr>
<tr><th id="1605">1605</th><td>			<b>if</b> (err == -ENOMEM &amp;&amp; !compact_scanners_met(cc)) {</td></tr>
<tr><th id="1606">1606</th><td>				ret = COMPACT_CONTENDED;</td></tr>
<tr><th id="1607">1607</th><td>				<b>goto</b> out;</td></tr>
<tr><th id="1608">1608</th><td>			}</td></tr>
<tr><th id="1609">1609</th><td>			<i>/*</i></td></tr>
<tr><th id="1610">1610</th><td><i>			 * We failed to migrate at least one page in the current</i></td></tr>
<tr><th id="1611">1611</th><td><i>			 * order-aligned block, so skip the rest of it.</i></td></tr>
<tr><th id="1612">1612</th><td><i>			 */</i></td></tr>
<tr><th id="1613">1613</th><td>			<b>if</b> (cc-&gt;direct_compaction &amp;&amp;</td></tr>
<tr><th id="1614">1614</th><td>						(cc-&gt;mode == MIGRATE_ASYNC)) {</td></tr>
<tr><th id="1615">1615</th><td>				cc-&gt;migrate_pfn = block_end_pfn(</td></tr>
<tr><th id="1616">1616</th><td>						cc-&gt;migrate_pfn - <var>1</var>, cc-&gt;order);</td></tr>
<tr><th id="1617">1617</th><td>				<i>/* Draining pcplists is useless in this case */</i></td></tr>
<tr><th id="1618">1618</th><td>				cc-&gt;last_migrated_pfn = <var>0</var>;</td></tr>
<tr><th id="1619">1619</th><td></td></tr>
<tr><th id="1620">1620</th><td>			}</td></tr>
<tr><th id="1621">1621</th><td>		}</td></tr>
<tr><th id="1622">1622</th><td></td></tr>
<tr><th id="1623">1623</th><td>check_drain:</td></tr>
<tr><th id="1624">1624</th><td>		<i>/*</i></td></tr>
<tr><th id="1625">1625</th><td><i>		 * Has the migration scanner moved away from the previous</i></td></tr>
<tr><th id="1626">1626</th><td><i>		 * cc-&gt;order aligned block where we migrated from? If yes,</i></td></tr>
<tr><th id="1627">1627</th><td><i>		 * flush the pages that were freed, so that they can merge and</i></td></tr>
<tr><th id="1628">1628</th><td><i>		 * compact_finished() can detect immediately if allocation</i></td></tr>
<tr><th id="1629">1629</th><td><i>		 * would succeed.</i></td></tr>
<tr><th id="1630">1630</th><td><i>		 */</i></td></tr>
<tr><th id="1631">1631</th><td>		<b>if</b> (cc-&gt;order &gt; <var>0</var> &amp;&amp; cc-&gt;last_migrated_pfn) {</td></tr>
<tr><th id="1632">1632</th><td>			<em>int</em> cpu;</td></tr>
<tr><th id="1633">1633</th><td>			<em>unsigned</em> <em>long</em> current_block_start =</td></tr>
<tr><th id="1634">1634</th><td>				block_start_pfn(cc-&gt;migrate_pfn, cc-&gt;order);</td></tr>
<tr><th id="1635">1635</th><td></td></tr>
<tr><th id="1636">1636</th><td>			<b>if</b> (cc-&gt;last_migrated_pfn &lt; current_block_start) {</td></tr>
<tr><th id="1637">1637</th><td>				cpu = get_cpu();</td></tr>
<tr><th id="1638">1638</th><td>				lru_add_drain_cpu(cpu);</td></tr>
<tr><th id="1639">1639</th><td>				drain_local_pages(zone);</td></tr>
<tr><th id="1640">1640</th><td>				put_cpu();</td></tr>
<tr><th id="1641">1641</th><td>				<i>/* No more flushing until we migrate again */</i></td></tr>
<tr><th id="1642">1642</th><td>				cc-&gt;last_migrated_pfn = <var>0</var>;</td></tr>
<tr><th id="1643">1643</th><td>			}</td></tr>
<tr><th id="1644">1644</th><td>		}</td></tr>
<tr><th id="1645">1645</th><td></td></tr>
<tr><th id="1646">1646</th><td>	}</td></tr>
<tr><th id="1647">1647</th><td></td></tr>
<tr><th id="1648">1648</th><td>out:</td></tr>
<tr><th id="1649">1649</th><td>	<i>/*</i></td></tr>
<tr><th id="1650">1650</th><td><i>	 * Release free pages and update where the free scanner should restart,</i></td></tr>
<tr><th id="1651">1651</th><td><i>	 * so we don't leave any returned pages behind in the next attempt.</i></td></tr>
<tr><th id="1652">1652</th><td><i>	 */</i></td></tr>
<tr><th id="1653">1653</th><td>	<b>if</b> (cc-&gt;nr_freepages &gt; <var>0</var>) {</td></tr>
<tr><th id="1654">1654</th><td>		<em>unsigned</em> <em>long</em> free_pfn = release_freepages(&amp;cc-&gt;freepages);</td></tr>
<tr><th id="1655">1655</th><td></td></tr>
<tr><th id="1656">1656</th><td>		cc-&gt;nr_freepages = <var>0</var>;</td></tr>
<tr><th id="1657">1657</th><td>		VM_BUG_ON(free_pfn == <var>0</var>);</td></tr>
<tr><th id="1658">1658</th><td>		<i>/* The cached pfn is always the first in a pageblock */</i></td></tr>
<tr><th id="1659">1659</th><td>		free_pfn = pageblock_start_pfn(free_pfn);</td></tr>
<tr><th id="1660">1660</th><td>		<i>/*</i></td></tr>
<tr><th id="1661">1661</th><td><i>		 * Only go back, not forward. The cached pfn might have been</i></td></tr>
<tr><th id="1662">1662</th><td><i>		 * already reset to zone end in compact_finished()</i></td></tr>
<tr><th id="1663">1663</th><td><i>		 */</i></td></tr>
<tr><th id="1664">1664</th><td>		<b>if</b> (free_pfn &gt; zone-&gt;compact_cached_free_pfn)</td></tr>
<tr><th id="1665">1665</th><td>			zone-&gt;compact_cached_free_pfn = free_pfn;</td></tr>
<tr><th id="1666">1666</th><td>	}</td></tr>
<tr><th id="1667">1667</th><td></td></tr>
<tr><th id="1668">1668</th><td>	count_compact_events(COMPACTMIGRATE_SCANNED, cc-&gt;total_migrate_scanned);</td></tr>
<tr><th id="1669">1669</th><td>	count_compact_events(COMPACTFREE_SCANNED, cc-&gt;total_free_scanned);</td></tr>
<tr><th id="1670">1670</th><td></td></tr>
<tr><th id="1671">1671</th><td>	trace_mm_compaction_end(start_pfn, cc-&gt;migrate_pfn,</td></tr>
<tr><th id="1672">1672</th><td>				cc-&gt;free_pfn, end_pfn, sync, ret);</td></tr>
<tr><th id="1673">1673</th><td></td></tr>
<tr><th id="1674">1674</th><td>	<b>return</b> ret;</td></tr>
<tr><th id="1675">1675</th><td>}</td></tr>
<tr><th id="1676">1676</th><td></td></tr>
<tr><th id="1677">1677</th><td><em>static</em> <b>enum</b> compact_result compact_zone_order(<b>struct</b> zone *zone, <em>int</em> order,</td></tr>
<tr><th id="1678">1678</th><td>		gfp_t gfp_mask, <b>enum</b> compact_priority prio,</td></tr>
<tr><th id="1679">1679</th><td>		<em>unsigned</em> <em>int</em> alloc_flags, <em>int</em> classzone_idx)</td></tr>
<tr><th id="1680">1680</th><td>{</td></tr>
<tr><th id="1681">1681</th><td>	<b>enum</b> compact_result ret;</td></tr>
<tr><th id="1682">1682</th><td>	<b>struct</b> compact_control cc = {</td></tr>
<tr><th id="1683">1683</th><td>		.nr_freepages = <var>0</var>,</td></tr>
<tr><th id="1684">1684</th><td>		.nr_migratepages = <var>0</var>,</td></tr>
<tr><th id="1685">1685</th><td>		.total_migrate_scanned = <var>0</var>,</td></tr>
<tr><th id="1686">1686</th><td>		.total_free_scanned = <var>0</var>,</td></tr>
<tr><th id="1687">1687</th><td>		.order = order,</td></tr>
<tr><th id="1688">1688</th><td>		.gfp_mask = gfp_mask,</td></tr>
<tr><th id="1689">1689</th><td>		.zone = zone,</td></tr>
<tr><th id="1690">1690</th><td>		.mode = (prio == COMPACT_PRIO_ASYNC) ?</td></tr>
<tr><th id="1691">1691</th><td>					MIGRATE_ASYNC :	MIGRATE_SYNC_LIGHT,</td></tr>
<tr><th id="1692">1692</th><td>		.alloc_flags = alloc_flags,</td></tr>
<tr><th id="1693">1693</th><td>		.classzone_idx = classzone_idx,</td></tr>
<tr><th id="1694">1694</th><td>		.direct_compaction = true,</td></tr>
<tr><th id="1695">1695</th><td>		.whole_zone = (prio == MIN_COMPACT_PRIORITY),</td></tr>
<tr><th id="1696">1696</th><td>		.ignore_skip_hint = (prio == MIN_COMPACT_PRIORITY),</td></tr>
<tr><th id="1697">1697</th><td>		.ignore_block_suitable = (prio == MIN_COMPACT_PRIORITY)</td></tr>
<tr><th id="1698">1698</th><td>	};</td></tr>
<tr><th id="1699">1699</th><td>	INIT_LIST_HEAD(&amp;cc.freepages);</td></tr>
<tr><th id="1700">1700</th><td>	INIT_LIST_HEAD(&amp;cc.migratepages);</td></tr>
<tr><th id="1701">1701</th><td></td></tr>
<tr><th id="1702">1702</th><td>	ret = compact_zone(zone, &amp;cc);</td></tr>
<tr><th id="1703">1703</th><td></td></tr>
<tr><th id="1704">1704</th><td>	VM_BUG_ON(!list_empty(&amp;cc.freepages));</td></tr>
<tr><th id="1705">1705</th><td>	VM_BUG_ON(!list_empty(&amp;cc.migratepages));</td></tr>
<tr><th id="1706">1706</th><td></td></tr>
<tr><th id="1707">1707</th><td>	<b>return</b> ret;</td></tr>
<tr><th id="1708">1708</th><td>}</td></tr>
<tr><th id="1709">1709</th><td></td></tr>
<tr><th id="1710">1710</th><td><em>int</em> sysctl_extfrag_threshold = <var>500</var>;</td></tr>
<tr><th id="1711">1711</th><td></td></tr>
<tr><th id="1712">1712</th><td><i class="doc">/**</i></td></tr>
<tr><th id="1713">1713</th><td><i class="doc"> * try_to_compact_pages - Direct compact to satisfy a high-order allocation</i></td></tr>
<tr><th id="1714">1714</th><td><i class="doc"> *<span class="command"> @gfp</span>_mask: The GFP mask of the current allocation</i></td></tr>
<tr><th id="1715">1715</th><td><i class="doc"> *<span class="command"> @order</span>: The order of the current allocation</i></td></tr>
<tr><th id="1716">1716</th><td><i class="doc"> *<span class="command"> @alloc</span>_flags: The allocation flags of the current allocation</i></td></tr>
<tr><th id="1717">1717</th><td><i class="doc"> *<span class="command"> @ac</span>: The context of current allocation</i></td></tr>
<tr><th id="1718">1718</th><td><i class="doc"> * <span class="command">@mode</span><span class="verb">: The migration mode for async, sync light, or sync migration</span></i></td></tr>
<tr><th id="1719">1719</th><td><i class="doc"><span class="verb"></span> *<span class="verb"></span></i></td></tr>
<tr><th id="1720">1720</th><td><i class="doc"><span class="verb"></span> *<span class="verb"> This is the main entry point for direct page compaction.</span></i></td></tr>
<tr><th id="1721">1721</th><td><i class="doc"><span class="verb"></span><span class="verb"> *</span>/</i><span class="verb"></span></td></tr>
<tr><th id="1722">1722</th><td><span class="verb"></span><b>enum</b> compact_result try_to_compact_pages(gfp_t gfp_mask, <em>unsigned</em> <em>int</em> order,</td></tr>
<tr><th id="1723">1723</th><td>		<em>unsigned</em> <em>int</em> alloc_flags, <em>const</em> <b>struct</b> alloc_context *ac,</td></tr>
<tr><th id="1724">1724</th><td>		<b>enum</b> compact_priority prio)</td></tr>
<tr><th id="1725">1725</th><td>{</td></tr>
<tr><th id="1726">1726</th><td>	<em>int</em> may_perform_io = gfp_mask &amp; __GFP_IO;</td></tr>
<tr><th id="1727">1727</th><td>	<b>struct</b> zoneref *z;</td></tr>
<tr><th id="1728">1728</th><td>	<b>struct</b> zone *zone;</td></tr>
<tr><th id="1729">1729</th><td>	<b>enum</b> compact_result rc = COMPACT_SKIPPED;</td></tr>
<tr><th id="1730">1730</th><td></td></tr>
<tr><th id="1731">1731</th><td>	<i>/*</i></td></tr>
<tr><th id="1732">1732</th><td><i>	 * Check if the GFP flags allow compaction - GFP_NOIO is really</i></td></tr>
<tr><th id="1733">1733</th><td><i>	 * tricky context because the migration might require IO</i></td></tr>
<tr><th id="1734">1734</th><td><i>	 */</i></td></tr>
<tr><th id="1735">1735</th><td>	<b>if</b> (!may_perform_io)</td></tr>
<tr><th id="1736">1736</th><td>		<b>return</b> COMPACT_SKIPPED;</td></tr>
<tr><th id="1737">1737</th><td></td></tr>
<tr><th id="1738">1738</th><td>	trace_mm_compaction_try_to_compact_pages(order, gfp_mask, prio);</td></tr>
<tr><th id="1739">1739</th><td></td></tr>
<tr><th id="1740">1740</th><td>	<i>/* Compact each zone in the list */</i></td></tr>
<tr><th id="1741">1741</th><td>	for_each_zone_zonelist_nodemask(zone, z, ac-&gt;zonelist, ac-&gt;high_zoneidx,</td></tr>
<tr><th id="1742">1742</th><td>								ac-&gt;nodemask) {</td></tr>
<tr><th id="1743">1743</th><td>		<b>enum</b> compact_result status;</td></tr>
<tr><th id="1744">1744</th><td></td></tr>
<tr><th id="1745">1745</th><td>		<b>if</b> (prio &gt; MIN_COMPACT_PRIORITY</td></tr>
<tr><th id="1746">1746</th><td>					&amp;&amp; compaction_deferred(zone, order)) {</td></tr>
<tr><th id="1747">1747</th><td>			rc = max_t(<b>enum</b> compact_result, COMPACT_DEFERRED, rc);</td></tr>
<tr><th id="1748">1748</th><td>			<b>continue</b>;</td></tr>
<tr><th id="1749">1749</th><td>		}</td></tr>
<tr><th id="1750">1750</th><td></td></tr>
<tr><th id="1751">1751</th><td>		status = compact_zone_order(zone, order, gfp_mask, prio,</td></tr>
<tr><th id="1752">1752</th><td>					alloc_flags, ac_classzone_idx(ac));</td></tr>
<tr><th id="1753">1753</th><td>		rc = max(status, rc);</td></tr>
<tr><th id="1754">1754</th><td></td></tr>
<tr><th id="1755">1755</th><td>		<i>/* The allocation should succeed, stop compacting */</i></td></tr>
<tr><th id="1756">1756</th><td>		<b>if</b> (status == COMPACT_SUCCESS) {</td></tr>
<tr><th id="1757">1757</th><td>			<i>/*</i></td></tr>
<tr><th id="1758">1758</th><td><i>			 * We think the allocation will succeed in this zone,</i></td></tr>
<tr><th id="1759">1759</th><td><i>			 * but it is not certain, hence the false. The caller</i></td></tr>
<tr><th id="1760">1760</th><td><i>			 * will repeat this with true if allocation indeed</i></td></tr>
<tr><th id="1761">1761</th><td><i>			 * succeeds in this zone.</i></td></tr>
<tr><th id="1762">1762</th><td><i>			 */</i></td></tr>
<tr><th id="1763">1763</th><td>			compaction_defer_reset(zone, order, false);</td></tr>
<tr><th id="1764">1764</th><td></td></tr>
<tr><th id="1765">1765</th><td>			<b>break</b>;</td></tr>
<tr><th id="1766">1766</th><td>		}</td></tr>
<tr><th id="1767">1767</th><td></td></tr>
<tr><th id="1768">1768</th><td>		<b>if</b> (prio != COMPACT_PRIO_ASYNC &amp;&amp; (status == COMPACT_COMPLETE ||</td></tr>
<tr><th id="1769">1769</th><td>					status == COMPACT_PARTIAL_SKIPPED))</td></tr>
<tr><th id="1770">1770</th><td>			<i>/*</i></td></tr>
<tr><th id="1771">1771</th><td><i>			 * We think that allocation won't succeed in this zone</i></td></tr>
<tr><th id="1772">1772</th><td><i>			 * so we defer compaction there. If it ends up</i></td></tr>
<tr><th id="1773">1773</th><td><i>			 * succeeding after all, it will be reset.</i></td></tr>
<tr><th id="1774">1774</th><td><i>			 */</i></td></tr>
<tr><th id="1775">1775</th><td>			defer_compaction(zone, order);</td></tr>
<tr><th id="1776">1776</th><td></td></tr>
<tr><th id="1777">1777</th><td>		<i>/*</i></td></tr>
<tr><th id="1778">1778</th><td><i>		 * We might have stopped compacting due to need_resched() in</i></td></tr>
<tr><th id="1779">1779</th><td><i>		 * async compaction, or due to a fatal signal detected. In that</i></td></tr>
<tr><th id="1780">1780</th><td><i>		 * case do not try further zones</i></td></tr>
<tr><th id="1781">1781</th><td><i>		 */</i></td></tr>
<tr><th id="1782">1782</th><td>		<b>if</b> ((prio == COMPACT_PRIO_ASYNC &amp;&amp; need_resched())</td></tr>
<tr><th id="1783">1783</th><td>					|| fatal_signal_pending(current))</td></tr>
<tr><th id="1784">1784</th><td>			<b>break</b>;</td></tr>
<tr><th id="1785">1785</th><td>	}</td></tr>
<tr><th id="1786">1786</th><td></td></tr>
<tr><th id="1787">1787</th><td>	<b>return</b> rc;</td></tr>
<tr><th id="1788">1788</th><td>}</td></tr>
<tr><th id="1789">1789</th><td></td></tr>
<tr><th id="1790">1790</th><td></td></tr>
<tr><th id="1791">1791</th><td><i>/* Compact all zones within a node */</i></td></tr>
<tr><th id="1792">1792</th><td><em>static</em> <em>void</em> compact_node(<em>int</em> nid)</td></tr>
<tr><th id="1793">1793</th><td>{</td></tr>
<tr><th id="1794">1794</th><td>	pg_data_t *pgdat = NODE_DATA(nid);</td></tr>
<tr><th id="1795">1795</th><td>	<em>int</em> zoneid;</td></tr>
<tr><th id="1796">1796</th><td>	<b>struct</b> zone *zone;</td></tr>
<tr><th id="1797">1797</th><td>	<b>struct</b> compact_control cc = {</td></tr>
<tr><th id="1798">1798</th><td>		.order = -<var>1</var>,</td></tr>
<tr><th id="1799">1799</th><td>		.total_migrate_scanned = <var>0</var>,</td></tr>
<tr><th id="1800">1800</th><td>		.total_free_scanned = <var>0</var>,</td></tr>
<tr><th id="1801">1801</th><td>		.mode = MIGRATE_SYNC,</td></tr>
<tr><th id="1802">1802</th><td>		.ignore_skip_hint = true,</td></tr>
<tr><th id="1803">1803</th><td>		.whole_zone = true,</td></tr>
<tr><th id="1804">1804</th><td>		.gfp_mask = GFP_KERNEL,</td></tr>
<tr><th id="1805">1805</th><td>	};</td></tr>
<tr><th id="1806">1806</th><td></td></tr>
<tr><th id="1807">1807</th><td></td></tr>
<tr><th id="1808">1808</th><td>	<b>for</b> (zoneid = <var>0</var>; zoneid &lt; MAX_NR_ZONES; zoneid++) {</td></tr>
<tr><th id="1809">1809</th><td></td></tr>
<tr><th id="1810">1810</th><td>		zone = &amp;pgdat-&gt;node_zones[zoneid];</td></tr>
<tr><th id="1811">1811</th><td>		<b>if</b> (!populated_zone(zone))</td></tr>
<tr><th id="1812">1812</th><td>			<b>continue</b>;</td></tr>
<tr><th id="1813">1813</th><td></td></tr>
<tr><th id="1814">1814</th><td>		cc.nr_freepages = <var>0</var>;</td></tr>
<tr><th id="1815">1815</th><td>		cc.nr_migratepages = <var>0</var>;</td></tr>
<tr><th id="1816">1816</th><td>		cc.zone = zone;</td></tr>
<tr><th id="1817">1817</th><td>		INIT_LIST_HEAD(&amp;cc.freepages);</td></tr>
<tr><th id="1818">1818</th><td>		INIT_LIST_HEAD(&amp;cc.migratepages);</td></tr>
<tr><th id="1819">1819</th><td></td></tr>
<tr><th id="1820">1820</th><td>		compact_zone(zone, &amp;cc);</td></tr>
<tr><th id="1821">1821</th><td></td></tr>
<tr><th id="1822">1822</th><td>		VM_BUG_ON(!list_empty(&amp;cc.freepages));</td></tr>
<tr><th id="1823">1823</th><td>		VM_BUG_ON(!list_empty(&amp;cc.migratepages));</td></tr>
<tr><th id="1824">1824</th><td>	}</td></tr>
<tr><th id="1825">1825</th><td>}</td></tr>
<tr><th id="1826">1826</th><td></td></tr>
<tr><th id="1827">1827</th><td><i>/* Compact all nodes in the system */</i></td></tr>
<tr><th id="1828">1828</th><td><em>static</em> <em>void</em> compact_nodes(<em>void</em>)</td></tr>
<tr><th id="1829">1829</th><td>{</td></tr>
<tr><th id="1830">1830</th><td>	<em>int</em> nid;</td></tr>
<tr><th id="1831">1831</th><td></td></tr>
<tr><th id="1832">1832</th><td>	<i>/* Flush pending updates to the LRU lists */</i></td></tr>
<tr><th id="1833">1833</th><td>	lru_add_drain_all();</td></tr>
<tr><th id="1834">1834</th><td></td></tr>
<tr><th id="1835">1835</th><td>	for_each_online_node(nid)</td></tr>
<tr><th id="1836">1836</th><td>		compact_node(nid);</td></tr>
<tr><th id="1837">1837</th><td>}</td></tr>
<tr><th id="1838">1838</th><td></td></tr>
<tr><th id="1839">1839</th><td><i>/* The written value is actually unused, all memory is compacted */</i></td></tr>
<tr><th id="1840">1840</th><td><em>int</em> sysctl_compact_memory;</td></tr>
<tr><th id="1841">1841</th><td></td></tr>
<tr><th id="1842">1842</th><td><i>/*</i></td></tr>
<tr><th id="1843">1843</th><td><i> * This is the entry point for compacting all nodes via</i></td></tr>
<tr><th id="1844">1844</th><td><i> * /proc/sys/vm/compact_memory</i></td></tr>
<tr><th id="1845">1845</th><td><i> */</i></td></tr>
<tr><th id="1846">1846</th><td><em>int</em> sysctl_compaction_handler(<b>struct</b> ctl_table *table, <em>int</em> write,</td></tr>
<tr><th id="1847">1847</th><td>			<em>void</em> __user *buffer, size_t *length, loff_t *ppos)</td></tr>
<tr><th id="1848">1848</th><td>{</td></tr>
<tr><th id="1849">1849</th><td>	<b>if</b> (write)</td></tr>
<tr><th id="1850">1850</th><td>		compact_nodes();</td></tr>
<tr><th id="1851">1851</th><td></td></tr>
<tr><th id="1852">1852</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1853">1853</th><td>}</td></tr>
<tr><th id="1854">1854</th><td></td></tr>
<tr><th id="1855">1855</th><td><em>int</em> sysctl_extfrag_handler(<b>struct</b> ctl_table *table, <em>int</em> write,</td></tr>
<tr><th id="1856">1856</th><td>			<em>void</em> __user *buffer, size_t *length, loff_t *ppos)</td></tr>
<tr><th id="1857">1857</th><td>{</td></tr>
<tr><th id="1858">1858</th><td>	proc_dointvec_minmax(table, write, buffer, length, ppos);</td></tr>
<tr><th id="1859">1859</th><td></td></tr>
<tr><th id="1860">1860</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1861">1861</th><td>}</td></tr>
<tr><th id="1862">1862</th><td></td></tr>
<tr><th id="1863">1863</th><td><u>#if defined(CONFIG_SYSFS) &amp;&amp; defined(CONFIG_NUMA)</u></td></tr>
<tr><th id="1864">1864</th><td><em>static</em> ssize_t sysfs_compact_node(<b>struct</b> device *dev,</td></tr>
<tr><th id="1865">1865</th><td>			<b>struct</b> device_attribute *attr,</td></tr>
<tr><th id="1866">1866</th><td>			<em>const</em> <em>char</em> *buf, size_t count)</td></tr>
<tr><th id="1867">1867</th><td>{</td></tr>
<tr><th id="1868">1868</th><td>	<em>int</em> nid = dev-&gt;id;</td></tr>
<tr><th id="1869">1869</th><td></td></tr>
<tr><th id="1870">1870</th><td>	<b>if</b> (nid &gt;= <var>0</var> &amp;&amp; nid &lt; nr_node_ids &amp;&amp; node_online(nid)) {</td></tr>
<tr><th id="1871">1871</th><td>		<i>/* Flush pending updates to the LRU lists */</i></td></tr>
<tr><th id="1872">1872</th><td>		lru_add_drain_all();</td></tr>
<tr><th id="1873">1873</th><td></td></tr>
<tr><th id="1874">1874</th><td>		compact_node(nid);</td></tr>
<tr><th id="1875">1875</th><td>	}</td></tr>
<tr><th id="1876">1876</th><td></td></tr>
<tr><th id="1877">1877</th><td>	<b>return</b> count;</td></tr>
<tr><th id="1878">1878</th><td>}</td></tr>
<tr><th id="1879">1879</th><td><em>static</em> DEVICE_ATTR(compact, S_IWUSR, NULL, sysfs_compact_node);</td></tr>
<tr><th id="1880">1880</th><td></td></tr>
<tr><th id="1881">1881</th><td><em>int</em> compaction_register_node(<b>struct</b> node *node)</td></tr>
<tr><th id="1882">1882</th><td>{</td></tr>
<tr><th id="1883">1883</th><td>	<b>return</b> device_create_file(&amp;node-&gt;dev, &amp;dev_attr_compact);</td></tr>
<tr><th id="1884">1884</th><td>}</td></tr>
<tr><th id="1885">1885</th><td></td></tr>
<tr><th id="1886">1886</th><td><em>void</em> compaction_unregister_node(<b>struct</b> node *node)</td></tr>
<tr><th id="1887">1887</th><td>{</td></tr>
<tr><th id="1888">1888</th><td>	<b>return</b> device_remove_file(&amp;node-&gt;dev, &amp;dev_attr_compact);</td></tr>
<tr><th id="1889">1889</th><td>}</td></tr>
<tr><th id="1890">1890</th><td><u>#endif /* CONFIG_SYSFS &amp;&amp; CONFIG_NUMA */</u></td></tr>
<tr><th id="1891">1891</th><td></td></tr>
<tr><th id="1892">1892</th><td><em>static</em> <b>inline</b> bool kcompactd_work_requested(pg_data_t *pgdat)</td></tr>
<tr><th id="1893">1893</th><td>{</td></tr>
<tr><th id="1894">1894</th><td>	<b>return</b> pgdat-&gt;kcompactd_max_order &gt; <var>0</var> || kthread_should_stop();</td></tr>
<tr><th id="1895">1895</th><td>}</td></tr>
<tr><th id="1896">1896</th><td></td></tr>
<tr><th id="1897">1897</th><td><em>static</em> bool kcompactd_node_suitable(pg_data_t *pgdat)</td></tr>
<tr><th id="1898">1898</th><td>{</td></tr>
<tr><th id="1899">1899</th><td>	<em>int</em> zoneid;</td></tr>
<tr><th id="1900">1900</th><td>	<b>struct</b> zone *zone;</td></tr>
<tr><th id="1901">1901</th><td>	<b>enum</b> zone_type classzone_idx = pgdat-&gt;kcompactd_classzone_idx;</td></tr>
<tr><th id="1902">1902</th><td></td></tr>
<tr><th id="1903">1903</th><td>	<b>for</b> (zoneid = <var>0</var>; zoneid &lt;= classzone_idx; zoneid++) {</td></tr>
<tr><th id="1904">1904</th><td>		zone = &amp;pgdat-&gt;node_zones[zoneid];</td></tr>
<tr><th id="1905">1905</th><td></td></tr>
<tr><th id="1906">1906</th><td>		<b>if</b> (!populated_zone(zone))</td></tr>
<tr><th id="1907">1907</th><td>			<b>continue</b>;</td></tr>
<tr><th id="1908">1908</th><td></td></tr>
<tr><th id="1909">1909</th><td>		<b>if</b> (compaction_suitable(zone, pgdat-&gt;kcompactd_max_order, <var>0</var>,</td></tr>
<tr><th id="1910">1910</th><td>					classzone_idx) == COMPACT_CONTINUE)</td></tr>
<tr><th id="1911">1911</th><td>			<b>return</b> true;</td></tr>
<tr><th id="1912">1912</th><td>	}</td></tr>
<tr><th id="1913">1913</th><td></td></tr>
<tr><th id="1914">1914</th><td>	<b>return</b> false;</td></tr>
<tr><th id="1915">1915</th><td>}</td></tr>
<tr><th id="1916">1916</th><td></td></tr>
<tr><th id="1917">1917</th><td><em>static</em> <em>void</em> kcompactd_do_work(pg_data_t *pgdat)</td></tr>
<tr><th id="1918">1918</th><td>{</td></tr>
<tr><th id="1919">1919</th><td>	<i>/*</i></td></tr>
<tr><th id="1920">1920</th><td><i>	 * With no special task, compact all zones so that a page of requested</i></td></tr>
<tr><th id="1921">1921</th><td><i>	 * order is allocatable.</i></td></tr>
<tr><th id="1922">1922</th><td><i>	 */</i></td></tr>
<tr><th id="1923">1923</th><td>	<em>int</em> zoneid;</td></tr>
<tr><th id="1924">1924</th><td>	<b>struct</b> zone *zone;</td></tr>
<tr><th id="1925">1925</th><td>	<b>struct</b> compact_control cc = {</td></tr>
<tr><th id="1926">1926</th><td>		.order = pgdat-&gt;kcompactd_max_order,</td></tr>
<tr><th id="1927">1927</th><td>		.total_migrate_scanned = <var>0</var>,</td></tr>
<tr><th id="1928">1928</th><td>		.total_free_scanned = <var>0</var>,</td></tr>
<tr><th id="1929">1929</th><td>		.classzone_idx = pgdat-&gt;kcompactd_classzone_idx,</td></tr>
<tr><th id="1930">1930</th><td>		.mode = MIGRATE_SYNC_LIGHT,</td></tr>
<tr><th id="1931">1931</th><td>		.ignore_skip_hint = true,</td></tr>
<tr><th id="1932">1932</th><td>		.gfp_mask = GFP_KERNEL,</td></tr>
<tr><th id="1933">1933</th><td></td></tr>
<tr><th id="1934">1934</th><td>	};</td></tr>
<tr><th id="1935">1935</th><td>	trace_mm_compaction_kcompactd_wake(pgdat-&gt;node_id, cc.order,</td></tr>
<tr><th id="1936">1936</th><td>							cc.classzone_idx);</td></tr>
<tr><th id="1937">1937</th><td>	count_compact_event(KCOMPACTD_WAKE);</td></tr>
<tr><th id="1938">1938</th><td></td></tr>
<tr><th id="1939">1939</th><td>	<b>for</b> (zoneid = <var>0</var>; zoneid &lt;= cc.classzone_idx; zoneid++) {</td></tr>
<tr><th id="1940">1940</th><td>		<em>int</em> status;</td></tr>
<tr><th id="1941">1941</th><td></td></tr>
<tr><th id="1942">1942</th><td>		zone = &amp;pgdat-&gt;node_zones[zoneid];</td></tr>
<tr><th id="1943">1943</th><td>		<b>if</b> (!populated_zone(zone))</td></tr>
<tr><th id="1944">1944</th><td>			<b>continue</b>;</td></tr>
<tr><th id="1945">1945</th><td></td></tr>
<tr><th id="1946">1946</th><td>		<b>if</b> (compaction_deferred(zone, cc.order))</td></tr>
<tr><th id="1947">1947</th><td>			<b>continue</b>;</td></tr>
<tr><th id="1948">1948</th><td></td></tr>
<tr><th id="1949">1949</th><td>		<b>if</b> (compaction_suitable(zone, cc.order, <var>0</var>, zoneid) !=</td></tr>
<tr><th id="1950">1950</th><td>							COMPACT_CONTINUE)</td></tr>
<tr><th id="1951">1951</th><td>			<b>continue</b>;</td></tr>
<tr><th id="1952">1952</th><td></td></tr>
<tr><th id="1953">1953</th><td>		cc.nr_freepages = <var>0</var>;</td></tr>
<tr><th id="1954">1954</th><td>		cc.nr_migratepages = <var>0</var>;</td></tr>
<tr><th id="1955">1955</th><td>		cc.total_migrate_scanned = <var>0</var>;</td></tr>
<tr><th id="1956">1956</th><td>		cc.total_free_scanned = <var>0</var>;</td></tr>
<tr><th id="1957">1957</th><td>		cc.zone = zone;</td></tr>
<tr><th id="1958">1958</th><td>		INIT_LIST_HEAD(&amp;cc.freepages);</td></tr>
<tr><th id="1959">1959</th><td>		INIT_LIST_HEAD(&amp;cc.migratepages);</td></tr>
<tr><th id="1960">1960</th><td></td></tr>
<tr><th id="1961">1961</th><td>		<b>if</b> (kthread_should_stop())</td></tr>
<tr><th id="1962">1962</th><td>			<b>return</b>;</td></tr>
<tr><th id="1963">1963</th><td>		status = compact_zone(zone, &amp;cc);</td></tr>
<tr><th id="1964">1964</th><td></td></tr>
<tr><th id="1965">1965</th><td>		<b>if</b> (status == COMPACT_SUCCESS) {</td></tr>
<tr><th id="1966">1966</th><td>			compaction_defer_reset(zone, cc.order, false);</td></tr>
<tr><th id="1967">1967</th><td>		} <b>else</b> <b>if</b> (status == COMPACT_PARTIAL_SKIPPED || status == COMPACT_COMPLETE) {</td></tr>
<tr><th id="1968">1968</th><td>			<i>/*</i></td></tr>
<tr><th id="1969">1969</th><td><i>			 * We use sync migration mode here, so we defer like</i></td></tr>
<tr><th id="1970">1970</th><td><i>			 * sync direct compaction does.</i></td></tr>
<tr><th id="1971">1971</th><td><i>			 */</i></td></tr>
<tr><th id="1972">1972</th><td>			defer_compaction(zone, cc.order);</td></tr>
<tr><th id="1973">1973</th><td>		}</td></tr>
<tr><th id="1974">1974</th><td></td></tr>
<tr><th id="1975">1975</th><td>		count_compact_events(KCOMPACTD_MIGRATE_SCANNED,</td></tr>
<tr><th id="1976">1976</th><td>				     cc.total_migrate_scanned);</td></tr>
<tr><th id="1977">1977</th><td>		count_compact_events(KCOMPACTD_FREE_SCANNED,</td></tr>
<tr><th id="1978">1978</th><td>				     cc.total_free_scanned);</td></tr>
<tr><th id="1979">1979</th><td></td></tr>
<tr><th id="1980">1980</th><td>		VM_BUG_ON(!list_empty(&amp;cc.freepages));</td></tr>
<tr><th id="1981">1981</th><td>		VM_BUG_ON(!list_empty(&amp;cc.migratepages));</td></tr>
<tr><th id="1982">1982</th><td>	}</td></tr>
<tr><th id="1983">1983</th><td></td></tr>
<tr><th id="1984">1984</th><td>	<i>/*</i></td></tr>
<tr><th id="1985">1985</th><td><i>	 * Regardless of success, we are done until woken up next. But remember</i></td></tr>
<tr><th id="1986">1986</th><td><i>	 * the requested order/classzone_idx in case it was higher/tighter than</i></td></tr>
<tr><th id="1987">1987</th><td><i>	 * our current ones</i></td></tr>
<tr><th id="1988">1988</th><td><i>	 */</i></td></tr>
<tr><th id="1989">1989</th><td>	<b>if</b> (pgdat-&gt;kcompactd_max_order &lt;= cc.order)</td></tr>
<tr><th id="1990">1990</th><td>		pgdat-&gt;kcompactd_max_order = <var>0</var>;</td></tr>
<tr><th id="1991">1991</th><td>	<b>if</b> (pgdat-&gt;kcompactd_classzone_idx &gt;= cc.classzone_idx)</td></tr>
<tr><th id="1992">1992</th><td>		pgdat-&gt;kcompactd_classzone_idx = pgdat-&gt;nr_zones - <var>1</var>;</td></tr>
<tr><th id="1993">1993</th><td>}</td></tr>
<tr><th id="1994">1994</th><td></td></tr>
<tr><th id="1995">1995</th><td><em>void</em> wakeup_kcompactd(pg_data_t *pgdat, <em>int</em> order, <em>int</em> classzone_idx)</td></tr>
<tr><th id="1996">1996</th><td>{</td></tr>
<tr><th id="1997">1997</th><td>	<b>if</b> (!order)</td></tr>
<tr><th id="1998">1998</th><td>		<b>return</b>;</td></tr>
<tr><th id="1999">1999</th><td></td></tr>
<tr><th id="2000">2000</th><td>	<b>if</b> (pgdat-&gt;kcompactd_max_order &lt; order)</td></tr>
<tr><th id="2001">2001</th><td>		pgdat-&gt;kcompactd_max_order = order;</td></tr>
<tr><th id="2002">2002</th><td></td></tr>
<tr><th id="2003">2003</th><td>	<b>if</b> (pgdat-&gt;kcompactd_classzone_idx &gt; classzone_idx)</td></tr>
<tr><th id="2004">2004</th><td>		pgdat-&gt;kcompactd_classzone_idx = classzone_idx;</td></tr>
<tr><th id="2005">2005</th><td></td></tr>
<tr><th id="2006">2006</th><td>	<i>/*</i></td></tr>
<tr><th id="2007">2007</th><td><i>	 * Pairs with implicit barrier in wait_event_freezable()</i></td></tr>
<tr><th id="2008">2008</th><td><i>	 * such that wakeups are not missed.</i></td></tr>
<tr><th id="2009">2009</th><td><i>	 */</i></td></tr>
<tr><th id="2010">2010</th><td>	<b>if</b> (!wq_has_sleeper(&amp;pgdat-&gt;kcompactd_wait))</td></tr>
<tr><th id="2011">2011</th><td>		<b>return</b>;</td></tr>
<tr><th id="2012">2012</th><td></td></tr>
<tr><th id="2013">2013</th><td>	<b>if</b> (!kcompactd_node_suitable(pgdat))</td></tr>
<tr><th id="2014">2014</th><td>		<b>return</b>;</td></tr>
<tr><th id="2015">2015</th><td></td></tr>
<tr><th id="2016">2016</th><td>	trace_mm_compaction_wakeup_kcompactd(pgdat-&gt;node_id, order,</td></tr>
<tr><th id="2017">2017</th><td>							classzone_idx);</td></tr>
<tr><th id="2018">2018</th><td>	wake_up_interruptible(&amp;pgdat-&gt;kcompactd_wait);</td></tr>
<tr><th id="2019">2019</th><td>}</td></tr>
<tr><th id="2020">2020</th><td></td></tr>
<tr><th id="2021">2021</th><td><i>/*</i></td></tr>
<tr><th id="2022">2022</th><td><i> * The background compaction daemon, started as a kernel thread</i></td></tr>
<tr><th id="2023">2023</th><td><i> * from the init process.</i></td></tr>
<tr><th id="2024">2024</th><td><i> */</i></td></tr>
<tr><th id="2025">2025</th><td><em>static</em> <em>int</em> kcompactd(<em>void</em> *p)</td></tr>
<tr><th id="2026">2026</th><td>{</td></tr>
<tr><th id="2027">2027</th><td>	pg_data_t *pgdat = (pg_data_t*)p;</td></tr>
<tr><th id="2028">2028</th><td>	<b>struct</b> task_struct *tsk = current;</td></tr>
<tr><th id="2029">2029</th><td></td></tr>
<tr><th id="2030">2030</th><td>	<em>const</em> <b>struct</b> cpumask *cpumask = cpumask_of_node(pgdat-&gt;node_id);</td></tr>
<tr><th id="2031">2031</th><td></td></tr>
<tr><th id="2032">2032</th><td>	<b>if</b> (!cpumask_empty(cpumask))</td></tr>
<tr><th id="2033">2033</th><td>		set_cpus_allowed_ptr(tsk, cpumask);</td></tr>
<tr><th id="2034">2034</th><td></td></tr>
<tr><th id="2035">2035</th><td>	set_freezable();</td></tr>
<tr><th id="2036">2036</th><td></td></tr>
<tr><th id="2037">2037</th><td>	pgdat-&gt;kcompactd_max_order = <var>0</var>;</td></tr>
<tr><th id="2038">2038</th><td>	pgdat-&gt;kcompactd_classzone_idx = pgdat-&gt;nr_zones - <var>1</var>;</td></tr>
<tr><th id="2039">2039</th><td></td></tr>
<tr><th id="2040">2040</th><td>	<b>while</b> (!kthread_should_stop()) {</td></tr>
<tr><th id="2041">2041</th><td>		trace_mm_compaction_kcompactd_sleep(pgdat-&gt;node_id);</td></tr>
<tr><th id="2042">2042</th><td>		wait_event_freezable(pgdat-&gt;kcompactd_wait,</td></tr>
<tr><th id="2043">2043</th><td>				kcompactd_work_requested(pgdat));</td></tr>
<tr><th id="2044">2044</th><td></td></tr>
<tr><th id="2045">2045</th><td>		kcompactd_do_work(pgdat);</td></tr>
<tr><th id="2046">2046</th><td>	}</td></tr>
<tr><th id="2047">2047</th><td></td></tr>
<tr><th id="2048">2048</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="2049">2049</th><td>}</td></tr>
<tr><th id="2050">2050</th><td></td></tr>
<tr><th id="2051">2051</th><td><i>/*</i></td></tr>
<tr><th id="2052">2052</th><td><i> * This kcompactd start function will be called by init and node-hot-add.</i></td></tr>
<tr><th id="2053">2053</th><td><i> * On node-hot-add, kcompactd will moved to proper cpus if cpus are hot-added.</i></td></tr>
<tr><th id="2054">2054</th><td><i> */</i></td></tr>
<tr><th id="2055">2055</th><td><em>int</em> kcompactd_run(<em>int</em> nid)</td></tr>
<tr><th id="2056">2056</th><td>{</td></tr>
<tr><th id="2057">2057</th><td>	pg_data_t *pgdat = NODE_DATA(nid);</td></tr>
<tr><th id="2058">2058</th><td>	<em>int</em> ret = <var>0</var>;</td></tr>
<tr><th id="2059">2059</th><td></td></tr>
<tr><th id="2060">2060</th><td>	<b>if</b> (pgdat-&gt;kcompactd)</td></tr>
<tr><th id="2061">2061</th><td>		<b>return</b> <var>0</var>;</td></tr>
<tr><th id="2062">2062</th><td></td></tr>
<tr><th id="2063">2063</th><td>	pgdat-&gt;kcompactd = kthread_run(kcompactd, pgdat, <q>"kcompactd%d"</q>, nid);</td></tr>
<tr><th id="2064">2064</th><td>	<b>if</b> (IS_ERR(pgdat-&gt;kcompactd)) {</td></tr>
<tr><th id="2065">2065</th><td>		pr_err(<q>"Failed to start kcompactd on node %d\n"</q>, nid);</td></tr>
<tr><th id="2066">2066</th><td>		ret = PTR_ERR(pgdat-&gt;kcompactd);</td></tr>
<tr><th id="2067">2067</th><td>		pgdat-&gt;kcompactd = NULL;</td></tr>
<tr><th id="2068">2068</th><td>	}</td></tr>
<tr><th id="2069">2069</th><td>	<b>return</b> ret;</td></tr>
<tr><th id="2070">2070</th><td>}</td></tr>
<tr><th id="2071">2071</th><td></td></tr>
<tr><th id="2072">2072</th><td><i>/*</i></td></tr>
<tr><th id="2073">2073</th><td><i> * Called by memory hotplug when all memory in a node is offlined. Caller must</i></td></tr>
<tr><th id="2074">2074</th><td><i> * hold mem_hotplug_begin/end().</i></td></tr>
<tr><th id="2075">2075</th><td><i> */</i></td></tr>
<tr><th id="2076">2076</th><td><em>void</em> kcompactd_stop(<em>int</em> nid)</td></tr>
<tr><th id="2077">2077</th><td>{</td></tr>
<tr><th id="2078">2078</th><td>	<b>struct</b> task_struct *kcompactd = NODE_DATA(nid)-&gt;kcompactd;</td></tr>
<tr><th id="2079">2079</th><td></td></tr>
<tr><th id="2080">2080</th><td>	<b>if</b> (kcompactd) {</td></tr>
<tr><th id="2081">2081</th><td>		kthread_stop(kcompactd);</td></tr>
<tr><th id="2082">2082</th><td>		NODE_DATA(nid)-&gt;kcompactd = NULL;</td></tr>
<tr><th id="2083">2083</th><td>	}</td></tr>
<tr><th id="2084">2084</th><td>}</td></tr>
<tr><th id="2085">2085</th><td></td></tr>
<tr><th id="2086">2086</th><td><i>/*</i></td></tr>
<tr><th id="2087">2087</th><td><i> * It's optimal to keep kcompactd on the same CPUs as their memory, but</i></td></tr>
<tr><th id="2088">2088</th><td><i> * not required for correctness. So if the last cpu in a node goes</i></td></tr>
<tr><th id="2089">2089</th><td><i> * away, we get changed to run anywhere: as the first one comes back,</i></td></tr>
<tr><th id="2090">2090</th><td><i> * restore their cpu bindings.</i></td></tr>
<tr><th id="2091">2091</th><td><i> */</i></td></tr>
<tr><th id="2092">2092</th><td><em>static</em> <em>int</em> kcompactd_cpu_online(<em>unsigned</em> <em>int</em> cpu)</td></tr>
<tr><th id="2093">2093</th><td>{</td></tr>
<tr><th id="2094">2094</th><td>	<em>int</em> nid;</td></tr>
<tr><th id="2095">2095</th><td></td></tr>
<tr><th id="2096">2096</th><td>	for_each_node_state(nid, N_MEMORY) {</td></tr>
<tr><th id="2097">2097</th><td>		pg_data_t *pgdat = NODE_DATA(nid);</td></tr>
<tr><th id="2098">2098</th><td>		<em>const</em> <b>struct</b> cpumask *mask;</td></tr>
<tr><th id="2099">2099</th><td></td></tr>
<tr><th id="2100">2100</th><td>		mask = cpumask_of_node(pgdat-&gt;node_id);</td></tr>
<tr><th id="2101">2101</th><td></td></tr>
<tr><th id="2102">2102</th><td>		<b>if</b> (cpumask_any_and(cpu_online_mask, mask) &lt; nr_cpu_ids)</td></tr>
<tr><th id="2103">2103</th><td>			<i>/* One of our CPUs online: restore mask */</i></td></tr>
<tr><th id="2104">2104</th><td>			set_cpus_allowed_ptr(pgdat-&gt;kcompactd, mask);</td></tr>
<tr><th id="2105">2105</th><td>	}</td></tr>
<tr><th id="2106">2106</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="2107">2107</th><td>}</td></tr>
<tr><th id="2108">2108</th><td></td></tr>
<tr><th id="2109">2109</th><td><em>static</em> <em>int</em> __init kcompactd_init(<em>void</em>)</td></tr>
<tr><th id="2110">2110</th><td>{</td></tr>
<tr><th id="2111">2111</th><td>	<em>int</em> nid;</td></tr>
<tr><th id="2112">2112</th><td>	<em>int</em> ret;</td></tr>
<tr><th id="2113">2113</th><td></td></tr>
<tr><th id="2114">2114</th><td>	ret = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,</td></tr>
<tr><th id="2115">2115</th><td>					<q>"mm/compaction:online"</q>,</td></tr>
<tr><th id="2116">2116</th><td>					kcompactd_cpu_online, NULL);</td></tr>
<tr><th id="2117">2117</th><td>	<b>if</b> (ret &lt; <var>0</var>) {</td></tr>
<tr><th id="2118">2118</th><td>		pr_err(<q>"kcompactd: failed to register hotplug callbacks.\n"</q>);</td></tr>
<tr><th id="2119">2119</th><td>		<b>return</b> ret;</td></tr>
<tr><th id="2120">2120</th><td>	}</td></tr>
<tr><th id="2121">2121</th><td></td></tr>
<tr><th id="2122">2122</th><td>	for_each_node_state(nid, N_MEMORY)</td></tr>
<tr><th id="2123">2123</th><td>		kcompactd_run(nid);</td></tr>
<tr><th id="2124">2124</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="2125">2125</th><td>}</td></tr>
<tr><th id="2126">2126</th><td>subsys_initcall(kcompactd_init)</td></tr>
<tr><th id="2127">2127</th><td></td></tr>
<tr><th id="2128">2128</th><td><u>#<span data-ppcond="984">endif</span> /* CONFIG_COMPACTION */</u></td></tr>
<tr><th id="2129">2129</th><td></td></tr>
</table><hr/><p id='footer'>
Generated on <em>2018-Jul-30</em> from project linux-4.14.y revision <em>linux-4.14.y</em><br />Powered by <a href='https://woboq.com'><img alt='Woboq' src='https://code.woboq.org/woboq-16.png' width='41' height='16' /></a> <a href='https://code.woboq.org'>Code Browser</a> 2.1
<br/>Generator usage only permitted with license.</p>
</div></body></html>
